<?xml version="1.0" encoding="utf-8"?>
<search>

    <entry>
      <title>About Me</title>
      <link href="/2022/09/05/About%20me/"/>
      <url>/2022/09/05/About%20me/</url>
      <content type="html"><![CDATA[Education Graduated from the University of Waterloo in 2022 with a Bachelor of Mathematics, majoring in Mathematical Finance and minoring in Statistics My academic training focused on artificial intelligence, quantitative finance, statistics, computer science, and pure mathematics, with strong foundations in probability theory, linear models, and algorithmic thinking I am currently pursuing a Master of Science in Artificial Intelligence at the University of Hong Kong, with coursework including Machine Learning, Deep Learning, NLP, Data Mining and Text Analytics, and Machine Learning in Trading and Finance Work Experience Data Analyst — RBC Insurance Canada Develop automation executable programs using Python, VBA, streamlining auditing, client, secondary insurer reporting, and pension administration processes reducing manual effort by an average of 15 days per month Enhance ETL processes with advanced Excel VBA macros for data cleaning and Python scripts MS Access for data management and validation, decreasing annuity data implementation time by 2 days per cycle Facilitate seamless data migration by improving and executing complex SQL scripts and validating data integrity with Python Xlwings, Pandas, and NumPy, ensuring accurate data transfer and system reliability Develop dynamic, interactive Tableau dashboards for monthly and ad-hoc business and retiree stats reporting, enabling strategic decision-making and effective annuity pricing for the VP and actuarial team. Commodity Reseacher Intern — Zhengzhou Commodity Exchange Futures &amp; Derivatives Research Institute Analyzed and calculated futures indices using Python (PySpark, Requests, SQLAlchemy, Numpy) and AWS Glue to create a data pipeline for processing extensive trading data. Visualized findings in Power BI to define annual research topics. Authored a research report on momentum trading, examining the correlation between futures prices and strategy penetration rates to identify potential index fluctuations in futures traded at the ZCE. Performed data cleaning such as removing duplicate or irrelevant observations, fixing structural errors (typos, or incorrect capitalization), filtering unwanted outliers, handling missing data, and validation. Presented research findings to stakeholders, translating complex insights into actionable recommendations using data storytelling and visualization in Tableau. Financial Analyst — Central China Securities Conducted secondary market research on the Shanghai Stock Exchange (SSE) to identify profitable stocks across different sectors and contributed to investment reports for external clients Assessed clients’ investment budgets and risk tolerance to recommend suitable financial products and customized investment strategies Demonstrated strong communication and teamwork skills by presenting financial analyses in clear, accessible terms and delivering portfolio recommendations during investment pitch presentations Interests Taekwondo has been one of my greatest interests in my lifetime. Back in high school, I was leading the school-sponsored taekwondo club. I highly think of the roles I played back then, including managing club members on daily basis, preparing shows for school ceremonies, etc. Here are some pics of our performance below: Show commencement: Me leading style show: Me leading boxing show: Men’s physique training is the area I show the greatest interest in as well. I have been learning, applying, and practicing physique training for 7 years, and gained a lot of experience. Furthermore, I am also into powerlifting, since body strength holds an unspoken attraction to me AboutMe About Me]]></content>
    </entry>

    <entry>
      <title>关于我</title>
      <link href="/2022/09/05/About%20me/index-zh.html"/>
      <url>/2022/09/05/About%20me/index-zh.html</url>
      <content type="html"><![CDATA[]]></content>
    </entry>

    <entry>
      <title>Data science and Machine learning in Python - 1. Review of Python</title>
      <link href="/2022/09/11/Python%20Crash%20Course/"/>
      <url>/2022/09/11/Python%20Crash%20Course/</url>
      <content type="html"><![CDATA[Python Comprehensive Syntax Review Data types Numbers Strings Printing Lists Dictionaries Booleans Tuples Sets Comparison Operators if, elif, else Statements for Loops while Loops range() list comprehension functions lambda expressions map and filter methods Data typesNumbers11 + 1 2 11 * 3 3 11 / 2 0.5 12 ** 4 16 14 % 2 0 15 % 2 1 1(2 + 3) * (5 + 5) 50 Variable Assignment12# Can not start with number or special charactersname_of_var = 2 12x = 2y = 3 1z = x + y 1z 5 Strings1&#x27;single quotes&#x27; &#39;single quotes&#39; 1&quot;double quotes&quot; &#39;double quotes&#39; 1&quot; wrap lot&#x27;s of other quotes&quot; &quot; wrap lot&#39;s of other quotes&quot; Printing1x = &#x27;hello&#x27; 1x &#39;hello&#39; 1print(x) hello 12num = 12name = &#x27;Sam&#x27; 1print(&#x27;My number is: &#123;one&#125;, and my name is: &#123;two&#125;, and more &#123;one&#125;&#x27;.format(one=num,two=name)) My number is: 12, and my name is: Sam, and more 12 1print(&#x27;My number is: &#123;&#125;, and my name is: &#123;&#125;&#x27;.format(num,name)) My number is: 12, and my name is: Sam Lists1[1,2,3] [1, 2, 3] 1[&#x27;hi&#x27;,1,[1,2]] [&#39;hi&#39;, 1, [1, 2]] 1my_list = [&#x27;a&#x27;,&#x27;b&#x27;,&#x27;c&#x27;] 1my_list.append(&#x27;d&#x27;) 1my_list [&#39;a&#39;, &#39;b&#39;, &#39;c&#39;, &#39;d&#39;] 1my_list[0] &#39;a&#39; 1my_list[1] &#39;b&#39; 1my_list[1:] [&#39;b&#39;, &#39;c&#39;, &#39;d&#39;] 1my_list[:1] [&#39;a&#39;] 1my_list[0] = &#x27;NEW&#x27; 1my_list [&#39;NEW&#39;, &#39;b&#39;, &#39;c&#39;, &#39;d&#39;] 1nest = [1,2,3,[4,5,[&#x27;target&#x27;]]] 1nest[3] [4, 5, [&#39;target&#39;]] 1nest[3][2] [&#39;target&#39;] 1nest[3][2][0] &#39;target&#39; Dictionaries1d = &#123;&#x27;key1&#x27;:&#x27;item1&#x27;,&#x27;key2&#x27;:&#x27;item2&#x27;&#125; 1d &#123;&#39;key1&#39;: &#39;item1&#39;, &#39;key2&#39;: &#39;item2&#39;&#125; 1d[&#x27;key1&#x27;] &#39;item1&#39; 1d[&#x27;key 3&#x27;] = &#x27;item3&#x27; 1d[&#x27;key 3&#x27;] &#39;item3&#39; Nested dictionaries allowed 1d2 = &#123;&#x27;k1&#x27;:d, &#x27;k2&#x27;: d&#125; 1print(d2) # dictionary does not store any order but just keys and elements. &#123;&#39;k1&#39;: &#123;&#39;key1&#39;: &#39;item1&#39;, &#39;key2&#39;: &#39;item2&#39;, &#39;key 3&#39;: &#39;item3&#39;&#125;, &#39;k2&#39;: &#123;&#39;key1&#39;: &#39;item1&#39;, &#39;key2&#39;: &#39;item2&#39;, &#39;key 3&#39;: &#39;item3&#39;&#125;&#125; Booleans1True True 1False False Tuples1t = (1,2,3) 1t[0] 1 Tuples are inmutable. You cannot change elemetns in tuples, here is an example. 1t[0] = &#x27;NEW&#x27; --------------------------------------------------------------------------- TypeError Traceback (most recent call last) &lt;ipython-input-44-97e4e33b36c2&gt; in &lt;module&gt;() ----&gt; 1 t[0] = &#39;NEW&#39; TypeError: &#39;tuple&#39; object does not support item assignment Sets1&#123;1,2,3&#125; &#123;1, 2, 3&#125; Set function gives you a set of unique elements 1&#123;1,2,3,1,2,1,2,3,3,3,3,2,2,2,1,1,2&#125; &#123;1, 2, 3&#125; Functions under SETS: 1s = &#123;1,2,3&#125; 12s.add(5)print(s) &#123;1, 2, 3, 5&#125; 12s.remove(5) #Notre here: remove/add function mutates the set s.print(s) &#123;1, 2, 3&#125; 1 Comparison Operators11 &gt; 2 False 11 &lt; 2 True 11 &gt;= 1 True 11 &lt;= 4 True 11 == 1 # &#x27;=&#x27; is an assignment operator True 1&#x27;hi&#x27; == &#x27;bye&#x27; False Logic Operators1(1 &gt; 2) and (2 &lt; 3) False 1(1 &gt; 2) or (2 &lt; 3) True 1(1 == 2) or (2 == 3) or (4 == 4) True if,elif, else Statements12if 1 &lt; 2: print(&#x27;Yep!&#x27;) Yep! 12if 1 &lt; 2: print(&#x27;yep!&#x27;) yep! 1234if 1 &lt; 2: print(&#x27;first&#x27;)else: print(&#x27;last&#x27;) first 1234if 1 &gt; 2: print(&#x27;first&#x27;)else: print(&#x27;last&#x27;) last 123456if 1 == 2: print(&#x27;first&#x27;)elif 3 == 3: print(&#x27;middle&#x27;)else: print(&#x27;Last&#x27;) middle Furthermore: 1print((3&gt;2) + 1); # 3&gt;2 gives us true, and true + 1 gives us 2. 2 1print((3&gt;4) + 1); 1 Follwoing example shows 1 and 0 are the embeded val for bool. 1234if 0: print(&quot;bool is false&quot;);if 1: print(&#x27;bool is true&#x27;); bool is true for Loops1seq = [1,2,3,4,5] 12for item in seq: # for loop allows you to iterate in a sequence; print(item) 1 2 3 4 5 12for item in seq: print(&#x27;Yep&#x27;) Yep Yep Yep Yep Yep 12for jelly in seq: print(jelly+jelly) 2 4 6 8 10 while Loops1234i = 1while i &lt; 5: print(&#x27;i is: &#123;&#125;&#x27;.format(i)) i = i+1 i is: 1 i is: 2 i is: 3 i is: 4 range()1range(5) range(0, 5) 12for i in range(5): print(i) 0 1 2 3 4 1list(range(5)) #make it a squence; [0, 1, 2, 3, 4] 1list(range(0,10,2)) #making a list from 0 to 10 with 2 stepss [0, 2, 4, 6, 8] list comprehension1x = [1,2,3,4] 1234out = []for item in x: out.append(item**2)print(out) [1, 4, 9, 16] 1[item**2 for item in x] # list comprehension [1, 4, 9, 16] functions12345def my_func(param1=&#x27;default&#x27;): &quot;&quot;&quot; Docstring goes here. &quot;&quot;&quot; print(param1) 1my_func # reports back the object &lt;function __main__.my_func&gt; 1my_func() # calls the function default 1my_func(&#x27;new param&#x27;) new param 1my_func(param1=&#x27;new param&#x27;) new param 12def square(x): return x**2 1out = square(2) 1print(out) 4 lambda expressions12def times2(var): return var*2 1times2(2) 4 1lambda var: var*2 &lt;function __main__.&lt;lambda&gt;(var)&gt; 12t = lambda var : var*2print(t(2)); 4 map and filter1seq = [1,2,3,4,5] 1map(times2,seq) # maps a function to each elemetn int the list &lt;map at 0x105316748&gt; 1list(map(times2,seq)) [2, 4, 6, 8, 10] 1list(map(lambda var: var*2,seq)) # lambda expression saves code writing [2, 4, 6, 8, 10] 1filter(lambda item: item%2 == 0,seq) &lt;filter at 0x105316ac8&gt; 1list(filter(lambda item: item%2 == 0,seq)) # if fucntion iterating on each element. [2, 4] methods1st = &#x27;hello my name is Sam&#x27; 1st.lower() # you can type st. and see all available methods &#39;hello my name is sam&#39; 1st.upper() &#39;HELLO MY NAME IS SAM&#39; 1st.split() # splits the string on the white space if there&#x27;s no input. [&#39;hello&#39;, &#39;my&#39;, &#39;name&#39;, &#39;is&#39;, &#39;Sam&#39;] 1tweet = &#x27;Go Sports! #Sports&#x27; 1tweet.split(&#x27;#&#x27;) [&#39;Go Sports! &#39;, &#39;Sports&#39;] 1tweet.split(&#x27;#&#x27;)[1] &#39;Sports&#39; 1d &#123;&#39;key1&#39;: &#39;item1&#39;, &#39;key2&#39;: &#39;item2&#39;&#125; 1d.keys() dict_keys([&#39;key2&#39;, &#39;key1&#39;]) 1d.items() dict_items([(&#39;key2&#39;, &#39;item2&#39;), (&#39;key1&#39;, &#39;item1&#39;)]) 1lst = [1,2,3] 1lst.pop() #randomly pops out last item 3 1lst [1, 2] 1lst.pop(0) 1 1lst [2] 1 1&#x27;x&#x27; in [1,2,3] False 12&#x27;x&#x27; in [&#x27;x&#x27;,&#x27;y&#x27;,&#x27;z&#x27;] True Tuple unpacking: 123x = [(1,2),(3,4), (5,6)]for a,b in x: print(a) 1 3 5 12for a,b in x : print(b); 2 4 6 CS Data science and Machine learning in Python - 1. Review of Python]]></content>
    </entry>

    <entry>
      <title>Python 数据科学与机器学习 - 1. Python 复习</title>
      <link href="/2022/09/11/Python%20Crash%20Course/index-zh.html"/>
      <url>/2022/09/11/Python%20Crash%20Course/index-zh.html</url>
      <content type="html"><![CDATA[Python 综合语法回顾 数据类型 数字 字符串 打印 列表 字典 布尔值 元组 集合 比较运算符 if/elif/else 语句 for 循环 while 循环 range() 列表推导式 函数 lambda 表达式 map 和 filter 方法 数据类型数字11 + 1 2 11 * 3 3 11 / 2 0.5 12 ** 4 16 14 % 2 0 15 % 2 1 1(2 + 3) * (5 + 5) 50 变量赋值12# 不能以数字或特殊字符开头name_of_var = 2 12x = 2y = 3 1z = x + y 1z 5 字符串1&#x27;single quotes&#x27; &#39;single quotes&#39; 1&quot;double quotes&quot; &#39;double quotes&#39; 1&quot; wrap lot&#x27;s of other quotes&quot; &quot; wrap lot&#39;s of other quotes&quot; 打印1x = &#x27;hello&#x27; 1x &#39;hello&#39; 1print(x) hello 12num = 12name = &#x27;Sam&#x27; 1print(&#x27;My number is: &#123;one&#125;, and my name is: &#123;two&#125;, and more &#123;one&#125;&#x27;.format(one=num,two=name)) My number is: 12, and my name is: Sam, and more 12 1print(&#x27;My number is: &#123;&#125;, and my name is: &#123;&#125;&#x27;.format(num,name)) My number is: 12, and my name is: Sam 列表1[1,2,3] [1, 2, 3] 1[&#x27;hi&#x27;,1,[1,2]] [&#39;hi&#39;, 1, [1, 2]] 1my_list = [&#x27;a&#x27;,&#x27;b&#x27;,&#x27;c&#x27;] 1my_list.append(&#x27;d&#x27;) 1my_list [&#39;a&#39;, &#39;b&#39;, &#39;c&#39;, &#39;d&#39;] 1my_list[0] &#39;a&#39; 1my_list[1] &#39;b&#39; 1my_list[1:] [&#39;b&#39;, &#39;c&#39;, &#39;d&#39;] 1my_list[:1] [&#39;a&#39;] 1my_list[0] = &#x27;NEW&#x27; 1my_list [&#39;NEW&#39;, &#39;b&#39;, &#39;c&#39;, &#39;d&#39;] 1nest = [1,2,3,[4,5,[&#x27;target&#x27;]]] 1nest[3] [4, 5, [&#39;target&#39;]] 1nest[3][2] [&#39;target&#39;] 1nest[3][2][0] &#39;target&#39; 字典1d = &#123;&#x27;key1&#x27;:&#x27;item1&#x27;,&#x27;key2&#x27;:&#x27;item2&#x27;&#125; 1d &#123;&#39;key1&#39;: &#39;item1&#39;, &#39;key2&#39;: &#39;item2&#39;&#125; 1d[&#x27;key1&#x27;] &#39;item1&#39; 1d[&#x27;key 3&#x27;] = &#x27;item3&#x27; 1d[&#x27;key 3&#x27;] &#39;item3&#39; 支持嵌套字典 1d2 = &#123;&#x27;k1&#x27;:d, &#x27;k2&#x27;: d&#125; 1print(d2) # 字典不保证顺序，只存储键和值。 &#123;&#39;k1&#39;: &#123;&#39;key1&#39;: &#39;item1&#39;, &#39;key2&#39;: &#39;item2&#39;, &#39;key 3&#39;: &#39;item3&#39;&#125;, &#39;k2&#39;: &#123;&#39;key1&#39;: &#39;item1&#39;, &#39;key2&#39;: &#39;item2&#39;, &#39;key 3&#39;: &#39;item3&#39;&#125;&#125; 布尔值1True True 1False False 元组1t = (1,2,3) 1t[0] 1 元组是不可变的。 你不能修改元组中的元素，示例如下。 1t[0] = &#x27;NEW&#x27; --------------------------------------------------------------------------- TypeError Traceback (most recent call 最后一个) &lt;ipython-input-44-97e4e33b36c2&gt; in &lt;module&gt;() ----&gt; 1 t[0] = &#39;NEW&#39; TypeError: &#39;tuple&#39; object does not support item assignment 集合1&#123;1,2,3&#125; &#123;1, 2, 3&#125; set 会返回去重后的集合 1&#123;1,2,3,1,2,1,2,3,3,3,3,2,2,2,1,1,2&#125; &#123;1, 2, 3&#125; 集合相关方法： 1s = &#123;1,2,3&#125; 12s.add(5)print(s) &#123;1, 2, 3, 5&#125; 12s.remove(5) # 注意：remove/add 会原地修改集合 s。print(s) &#123;1, 2, 3&#125; 1 比较运算符11 &gt; 2 False 11 &lt; 2 True 11 &gt;= 1 True 11 &lt;= 4 True 11 == 1 # &#x27;=&#x27; is an assignment operator True 1&#x27;hi&#x27; == &#x27;bye&#x27; False 逻辑运算符1(1 &gt; 2) 与 (2 &lt; 3) False 1(1 &gt; 2) 或 (2 &lt; 3) True 1(1 == 2) 或 (2 == 3) 或 (4 == 4) True if/elif/else 语句12if 1 &lt; 2: print(&#x27;对！&#x27;) 对！ 12if 1 &lt; 2: print(&#x27;对！&#x27;) 对！ 1234if 1 &lt; 2: print(&#x27;第一个&#x27;)否则: print(&#x27;最后一个&#x27;) 第一个 1234if 1 &gt; 2: print(&#x27;第一个&#x27;)否则: print(&#x27;最后一个&#x27;) 最后一个 123456if 1 == 2: print(&#x27;第一个&#x27;)否则如果 3 == 3: print(&#x27;中间&#x27;)否则: print(&#x27;Last&#x27;) 中间 另外： 1print((3&gt;2) + 1); # 3&gt;2 gives us true, and true + 1 gives us 2. 2 1print((3&gt;4) + 1); 1 Follwoing example shows 1 and 0 are the embeded val for bool. 1234if 0: print(&quot;bool 为 false&quot;);if 1: print(&#x27;bool 为 true&#x27;); bool 为 true for 循环1seq = [1,2,3,4,5] 12for item in seq: # for 循环允许你遍历序列； print(item) 1 2 3 4 5 12for item in seq: print(&#x27;Yep&#x27;) Yep Yep Yep Yep Yep 12for 果冻 in seq: print(果冻+果冻) 2 4 6 8 10 while 循环1234i = 1while i &lt; 5: print(&#x27;i is: &#123;&#125;&#x27;.format(i)) i = i+1 i is: 1 i is: 2 i is: 3 i is: 4 range()1range(5) range(0, 5) 12for i in range(5): print(i) 0 1 2 3 4 1list(range(5)) # 让它成为一个序列； [0, 1, 2, 3, 4] 1list(range(0,10,2)) #making a list from 0 to 10 with 2 stepss [0, 2, 4, 6, 8] 列表推导式1x = [1,2,3,4] 1234out = []for item in x: out.append(item**2)print(out) [1, 4, 9, 16] 1[item**2 for item in x] # 列表推导式 [1, 4, 9, 16] 函数12345def my_func(param1=&#x27;default&#x27;): &quot;&quot;&quot; 文档字符串写在这里。 &quot;&quot;&quot; print(param1) 1my_func # 返回对象本身 &lt;function __main__.my_func&gt; 1my_func() # 调用函数 default 1my_func(&#x27;新参数&#x27;) 新参数 1my_func(param1=&#x27;新参数&#x27;) 新参数 12def 平方(x): return x**2 1out = square(2) 1print(out) 4 lambda 表达式12def times2(var): return var*2 1times2(2) 4 1lambda var: var*2 &lt;function __main__.&lt;lambda&gt;(var)&gt; 12t = lambda var : var*2print(t(2)); 4 map 和 filter1seq = [1,2,3,4,5] 1map(times2,seq) # 将函数映射到列表中的每个元素 &lt;map at 0x105316748&gt; 1list(map(times2,seq)) [2, 4, 6, 8, 10] 1list(map(lambda var: var*2,seq)) # lambda 表达式可以减少代码书写 [2, 4, 6, 8, 10] 1filter(lambda item: item%2 == 0,seq) &lt;filter at 0x105316ac8&gt; 1list(filter(lambda item: item%2 == 0,seq)) # if 函数对每个元素进行判断 [2, 4] 方法1st = &#x27;hello my name is Sam&#x27; 1st.lower() # 你可以输入 st. 查看所有可用方法 &#39;hello my name is sam&#39; 1st.upper() &#39;HELLO MY NAME IS SAM&#39; 1st.split() # splits the string on the white space if there&#x27;s no input. [&#39;hello&#39;, &#39;my&#39;, &#39;name&#39;, &#39;is&#39;, &#39;Sam&#39;] 1tweet = &#x27;Go Sports! #Sports&#x27; 1tweet.split(&#x27;#&#x27;) [&#39;Go Sports! &#39;, &#39;Sports&#39;] 1tweet.split(&#x27;#&#x27;)[1] &#39;Sports&#39; 1d &#123;&#39;key1&#39;: &#39;item1&#39;, &#39;key2&#39;: &#39;item2&#39;&#125; 1d.keys() dict_keys([&#39;key2&#39;, &#39;key1&#39;]) 1d.items() dict_items([(&#39;key2&#39;, &#39;item2&#39;), (&#39;key1&#39;, &#39;item1&#39;)]) 1lst = [1,2,3] 1lst.pop() # 弹出最后一个元素 3 1lst [1, 2] 1lst.pop(0) 1 1lst [2] 1 1&#x27;x&#x27; in [1,2,3] False 12&#x27;x&#x27; in [&#x27;x&#x27;,&#x27;y&#x27;,&#x27;z&#x27;] True 元组解包： 123x = [(1,2),(3,4), (5,6)]for a,b in x: print(a) 1 3 5 12for a,b in x : print(b); 2 4 6 计算机科学 Python 数据科学与机器学习 - 1. Python 复习]]></content>
    </entry>

    <entry>
      <title>READ ME</title>
      <link href="/2022/08/08/README/"/>
      <url>/2022/08/08/README/</url>
      <content type="html"><![CDATA[Note on blog reading Each blog post has categories. The main categories are: About Me, AI, CS, Math, and Quant, and the last category indicates the topic name. Each main category page includes the first post of each topic. Note: Quant stands for quantitative finance; Math stands for mathematics; CS stands for computer science; AI stands for artificial intelligence; About Me is the personal section. Posts in each topic are differentiated by a suffix composed of order number and content name, which helps you locate and read posts. More will be coming soon here About Me READ ME]]></content>
    </entry>

    <entry>
      <title>阅读说明</title>
      <link href="/2022/08/08/README/index-zh.html"/>
      <url>/2022/08/08/README/index-zh.html</url>
      <content type="html"><![CDATA[阅读说明 每篇博客都有类别。主类别包括：关于我、人工智能、计算机科学、数学、量化，最后一个类别表示具体主题名称。 每个主类别页面只收录各主题的第一篇文章。 说明：Quant 代表量化金融；Math 代表数学；CS 代表计算机科学；AI 代表人工智能；About Me 为个人介绍。 每个主题的文章会用后缀区分，由序号与内容名称组成，方便定位阅读。 更多内容即将更新 关于我 阅读说明]]></content>
    </entry>

    <entry>
      <title>1.Basics of Financial Markets</title>
      <link href="/2022/08/25/Quantitative-Trading/"/>
      <url>/2022/08/25/Quantitative-Trading/</url>
      <content type="html"><![CDATA[Basics of Financial MarketsStockBasics of Stock Stock is the ownership of a small piece of a company This entitles the owner of the stock to a proportion of the corporation’s asset and profits equal to how much stock they own Shares are the units of stocks Investment can be as shares of the company Price of Stock General concept of how stock works: companies raise capital by borrowing or issuing bonds or go public Companies have to issue stock via the Stock Exchange, which has a list of companies that want to raise capital Brokers firms can provide the platform where investors can buy or sell stocks Stock price fluctuates mainly due to supply and demand Stock price is usually expressed as S(t) and very similar to a Random Walk The risk of investment correlates the volatility of stock price, measure of the dispersion Commodities Commodities are raw products such as gold, oil, natural gas Commodities, such as oil, are EXTREMELY VOLATILE, which is why we have future contracts Commodity usually fluctuates like random walk Commodity typically rise when inflation is accelerating, which is why we include it in portfolio Future Contract Example: The major cost for airlines is oil, so they use oil futures to protect them from rising oil prices Future Contracts and commodities are traded in Future Market, such as New York Mercantile Exchange, Primary Market What if we cannot invest into a commodity directly Invest into a company that heavily relies on the given commodity Example: gold and shares of gold mines may be correlated It is also positively correlated with companies’ performance so that we Use Pairs Trading Strategy Currencies and ForexCurrencies Definiton Exchange rate is the rate at which one national currency will be exchanged for another Currency is worth another currency Government and central banks can influence currencies and exchange rates Currency TraitsWhy do exchange rates fluctuate? Exchange rate rise and fall due to fluctuation of supply and demand Exchange rates are usually similar to a random walk FOREX Market International Currencies are traded in FOREX market Investors use FOREX Broker Firms that have the ability to buy and sell currencies. Factors affecting exchange rates interest rates: interest rate is major factor that can be manipulated by the central bank of a country. Investors will lend money to the banks of the given country for higher returns Therefore, higher interest rates indicate higher exchange rates Money supply: the money supply created by the central bank by printing too much currency may trigger inflation. Investors do not like inflation so they will leave the currency that can push the value of a currency down Therefore, higher money supply may lead to lower exchange rates Financial stability: the financial stability and economic growth of a given country have a huge impact on the value of the exchange rate Therefore, better economic performance leads to higher exchange rate There are arbitrage opportunities when currencies are mispriced Long and Short position Long Position: You own the security, and investors maintain long position in the expectation that the stock will increase in value in the future, such that: S(tT) > S(t0) Short position means you sell the security, with expectation that stock will decrease such that: S(tT) 0) Short Position leads to another action: Short Selling Short Selling: sell something you don’t actually own Short Selling is when an investor borrows shares and immediately sells them, expecting he or she can buy them up later at a lower price and return them to the lender and pocket the difference, and the profit will be: S(t0) - S(tT), given that S(t0) > S(tT) Risks with Short and Long Positions Shorting is more risker than long positions For example, for lone position, you can only lose 100% of your money; however, there is no limit of how much you can lose for short selling, because there is no limit for a given stock to increase, and the lose of short position will be: S(tT) - S(t0), given that S(t0) T) S(tT) could go really high meaning you have to pay more to buy it back and return Market Basics Bear-ish Market: is when the market experiences stable prices declines Therefore, securities fall for a sustained period of time, where we can make profit with short positions Bull-ish Market: is when the market experiences stable price increase, where we can make profit with long positions Quant READ ME 1.Basics of Financial Markets]]></content>
    </entry>

    <entry>
      <title>1. 金融市场基础</title>
      <link href="/2022/08/25/Quantitative-Trading/index-zh.html"/>
      <url>/2022/08/25/Quantitative-Trading/index-zh.html</url>
      <content type="html"><![CDATA[金融市场基础股票股票基础 股票代表对公司的一小部分所有权 这使股东按持股比例分享公司的资产与利润 股票的单位是“股” 投资可以通过购买公司的股份来实现 股票价格 股票运作的基本概念：公司通过借款、发行债券或上市来融资 公司通过证券交易所发行股票，交易所列出希望融资的公司 券商为投资者提供买卖股票的平台 股票价格主要受供求影响而波动 Stock price is usually expressed as S(t) and very similar to a 随机游走 投资风险与股价波动率相关， 波动率衡量离散程度 大宗商品 大宗商品是原材料，例如 黄金、原油、天然气 商品（如原油）具有 极高的波动性，因此我们需要 期货合约 商品价格通常像随机游走一样波动 通胀加速时商品通常会上涨，因此会被纳入投资组合 期货合约 例如：航空公司的主要成本是燃油，因此他们使用 原油期货 来对冲油价上涨 期货合约和商品在期货市场交易，例如纽约商品交易所（NYMEX）， 主市场 如果无法直接投资商品 投资于高度依赖该商品的公司 例如：黄金价格与金矿公司股票可能相关 它与公司业绩也可能正相关，因此可以使用配对交易策略 货币与外汇货币定义 汇率是一个国家货币兑换另一种货币的比率 一种货币对另一种货币具有价值 政府和央行可以影响货币与汇率 货币特征汇率为什么会波动？ 汇率的升跌取决于 供求变化 汇率通常类似于 随机游走 外汇市场 国际货币在外汇市场交易 投资者通过 外汇经纪商 进行货币买卖。 影响汇率的因素 利率：利率是可被 该国央行调控的关键因素。投资者会把资金借给该国银行以获取更高回报 因此，更高的利率通常意味着更高的汇率 货币供应量：央行过度印钞会引发通胀。投资者不喜欢通胀，会抛售该货币，从而压低其价值 因此，货币供应量增加可能导致汇率下降 金融稳定：一国的金融稳定与 经济增长 对该国汇率有 巨大影响 因此，经济表现越好，汇率越高 当货币定价偏离时会出现套利机会 多头与空头头寸 多头头寸：投资者持有证券，预期其未来上涨，因此： S(tT) > S(t0) 空头头寸：投资者卖出证券，预期其价格下跌，因此： S(tT) 0) 空头头寸会引出另一种操作： 卖空 卖空：卖出并不实际持有的资产 卖空 是指 投资者借入股票并立即卖出，期望以后以更低价格买回归还并赚取差价，利润为： S(t0) - S(tT), given that S(t0) > S(tT) 多空头寸的风险 做空 风险高于 做多 For example, for lone position, you can only lose 100% of your money; however, 做空的亏损没有上限，因为 没有上限 对单只股票的上涨而言，空头亏损为： S(tT) - S(t0), given that S(t0) T) S(tT) could go really high meaning you have to pay more to buy it back and return 市场基础 熊市：当市场经历 持续的价格下跌 因此证券在一段时间内下跌，我们可以 通过做空获利 牛市：当市场经历 持续上涨时，我们可以 通过做多获利 量化 说明 1. 金融市场基础]]></content>
    </entry>

    <entry>
      <title>1.Basics of Financial Markets</title>
      <link href="/2026/02/05/Numeric-Methods/"/>
      <url>/2026/02/05/Numeric-Methods/</url>
      <content type="html"><![CDATA[Summary of Chapter 1: Convolution Operators and Kernel Operators CNNs learn hierarchical features from local receptive fields. 1. Convolutional Neural Networks (CNNs) Origins and Definition: Introduced by LeCun in 1989, CNNs are specialized networks for grid-like data such as time-series (1‑D grids) and images (2‑D grids). Core Operation: CNNs replace general matrix multiplication with convolution (or cross‑correlation) in at least one layer to extract features. Cross‑Correlation vs. Convolution: Most frameworks implement cross‑correlation (no kernel flip) for simplicity, with no loss in performance. Key Advantages of Convolution in ML Sparse interaction: Each output depends on a small subset of inputs. Parameter sharing: The same kernel is reused across spatial locations. Equivariance: Feature maps shift in response to input shifts. CNN Architecture Inspired by the visual cortex and receptive fields. Supports multi‑channel inputs (e.g., RGB) with depth‑matched kernels. Stacks layers to capture hierarchical features from edges to objects. Padding and Stride Same Padding: Output size matches input size. Valid Padding: No padding; output shrinks. Stride: Step size of the kernel movement. Pooling Layers Reduces spatial dimensions and computation. Improves translation invariance. Max Pooling suppresses noise better than Average Pooling. Fully‑Connected Layers Added near the end to combine extracted features for classification (e.g., Softmax). Popular CNN Architectures LeNet, AlexNet, VGGNet, GoogLeNet, ResNet, ZFNet. 2. Kernel Methods Motivation: Handle diverse data types with pairwise comparisons. Kernel Trick: Implicitly compute inner products in high‑dimensional spaces. Representer Theorem: Optimal solutions lie in the span of kernel evaluations. Kernels define similarity without explicit feature mapping. Positive Definite Kernels Symmetric functions \(K(x, x')\) that are positive semi‑definite. Examples: Linear, Polynomial, Gaussian RBF kernels. Kernels can be added, multiplied, and scaled while preserving p.d. property. Kernel Methods in Supervised Learning Learn \(f: X o Y\) by minimizing empirical risk + regularization. Support regression, classification, and structured output tasks. Kernel Ridge Regression (KRR) \(\hat{f}(x) = \sum_{i=1}^n lpha_i K(x_i, x)\) \(\min_{lpha \in \mathbb{R}^n} rac{1}{n} \|K lpha - y\|^2 + \lambda lpha^T K lpha\) Timeline Table of Key Concepts YearEvent/ConceptDescription 1989Introduction of CNNs by LeCunCNNs introduced for grid‑like data processing 1950Aronszajn Theorem (p.d. Kernels)Characterization of p.d. kernels via Hilbert spaces —Development of Kernel MethodsKernel trick and representer theorem in learning Key Definitions and Concepts in Table Form TermDefinition/Description ConvolutionKernel sliding over input to extract features. Cross‑CorrelationConvolution without kernel flipping. PoolingDownsampling for spatial invariance. Positive Definite KernelSymmetric \(K\) with \(\sum_{i,j} a_i a_j K(x_i, x_j) \ge 0\). Kernel TrickImplicit inner products in high‑dimensional space. Representer TheoremSolution lies in span of kernel evaluations. KRRRegularized least squares with kernels. Key Insights CNNs leverage local connectivity and parameter sharing for efficiency. Kernels enable nonlinear learning in rich feature spaces. Representer theorem reduces infinite‑dimensional optimization to finite form. Regularization is critical to prevent overfitting. Kernel/parameter selection remains an active research area. Conclusion This chapter introduces convolution operators (CNNs) and kernel operators as two foundational ML frameworks. CNNs excel in structured data via spatial hierarchies and parameter efficiency. Kernel methods provide a rigorous, flexible approach to nonlinear learning through p.d. kernels, Hilbert spaces, and the kernel trick. Math 1. Convolution and Kernel Operators]]></content>
    </entry>

    <entry>
      <title>1.Basics of Financial Markets</title>
      <link href="/2026/02/05/Numeric-Methods/index-zh.html"/>
      <url>/2026/02/05/Numeric-Methods/index-zh.html</url>
      <content type="html"><![CDATA[第一章总结：卷积算子与核算子 CNN 通过局部感受野学习层级特征。 1. 卷积神经网络（CNN） 起源与定义：LeCun 于 1989 年提出 CNN，用于处理网格结构数据，如时间序列（1D）与图像（2D）。 核心操作：至少在一层用卷积（或互相关）替代矩阵乘法以提取特征。 互相关 vs 卷积：多数框架实现互相关（不翻转核），简化实现而不影响效果。 卷积在机器学习中的关键优势 稀疏连接：每个输出仅依赖少量输入。 参数共享：同一卷积核在不同位置共享参数。 等变性：输入平移会导致特征图相应平移。 CNN 架构 模仿视觉皮层神经元与感受野机制。 支持多通道输入（如 RGB），卷积核深度与输入一致。 多层卷积捕捉从边缘到对象的层级特征。 填充与步幅 Same Padding：输出尺寸与输入相同。 Valid Padding：不填充，输出变小。 Stride：卷积核移动的步长。 池化层 降低空间维度与计算量。 增强平移不变性。 最大池化比平均池化更能抑制噪声。 全连接层 通常位于末端，用于组合特征并完成分类（如 Softmax）。 经典 CNN 架构 LeNet、AlexNet、VGGNet、GoogLeNet、ResNet、ZFNet。 2. 核方法 动机：以成对相似度处理多类型数据（向量、字符串、图像等）。 核技巧：在高维空间中隐式计算内积。 表示定理：最优解位于核函数在样本点的线性张成空间。 核函数定义相似度，无需显式特征映射。 正定核 对称且半正定的函数 \(K(x, x')\)。 例：线性核、多项式核、高斯 RBF 核。 核函数可加、可乘、可缩放，保持正定性。 监督学习中的核方法 通过经验风险 + 正则化学习 \(f: X o Y\)。 适用于回归、分类与结构化输出问题。 核岭回归（KRR） \(\hat{f}(x) = \sum_{i=1}^n lpha_i K(x_i, x)\) \(\min_{lpha \in \mathbb{R}^n} rac{1}{n} \|K lpha - y\|^2 + \lambda lpha^T K lpha\) 关键概念时间线 年份事件/概念说明 1989LeCun 引入 CNN用于网格数据处理 1950Aronszajn 定理（正定核）通过希尔伯特空间刻画正定核 —核方法发展核技巧与表示定理在学习中的应用 关键定义与概念 术语定义/说明 卷积卷积核滑动以提取特征。 互相关不翻转卷积核的卷积操作。 池化下采样以增强平移不变性。 正定核满足 \(\sum_{i,j} a_i a_j K(x_i, x_j) \ge 0\) 的对称函数。 核技巧隐式计算高维内积。 表示定理最优解在核函数张成空间内。 KRR带正则化的核回归。 关键洞见 CNN 通过局部连接与参数共享提升效率。 核方法在高维特征空间实现非线性学习。 表示定理将无限维优化降为有限维。 正则化对防止过拟合至关重要。 核与参数选择仍是活跃研究问题。 结论 本章介绍了两类核心方法：卷积算子（CNN）与核算子（核方法）。CNN 利用空间层级结构与参数效率处理结构化数据；核方法通过正定核与核技巧实现灵活的非线性学习。 数学 1. 卷积与核算子]]></content>
    </entry>

    <entry>
      <title>04. Multimodal and Cross-Modal Fusion</title>
      <link href="/2026/02/08/Multimodal/"/>
      <url>/2026/02/08/Multimodal/</url>
      <content type="html"><![CDATA[Multimodal systems integrate text, audio, images, and video to enable cross-media understanding and generation. Core steps include modality-specific feature extraction, mapping to a shared embedding space, and multi-stage fusion (early, mid, late). Applications include image-to-text, text-to-image/audio, and cross-modal generation, with challenges in cost, latency, and long-sequence consistency. AI 04. Multimodal and Cross-Modal Fusion]]></content>
    </entry>

    <entry>
      <title>04. 多模态技术与跨模态融合概述</title>
      <link href="/2026/02/08/Multimodal/index-zh.html"/>
      <url>/2026/02/08/Multimodal/index-zh.html</url>
      <content type="html"><![CDATA[多模态技术整合文本、音频、图像与视频，支持跨媒介理解与生成。关键步骤包括特征提取、统一映射到共享向量空间，以及早期/中期/晚期融合。典型应用为图生文、文生图/音频与跨模态生成，同时面临生成成本高、耗时长与一致性问题等挑战。 人工智能 04. 多模态技术与跨模态融合概述]]></content>
    </entry>

    <entry>
      <title>05. Agent</title>
      <link href="/2026/02/09/Agent/"/>
      <url>/2026/02/09/Agent/</url>
      <content type="html"><![CDATA[Agent is a goal-driven system that combines decision-making, perception, and execution to act in the world. It decomposes tasks, perceives multimodal inputs, and adapts strategies through feedback. Applications include intelligent customer service and office automation, while challenges include compute cost, safety, and continuous iteration. AI 05. Agent]]></content>
    </entry>

    <entry>
      <title>05. Agent 知识文档</title>
      <link href="/2026/02/09/Agent/index-zh.html"/>
      <url>/2026/02/09/Agent/index-zh.html</url>
      <content type="html"><![CDATA[Agent 是目标驱动的智能系统，融合决策、感知与执行来完成任务。它能拆解问题、理解多模态输入，并根据反馈动态优化策略。应用包括智能客服与办公自动化，挑战在于计算成本、安全可控和持续迭代。 人工智能 05. Agent 知识文档]]></content>
    </entry>

    <entry>
      <title>06. Fine-Tuning</title>
      <link href="/2026/02/10/Fine-Tuning/"/>
      <url>/2026/02/10/Fine-Tuning/</url>
      <content type="html"><![CDATA[Fine-tuning specializes a pretrained model for tasks, styles, and controllability. Common strategies include SFT, RLHF, DPO, rejection sampling, model averaging, and LoRA/PEFT. Pretraining provides general knowledge; fine-tuning refines for domain performance. AI 06. Fine-Tuning]]></content>
    </entry>

    <entry>
      <title>06. 微调（Fine-Tuning）知识文档</title>
      <link href="/2026/02/10/Fine-Tuning/index-zh.html"/>
      <url>/2026/02/10/Fine-Tuning/index-zh.html</url>
      <content type="html"><![CDATA[微调让通用模型成为领域专家，常见策略包括 SFT、RLHF、DPO、拒绝采样、模型平均与 LoRA/PEFT。预训练提供基础能力，微调针对任务与风格做专业化优化。 人工智能 06. 微调（Fine-Tuning）知识文档]]></content>
    </entry>

    <entry>
      <title>07. RAG / MCP / A2A</title>
      <link href="/2026/02/11/RAG-MCP-A2A/"/>
      <url>/2026/02/11/RAG-MCP-A2A/</url>
      <content type="html"><![CDATA[Session 5 covers product workflow, memory management, MCP tool calls, A2A collaboration, and RAG system design. It highlights data cleaning, chunking, vectorization, and cost control, plus multi-agent coordination for efficiency. AI 07. RAG / MCP / A2A]]></content>
    </entry>

    <entry>
      <title>07. RAG / MCP / A2A 知识文档</title>
      <link href="/2026/02/11/RAG-MCP-A2A/index-zh.html"/>
      <url>/2026/02/11/RAG-MCP-A2A/index-zh.html</url>
      <content type="html"><![CDATA[Session 5 介绍产品流程、记忆管理、MCP 工具调用、A2A 协作与 RAG 系统设计，强调清理、切块、向量化与成本控制，并通过多智能体协作提升效率。 人工智能 07. RAG / MCP / A2A 知识文档]]></content>
    </entry>

    <entry>
      <title>08. AI Training and Fine-Tuning Strategy Summary</title>
      <link href="/2026/02/12/Training-Strategies-Summary/"/>
      <url>/2026/02/12/Training-Strategies-Summary/</url>
      <content type="html"><![CDATA[Summary of training strategies: pretraining, fine-tuning (SFT, LoRA/QLoRA), alignment (DPO/RLHF), RAG, distillation, base model selection, and implementation steps. AI 08. AI Training and Fine-Tuning Strategy Summary]]></content>
    </entry>

    <entry>
      <title>08. AI 模型训练与微调策略知识文档</title>
      <link href="/2026/02/12/Training-Strategies-Summary/index-zh.html"/>
      <url>/2026/02/12/Training-Strategies-Summary/index-zh.html</url>
      <content type="html"><![CDATA[总结预训练、微调（SFT、LoRA/QLoRA）、对齐（DPO/RLHF）、RAG、蒸馏/剪枝、底座选择与实施步骤，强调依据问题选择策略并提升体验。 人工智能 08. AI 模型训练与微调策略知识文档]]></content>
    </entry>

    <entry>
      <title>09. Prompt Engineering &amp; Hallucination</title>
      <link href="/2026/02/13/Prompt-Engineering/"/>
      <url>/2026/02/13/Prompt-Engineering/</url>
      <content type="html"><![CDATA[Prompt engineering notes: CROFTC vs CO-STAR, prompting tips, ICL, CoT/CoD, P-tuning, ultimate prompt, RAG, and hallucination mitigation. AI 09. Prompt Engineering & Hallucination]]></content>
    </entry>

    <entry>
      <title>09. 提示词工程与幻觉</title>
      <link href="/2026/02/13/Prompt-Engineering/index-zh.html"/>
      <url>/2026/02/13/Prompt-Engineering/index-zh.html</url>
      <content type="html"><![CDATA[提示词工程笔记：CROFTC 与 CO-STAR、提示词技巧、ICL、CoT/CoD、P-tuning、终极提示词、RAG 与幻觉缓解。 人工智能 09. 提示词工程与幻觉]]></content>
    </entry>

    <entry>
      <title>01. Deep Learning Basics</title>
      <link href="/2026/02/14/Deep-Learning-Basics/"/>
      <url>/2026/02/14/Deep-Learning-Basics/</url>
      <content type="html"><![CDATA[Deep learning basics covering loss functions, gradient descent, overfitting, weight regularization, dropout, and early stopping. AI 01. Deep Learning Basics]]></content>
    </entry>

    <entry>
      <title>01. 深度学习基础</title>
      <link href="/2026/02/14/Deep-Learning-Basics/index-zh.html"/>
      <url>/2026/02/14/Deep-Learning-Basics/index-zh.html</url>
      <content type="html"><![CDATA[深度学习基础内容：常见损失函数、梯度下降、过拟合、权重正则化、Dropout 与早停法。 人工智能 01. 深度学习基础]]></content>
    </entry>

    <entry>
      <title>02. Convolutional Neural Networks</title>
      <link href="/2026/02/15/Convolutional-Neural-Network/"/>
      <url>/2026/02/15/Convolutional-Neural-Network/</url>
      <content type="html"><![CDATA[CNN notes covering convolutional layers, padding, translation invariance, pooling, dense layers, computation, and visualization methods. AI 02. Convolutional Neural Networks]]></content>
    </entry>

    <entry>
      <title>02. 卷积神经网络</title>
      <link href="/2026/02/15/Convolutional-Neural-Network/index-zh.html"/>
      <url>/2026/02/15/Convolutional-Neural-Network/index-zh.html</url>
      <content type="html"><![CDATA[卷积神经网络笔记：卷积层、填充、平移不变性、池化、全连接层、计算与可视化方法。 人工智能 02. 卷积神经网络]]></content>
    </entry>

    <entry>
      <title>05. Transformer</title>
      <link href="/2026/02/16/Transformer-Notes/"/>
      <url>/2026/02/16/Transformer-Notes/</url>
      <content type="html"><![CDATA[Transformer notes with Q/K/V example, training vs inference, teacher forcing, architecture, attention types, masks, feedforward, and advantages. AI 05. Transformer]]></content>
    </entry>

    <entry>
      <title>05. Transformer</title>
      <link href="/2026/02/16/Transformer-Notes/index-zh.html"/>
      <url>/2026/02/16/Transformer-Notes/index-zh.html</url>
      <content type="html"><![CDATA[Transformer 笔记：Q/K/V 示例、训练与推理、Teacher Forcing、架构、注意力类型与掩码、前馈网络与优势。 人工智能 05. Transformer]]></content>
    </entry>


    <entry>
      <title>04. Generative Models</title>
      <link href="/2026/02/17/Generative-Models/"/>
      <url>/2026/02/17/Generative-Models/</url>
      <content type="html"><![CDATA[Generative models overview: DeepDream, VAE, GAN, cGAN, adversarial examples, and adversarial training. AI 04. Generative Models]]></content>
    </entry>

    <entry>
      <title>04. 生成模型</title>
      <link href="/2026/02/17/Generative-Models/index-zh.html"/>
      <url>/2026/02/17/Generative-Models/index-zh.html</url>
      <content type="html"><![CDATA[生成模型概览：DeepDream、VAE、GAN、cGAN、对抗样本与对抗训练。 人工智能 04. 生成模型]]></content>
    </entry>


    <entry>
      <title>01. Decision Tree and Random Forest</title>
      <link href="/2026/02/10/Decision-Tree-and-Random-Forest/"/>
      <url>/2026/02/10/Decision-Tree-and-Random-Forest/</url>
      <content type="html"><![CDATA[01. Decision Tree and Random Forest | Rongyan's Blog Rongyan's Blog ← Data Mining and Text Analytics HOME BLOG CATEGORIES AboutMe AI CS Math Quant EN | 中文 Rongyan's Blog HOME BLOG EN | 中文 01. Decision Tree and Random Forest Feb 10 2026 Study Note: Decision Tree 1. Overview: What is a Decision Tree and What Are Decision Criteria? A Decision Tree is a supervised learning model used for classification and regression that makes predictions by recursively splitting data based on feature values. At each node, the tree selects a decision criterion (also called an impurity-based criterion) to determine which feature and split best separate the data . The core idea is simple: Choose the split that makes the resulting child nodes as “pure” as possible. Here, purity means: In classification : samples in a node mostly belong to the same class In regression : target values in a node are as close to each other as possible Common decision criteria include: Entropy / Information Gain (information-theoretic uncertainty reduction) Gini Impurity (probability of misclassification) Variance / Mean Squared Error (MSE) (used in regression) 2. Decision Criteria Explained (with Examples) 2.1 Entropy Meaning: Entropy measures how uncertain or mixed the class labels are in a node. Formula: \[ H(S) = -\sum_{k=1}^K p_k \log_2 p_k \] \(p_k\): proportion of class \(k\) \(H = 0\): node is perfectly pure Maximum when classes are evenly mixed Example: A node has 5 positive and 5 negative samples: \[ H = - (0.5\log_2 0.5 + 0.5\log_2 0.5) = 1 \] (high uncertainty) 2.2 Information Gain (IG) Meaning: Information Gain measures how much entropy decreases after a split . Formula: \[ IG(S, A) = H(S) - \sum_{v} \frac{|S_v|}{|S|} H(S_v) \] Example: Parent entropy = 1 Split produces: Left node entropy = 0 (pure) Right node entropy = 0.5 If the split is even: \[ IG = 1 - (0.5 \cdot 0 + 0.5 \cdot 0.5) = 0.75 \] Higher IG ⇒ better split. Summary of entropy and info gain: we choose the best information gain (highest number) for subcategories at each layer to decide which subcategory should be the parent of that layer, where information gain is calculated by entropy(prev layer) - entropy(sub categories) . Below is an example of wheather to play tennis or not: 2.3 Gini Impurity Meaning: Gini impurity measures the probability that a randomly chosen sample is misclassified. Formula: \[ G(S) = 1 - \sum_{k=1}^K p_k^2 \] Example: A node with 7 positives and 3 negatives: \[ G = 1 - (0.7^2 + 0.3^2) = 0.42 \] \(G = 0\): perfectly pure Widely used in CART due to efficiency Following are slides about Gini impurity approach: Perfect split: Imperfect split 2.3.1 Comparison between gini and entropy: Entropy versus Gini Impurity If the data set is completely homogeneous (one class) then the impurity is 0, therefore entropy is 0. It is completely non-homogeneous & impurity is 100%, therefore entropy is 1. Entropy and Gini Impurity give similar results in practice They only disagree in about 2% of cases Entropy might be slower to compute, because of the log 2.4 Variance / MSE (Regression Trees) Meaning: Used when the target is continuous; measures spread of target values. Formula: \[ \text{MSE}(S) = \frac{1}{|S|}\sum_{i \in S}(y_i - \bar y_S)^2 \] Goal: Choose the split that minimizes weighted MSE of child nodes. Example: Targets: \([200, 210, 220, 500]\) Splitting separates low and high prices → variance drops significantly. 3. How a Decision Tree Is Trained Decision tree training is a greedy, top-down, recursive process : Start at the root node with all training samples. Evaluate all possible splits : For every feature For every possible threshold or category subset Compute impurity reduction using a chosen criterion. Select the best split (maximum impurity reduction). Partition the data into child nodes. Repeat recursively on each child node. Stop when: Node is pure Minimum sample size reached Maximum depth reached No significant impurity reduction (Optional) Pruning is applied to reduce overfitting. Key property: Decision trees optimize locally , not globally. 4. Real-World Example: Loan Approval Prediction Problem A bank wants to decide whether to approve or reject a loan based on applicant information. Training Data (Simplified) Income Credit Score Debt Ratio Loan Approved 80k 720 0.2 Yes 45k 650 0.4 No 30k 600 0.5 No 90k 780 0.1 Yes 60k 690 0.3 Yes Training Process All samples start at the root node. The tree evaluates splits such as: `Credit Score > 700` `Income > 50k` `Debt Ratio Suppose `Credit Score > 700` yields the largest impurity reduction. The tree splits and continues training on remaining mixed nodes. Final Learned Tree Credit Score > 700? ├── Yes → Approve Loan └── No ├── Debt Ratio Prediction For a new applicant: Credit Score = 680 Debt Ratio = 0.28 Decision path: Credit Score > 700? → No Debt Ratio → Loan Approved Code Demo: from sklearn.datasets import load_iris from sklearn import tree from matplotlib import pyplot as plt import pandas as pd iris = load_iris() X = iris.data y = iris.target df = pd.DataFrame(list(zip(X, y)), columns =['Features', 'Label']) df #build decision tree clf = tree.DecisionTreeClassifier(criterion='entropy', max_depth=4,min_samples_leaf=4) #max_depth represents max level allowed in each tree, min_samples_leaf minumum samples storable in leaf node #fit the tree to iris dataset clf.fit(X,y) #plot decision tree fig, ax = plt.subplots(figsize=(6, 6)) #figsize value changes the size of plot tree.plot_tree(clf,ax=ax,feature_names=['sepal length','sepal width','petal length','petal width']) plt.show() Key Takeaway A decision tree learns interpretable rules by greedily selecting feature splits that reduce impurity, making it a powerful and transparent model for both classification and regression tasks. Stop here and think: overfiting! Now we introduce pruing!"> From here you may wonder what if our tree is too complex --> overfiting! Now we introduce pruing! Pruning is a technique that reduces the size of a decision tree by removing branches of the tree which provide little predictive power It is a regularization method that reduces the complexity of the final model, thus reducing overfitting Pruning methods: Pre-pruning: Stop the tree building algorithm before it fully classifies the data: Entropy (or Gini Impurity) of the current set Number of samples in the current set Gain of the best-splitting attribute Depth of the tree Post-pruning: Build the complete tree, then replace some non-leaf nodes with leaf nodes if this improves validation error Decision Tree alos: Handling of missing values How ID3 Handles Numerical Attributes 1. Short answer (core idea) ID3 handles numerical attributes by converting them into binary categorical splits using a threshold, then applying Information Gain to choose the best threshold. In other words, ID3 does not treat numerical attributes directly as continuous . Instead, it discretizes them into a yes/no question of the form: \[ x_j \le t \quad \text{vs.} \quad x_j > t \] 2. Why ID3 needs special handling for numerical attributes The original ID3 algorithm was designed for: Categorical (discrete) features Using Entropy and Information Gain Problem with numerical attributes: Infinite possible values Cannot directly enumerate “all values” like categories So ID3 must transform numerical attributes into categorical decisions . 3. Step-by-step: How ID3 handles a numerical attribute Assume we have: Numerical feature \(x\) Target label \(y\) Step 1: Sort the data by the numerical attribute Example: Value \(x\) Class \(y\) 20 No 30 No 40 Yes 60 Yes Step 2: Generate candidate thresholds ID3 considers midpoints between consecutive values : \[ t_1 = \frac{20+30}{2}=25,\quad t_2 = \frac{30+40}{2}=35,\quad t_3 = \frac{40+60}{2}=50 \] Step 3: Convert each threshold into a binary split For each \(t\): x ≤ t vs. x > t Example for \(t=35\): Left: {20, 30} Right: {40, 60} Step 4: Compute Information Gain for each threshold For each split: \[ IG(t) = H(S) - \left( \frac{|S_L|}{|S|}H(S_L) + \frac{|S_R|}{|S|}H(S_R) \right) \] ID3 computes this exactly the same way as for categorical features. Step 5: Choose the best threshold \[ t^* = \arg\max_t IG(t) \] The numerical attribute is then treated as a binary categorical attribute at that node. 4. Simple example Feature: Age Target: Play Tennis Candidate split: Age ≤ 35? ├── Yes → Mostly No └── No → Mostly Yes This split is chosen if it gives the highest information gain . 5. More detailed example (with entropy) Suppose parent node has: 6 Yes, 4 No Entropy: \[ H(S)= -0.6\log_2 0.6 - 0.4\log_2 0.4 = 0.971 \] Split at \(t=35\): Left: 4 No → entropy = 0 Right: 6 Yes → entropy = 0 Information Gain: \[ IG = 0.971 - 0 = 0.971 \] 👉 Perfect split → ID3 selects it. 6. Important characteristics of ID3’s approach What ID3 does well Simple and intuitive Uses same criterion (Information Gain) everywhere Produces interpretable threshold rules Limitations Greedy and local Sensitive to noise Repeated threshold testing is computationally expensive Information Gain is biased toward many-valued attributes 7. Comparison with other tree algorithms Algorithm Numerical handling ID3 Binary threshold + IG C4.5 Threshold + Gain Ratio CART Threshold + Gini / MSE LightGBM Histogram-based thresholds 8. Summary ID3 handles numerical attributes by sorting values, testing candidate threshold-based binary splits, and selecting the threshold that maximizes Information Gain. Pros and Cons of Decision tree: Pros: Compared to other algorithms decision trees requires less effort for data preparation during pre-processing. A decision tree does not require normalization of data. A decision tree does not require scaling of data as well. Missing values in the data also do NOT affect the process of building a decision tree to any considerable extent. A Decision tree model is very intuitive and easy to explain to technical teams as well as stakeholders. It can handle outlier problem. Random Forest can be used to solve both classification as well as regression problems. Decision Tree can handle both continuous and categorical variables. Cons: It has overfitting problem as the nodes were spited in detail. A small change in the data can cause a large change in the structure of the decision tree causing instability. For a Decision tree sometimes calculation can go far more complex compared to other algorithms. Decision tree often involves higher time to train the model. Decision tree training is relatively expensive as the complexity and time has taken are more. The Decision Tree algorithm is inadequate for applying regression and predicting continuous values . Random Forest: RF Algo Diversity- When creating an individual tree, not all qualities, variables, or features are taken into account; each tree is unique. Immune to dimensionality constraint- The feature space is minimized because each tree does not consider all features. Parallelization- Each tree is built from scratch using different data and properties. This means we can fully utilize the CPU to create random forests. Train-Test split- In a random forest, there is no need to separate the data for train and test because the decision tree will always miss 30% of the data. Stability- The result is stable because it is based on majority voting/averaging. How Random Forest Classifier is different from decision trees Overfitting - Overfitting is not there as in Decision trees since random forests are formed from subsets of data, and the final output is based on average or majority rating. Speed - Random Forest Algorithm is relatively slower than Decision Trees. Process - Random forest collects data at random, forms a decision tree, and averages the results. It does not rely on any formulas as in Decision trees. RF code demo from sklearn.ensemble import RandomForestClassifier from sklearn.model_selection import train_test_split from sklearn.metrics import accuracy_score from sklearn.datasets import load_breast_cancer # Load example dataset X, y = load_breast_cancer(return_X_y=True) # Train / test split X_train, X_test, y_train, y_test = train_test_split( X, y, test_size=0.2, random_state=42 ) # Build Random Forest rf = RandomForestClassifier( n_estimators=100, # number of trees max_depth=None, # let trees grow fully min_samples_split=2, # minimum samples to split random_state=42 ) # Train rf.fit(X_train, y_train) # Predict y_pred = rf.predict(X_test) # Evaluate print("Accuracy:", accuracy_score(y_test, y_pred)) 1. Overview: What is a Decision Tree and What Are Decision Criteria? 2. Decision Criteria Explained (with Examples) 3. How a Decision Tree Is Trained 4. Real-World Example: Loan Approval Prediction Code Demo: Key Takeaway Stop here and think: Decision Tree alos: Handling of missing values 1. Short answer (core idea) 2. Why ID3 needs special handling for numerical attributes 3. Step-by-step: How ID3 handles a numerical attribute 4. Simple example 5. More detailed example (with entropy) 6. Important characteristics of ID3’s approach 7. Comparison with other tree algorithms 8. Summary Pros and Cons of Decision tree: Random Forest: Artificial Intelligence | Financial Engineering | Mathematics | Computer Science Rongyan 2026]]></content>
    </entry>

    <entry>
      <title>01. 决策树与随机森林</title>
      <link href="/2026/02/10/Decision-Tree-and-Random-Forest/index-zh.html"/>
      <url>/2026/02/10/Decision-Tree-and-Random-Forest/index-zh.html</url>
      <content type="html"><![CDATA[01. 决策树与随机森林 | 榕言的博客 榕言的博客 ← 数据挖掘与文本分析 首页 博客 分类 关于我 人工智能 计算机科学 数学 量化 EN | 中文 榕言 首页 博客 EN | 中文 01. 决策树与随机森林 2026-02-10 学习笔记：决策树 1. 概述：什么是决策树？什么是决策准则？ 决策树（Decision Tree） 是一种用于 分类与回归 的监督学习模型，它通过递归地按照特征值划分数据来进行预测。 在每个节点，树会选择一个 决策准则 （也叫“基于不纯度的准则”），用来决定 哪个特征、哪个划分能最好地分开数据 。 核心思想很简单： 选择使子节点尽可能“纯”的划分。 这里的“纯度”含义： 在 分类 中：节点内样本尽量属于同一类别 在 回归 中：节点内目标值尽量彼此接近 常见决策准则包括： 熵 / 信息增益（Entropy / Information Gain） （信息论中的不确定性下降） 基尼不纯度（Gini Impurity） （误分类概率） 方差 / 均方误差（Variance / MSE） （用于回归树） 2. 决策准则详解（含例子） 2.1 熵（Entropy） 含义： 熵衡量一个节点中类别标签有多“混乱/不确定”。 公式： \[ H(S) = -\sum_{k=1}^K p_k \log_2 p_k \] \(p_k\)：第 \(k\) 类的比例 \(H = 0\)：节点完全纯 当类别均衡时熵最大 例子： 一个节点有 5 个正例、5 个负例： \[ H = - (0.5\log_2 0.5 + 0.5\log_2 0.5) = 1 \] （不确定性很高） 2.2 信息增益（Information Gain, IG） 含义： 信息增益衡量 划分后熵降低了多少 。 公式： \[ IG(S, A) = H(S) - \sum_{v} \frac{|S_v|}{|S|} H(S_v) \] 例子： 父节点熵 = 1 划分后： 左子节点熵 = 0（纯） 右子节点熵 = 0.5 如果左右均分： \[ IG = 1 - (0.5 \cdot 0 + 0.5 \cdot 0.5) = 0.75 \] IG 越大 ⇒ 划分越好。 熵与信息增益总结：我们在每一层选择 信息增益最大 的划分作为父节点。信息增益 = 上一层熵 − 子类别熵 。下面是是否打网球的例子： 2.3 基尼不纯度（Gini Impurity） 含义： 基尼不纯度衡量随机抽样被误分类的概率。 公式： \[ G(S) = 1 - \sum_{k=1}^K p_k^2 \] 例子： 一个节点 7 个正例、3 个负例： \[ G = 1 - (0.7^2 + 0.3^2) = 0.42 \] \(G = 0\)：完全纯 因效率高，常用于 CART 下面是基尼不纯度的相关幻灯片： 完美划分： 非完美划分： 2.3.1 基尼与熵的比较： 熵 vs 基尼不纯度 如果数据集完全同质（只有一个类别），则不纯度为 0，熵也为 0。 如果完全非同质且不纯度为 100%，则熵为 1。 实际中熵与基尼的结果非常接近。 两者只在约 2% 的情况产生分歧。 熵可能计算更慢，因为包含对数运算。 2.4 方差 / MSE（回归树） 含义： 用于目标为连续值时，衡量目标值的离散程度。 公式： \[ \text{MSE}(S) = \frac{1}{|S|}\sum_{i \in S}(y_i - \bar y_S)^2 \] 目标： 选择使子节点 加权 MSE 最小 的划分。 例子： 目标值：\([200, 210, 220, 500]\) 划分能把低价与高价分开 → 方差显著下降。 3. 决策树如何训练 决策树训练是一个 贪心的、自上而下的递归过程 ： 从根节点开始 包含所有训练样本。 评估所有可能的划分 ： 对每个特征 对每个可能阈值或类别子集 计算不纯度下降 （根据所选准则）。 选择最佳划分 （最大不纯度下降）。 划分数据 到子节点。 对子节点递归重复 。 停止条件 ： 节点已纯 达到最小样本数 达到最大深度 不纯度下降不明显 （可选）剪枝 以减少过拟合。 关键性质： 决策树进行的是 局部优化 ，不是全局优化。 4. 真实案例：贷款审批预测 问题 银行希望根据申请人信息判断 批准或拒绝贷款 。 训练数据（简化） 收入 信用分 负债率 是否批准 80k 720 0.2 是 45k 650 0.4 否 30k 600 0.5 否 90k 780 0.1 是 60k 690 0.3 是 训练过程 所有样本起始于根节点。 决策树评估划分： `信用分 > 700` `收入 > 50k` `负债率 假设 `信用分 > 700` 带来最大不纯度下降。 继续在混合子节点上训练。 最终学到的树 信用分 > 700? ├── 是 → 批准贷款 └── 否 ├── 负债率 预测 对新申请人： 信用分 = 680 负债率 = 0.28 决策路径： 信用分 > 700? → 否 负债率 → 批准贷款 代码示例： from sklearn.datasets import load_iris from sklearn import tree from matplotlib import pyplot as plt import pandas as pd iris = load_iris() X = iris.data y = iris.target df = pd.DataFrame(list(zip(X, y)), columns =['Features', 'Label']) df #build decision tree clf = tree.DecisionTreeClassifier(criterion='entropy', max_depth=4,min_samples_leaf=4) #max_depth represents max level allowed in each tree, min_samples_leaf minumum samples storable in leaf node #fit the tree to iris dataset clf.fit(X,y) #plot decision tree fig, ax = plt.subplots(figsize=(6, 6)) #figsize value changes the size of plot tree.plot_tree(clf,ax=ax,feature_names=['sepal length','sepal width','petal length','petal width']) plt.show() 关键结论 决策树通过贪心地选择能降低不纯度的特征划分来学习可解释规则，是一个强大且透明的分类与回归模型。 停下来思考： 到这里你可能会想：如果树太复杂会怎样？→ 过拟合！现在引入剪枝！ 剪枝是一种通过移除贡献较小的分支来缩小树规模的方法 它是一种正则化方法，可降低最终模型复杂度，从而减少过拟合 剪枝方法： 预剪枝（Pre-pruning） ：在树完全分类数据前就停止构建： 当前集合的熵（或基尼不纯度） 当前集合的样本数 最佳划分特征的增益 树的深度 后剪枝（Post-pruning） ：先生成完整树，再用叶节点替换某些非叶节点以提高验证误差 决策树还有： 缺失值的处理 ID3 如何处理数值型属性 1. 简短回答（核心思想） ID3 通过阈值把数值型属性转成二元划分，并用信息增益选择最佳阈值。 换句话说，ID3 不会直接把数值属性当作连续变量处理 。 而是将其 离散化 为如下二元问题： \[ x_j \le t \quad \text{vs.} \quad x_j > t \] 2. 为什么 ID3 需要特殊处理数值属性 原始 ID3 算法 设计用于： 类别型（离散）特征 使用 熵 与 信息增益 数值属性的问题： 可能取值无限 无法像类别一样枚举所有取值 因此 ID3 必须 把数值型特征转换为类别型决策 。 3. 逐步说明：ID3 如何处理数值型属性 假设我们有： 数值特征 \(x\) 目标标签 \(y\) Step 1：按数值特征排序 例子： 数值 \(x\) 类别 \(y\) 20 否 30 否 40 是 60 是 Step 2：生成候选阈值 ID3 取相邻值的中点： \[ t_1 = \frac{20+30}{2}=25,\quad t_2 = \frac{30+40}{2}=35,\quad t_3 = \frac{40+60}{2}=50 \] Step 3：将每个阈值转成二元划分 对每个 \(t\)：x ≤ t vs. x > t 以 \(t=35\) 为例： 左：{20, 30} 右：{40, 60} Step 4：计算每个阈值的信息增益 对每个划分： \[ IG(t) = H(S) - \left( \frac{|S_L|}{|S|}H(S_L) + \frac{|S_R|}{|S|}H(S_R) \right) \] ID3 与类别特征完全一样 计算 IG。 Step 5：选择最佳阈值 \[ t^* = \arg\max_t IG(t) \] 该数值特征在该节点被视为 二元类别特征 。 4. 简单例子 特征： 年龄 目标： 是否打网球 候选划分： 年龄 ≤ 35? ├── 是 → 多数为否 └── 否 → 多数为是 若该划分带来最大信息增益，则被选中。 5. 更详细的例子（含熵） 父节点： 6 是，4 否 熵： \[ H(S)= -0.6\log_2 0.6 - 0.4\log_2 0.4 = 0.971 \] 在 \(t=35\) 处划分： 左：4 否 → 熵 = 0 右：6 是 → 熵 = 0 信息增益： \[ IG = 0.971 - 0 = 0.971 \] 👉 完美划分 → ID3 选择它。 6. ID3 方法的重要特性 ID3 擅长的方面 简单直观 全部使用同一标准（信息增益） 产生可解释的阈值规则 局限性 贪心且局部 对噪声敏感 多次阈值测试计算成本高 信息增益偏向多值属性 7. 与其他树算法的比较 算法 数值处理方式 ID3 二元阈值 + 信息增益 C4.5 阈值 + 增益率（Gain Ratio） CART 阈值 + 基尼 / MSE LightGBM 基于直方图的阈值 8. 总结 ID3 通过排序、测试候选阈值的二元划分，并选择信息增益最大的阈值来处理数值型特征。 决策树的优缺点： 优点： 与其他算法相比，决策树在预处理阶段所需的数据准备更少。 决策树不需要数据归一化。 决策树不需要数据缩放。 数据中的缺失值对构建决策树影响不大。 决策树模型直观、易解释，便于向技术团队和业务方说明。 能处理异常值问题。 随机森林可用于分类与回归任务。 决策树能同时处理连续变量和类别变量。 缺点： 由于节点划分过细，容易过拟合。 数据的微小变化 可能导致树结构发生 巨大变化 ，从而不稳定。 决策树有时计算会比其他算法 复杂得多 。 决策树训练往往耗时更长。 决策树训练成本较高，因为复杂度和时间开销更大。 决策树算法对 回归 和 连续值预测 并不充分。 随机森林（Random Forest）： RF 算法 多样性——构建单棵树时不会使用全部特征，因此每棵树都不同。 抗维度约束 ：每棵树不会考虑全部特征，因此特征空间更小。 可并行化 ：每棵树用不同数据与特征从头构建，可充分利用 CPU。 训练-测试分割 ：随机森林不需要显式分训练/测试，因为每棵树都会遗漏约 30% 的数据。 稳定性 ：最终结果基于多数投票/平均，因此更稳定。 随机森林分类器与决策树的区别 过拟合 ：随机森林由数据子集构成，最终输出取平均/多数投票，因此过拟合较少。 速度 ：随机森林算法通常比决策树慢。 过程 ：随机森林随机抽样、建树、再平均结果，不依赖单一公式。 RF 代码示例 from sklearn.ensemble import RandomForestClassifier from sklearn.model_selection import train_test_split from sklearn.metrics import accuracy_score from sklearn.datasets import load_breast_cancer # Load example dataset X, y = load_breast_cancer(return_X_y=True) # Train / test split X_train, X_test, y_train, y_test = train_test_split( X, y, test_size=0.2, random_state=42 ) # Build Random Forest rf = RandomForestClassifier( n_estimators=100, # number of trees max_depth=None, # let trees grow fully min_samples_split=2, # minimum samples to split random_state=42 ) # Train rf.fit(X_train, y_train) # Predict y_pred = rf.predict(X_test) # Evaluate print("Accuracy:", accuracy_score(y_test, y_pred)) 1. 概述：什么是决策树？什么是决策准则？ 2. 决策准则详解（含例子） 3. 决策树如何训练 4. 真实案例：贷款审批预测 代码示例： 关键结论 停下来思考： 决策树还有： 缺失值的处理 1. 简短回答（核心思想） 2. 为什么 ID3 需要特殊处理数值属性 3. 逐步说明：ID3 如何处理数值型属性 4. 简单例子 5. 更详细的例子（含熵） 6. ID3 方法的重要特性 7. 与其他树算法的比较 8. 总结 决策树的优缺点： 随机森林（Random Forest）： 人工智能 | 金融工程 | 数学 | 计算机科学 袁榕言 2026]]></content>
    </entry>
</search>
