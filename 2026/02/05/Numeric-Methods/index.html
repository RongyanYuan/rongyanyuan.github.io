
<!DOCTYPE html>

<html lang="en">
<head>
<meta charset="utf-8"/>
<meta content="width=device-width, initial-scale=1.0" name="viewport"/>
<title>  Neural Networks, CNNs, and Kernel Methods |    Rongyan's Blog</title>
<meta content="Financial Engineering | Mathematics | Computer Science" name="description"/>
<!-- 标签页图标 -->
<!-- 图标库 -->
<link href="https://cdn.jsdelivr.net/npm/remixicon@2.2.0/fonts/remixicon.css" rel="stylesheet"/>
<!-- 动画库 -->
<link href="https://cdn.jsdelivr.net/gh/fushaolei/cdn-white@1.0/css/animate.css" rel="stylesheet"/>
<!-- css文件 -->
<link href="/css/white.css" rel="stylesheet"/>
<!-- 代码高亮 -->
<meta content="Hexo 6.2.0" name="generator"/></head>
<body>
<div class="menu-outer">
<div class="menu-inner">
<div class="menu-site-name animate__animated animate__fadeInUp">
<a href="/">
          Rongyan's Blog
        </a>
<div class="menu-redirect"><a class="nav-link back-topic-link" href="/categories/math/numeric-methods/">← Numerical Methods</a></div></div>
<div class="menu-group">
<ul class="menu-ul">
<a class="nav-link" href="/">
<li class="menu-li animate__animated animate__fadeInUp">
              HOME
            </li>
</a>
<a class="nav-link" href="/archives">
<li class="menu-li animate__animated animate__fadeInUp">
              BLOG
            </li>
</a>
<li class="menu-li animate__animated animate__fadeInUp" id="sort">
             CATEGORIES
             <div class="categories-outer" id="sort-div">
<ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/AboutMe/">AboutMe</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/ai/">AI</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs/">CS</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/math/">Math</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/quant/">Quant</a></li></ul>
</div>
</li>
<a href="/search">
<li class="menu-li animate__animated animate__fadeInUp">
<i class="ri-search-line"></i>
</li>
</a>
<li class="menu-li animate__animated animate__fadeInUp lang-switcher-desktop">
<a href="/2026/02/05/Numeric-Methods/index.html">EN</a> |
            <a href="/2026/02/05/Numeric-Methods/index-zh.html">中文</a>
</li>
<li class="menu-li animate__animated animate__fadeInUp" id="mobile-menu">
<i class="ri-menu-line"></i>
</li>
</ul>
</div>
</div>
</div>
<div class="animate__animated animate__fadeIn" id="mobile-main">
<div class="mobile-menu-inner">
<div class="mobile-menu-site-name animate__animated animate__fadeInUp">
<a href="/">
        Rongyan's Blog
      </a>
</div>
<div class="mobile-menu-group" id="mobile-close">
<i class="ri-close-line"></i>
</div>
</div>
<div class="mobile-menu-div">
<a class="mobile-nav-link" href="/">
<div class="mobile-menu-child animate__animated animate__fadeInUp">
<span>HOME</span>
</div>
</a>
<a class="mobile-nav-link" href="/archives">
<div class="mobile-menu-child animate__animated animate__fadeInUp">
<span>BLOG</span>
</div>
</a>
<div class="mobile-menu-child animate__animated animate__fadeInUp lang-switcher-mobile">
<a href="/2026/02/05/Numeric-Methods/index.html">EN</a> |
      <a href="/2026/02/05/Numeric-Methods/index-zh.html">中文</a>
</div>
<a href="/search">
<div class="mobile-menu-child animate__animated animate__fadeInUp">
<i class="ri-search-line"></i>
</div>
</a>
</div>
</div>
<div class="body-outer">
<div class="body-inner">
<article class="post-inner">
<div class="post-content-outer">
<div class="post-intro">
<div class="post-title animate__animated animate__fadeInUp">Neural Networks, CNNs, and Kernel Methods</div>
<div class="meta-intro animate__animated animate__fadeInUp">Feb 05 2026</div>
</div>
<div class="post-content-inner">
<div class="post-content-inner-space">
</div>
<div class="post-content-main animate__animated animate__fadeInUp"><h1 id="NN-CNN-Kernel-Notes"><a class="headerlink" href="#NN-CNN-Kernel-Notes" title="Gist"></a>Gist</h1><p>These notes cover neural networks and backpropagation, CNN design details, and kernel methods with Hilbert space foundations. 
          All formulas are written in standard \(\LaTeX\) math notation.</p><h2 id="Reading-List"><a class="headerlink" href="#Reading-List" title="Reading List"></a>Reading List</h2><ul>
<li><a href="/pdf/lecture20.pdf" target="_blank">Deep Learning and CNN by CMU</a></li>
<li><a href="/pdf/lecture1_whatIsRKHS.pdf" target="_blank">Lecture 1: What is RKHS?</a></li>
</ul><h2 id="Convolution-Cross-Correlation"><a class="headerlink" href="#Convolution-Cross-Correlation" title="Understanding Convolution and Cross-Correlation"></a>Understanding Convolution and Cross-Correlation</h2><h3>1. Convolution Definition (Continuous and Discrete)</h3><p>Convolution is an operation that combines two functions (or signals) to produce a third. In signal processing and image processing, it combines an input signal with a kernel (filter) to create an output.</p><ul>
<li><strong>Continuous convolution</strong>: defined using an integral.</li>
<li><strong>Discrete convolution</strong>: defined using a sum.</li>
</ul><h3>2. Continuous Convolution</h3><p>In continuous convolution, the operation combines two functions, \( f(t) \) and \( g(t) \), and is defined as:</p><p>\[
(f * g)(t) = \int_{-\infty}^{\infty} f(\tau) g(t - \tau) d\tau
\]</p><h4>Breaking it down</h4><ul>
<li>\( f(t) \): the input function (signal).</li>
<li>\( g(t) \): the kernel (filter or pattern).</li>
<li>\( t \): the current time/position in the output.</li>
<li>\( \tau \): integration variable representing shifts of \( f(t) \).</li>
<li>\( g(t-\tau) \): the kernel is shifted and <strong>flipped</strong> (this makes it convolution rather than correlation).</li>
</ul><h3>3. Discrete Convolution</h3><p>For discrete signals, the formula is:</p><p>\[
(f * g)[n] = \sum_{m=-\infty}^{\infty} f[m] \cdot g[n - m]
\]</p><h4>Breaking it down</h4><ul>
<li>\( f[m] \): input signal at index \( m \).</li>
<li>\( g[n-m] \): shifted, flipped kernel.</li>
<li>\( n \): output index.</li>
</ul><h3>4. Why Is the Kernel Flipped?</h3><ul>
<li>Flipping is part of the formal definition of convolution.</li>
<li>It ensures symmetry (commutativity) and aligns with Fourier properties.</li>
<li>It makes convolution consistent with the Fourier transform identity: convolution in time equals multiplication in frequency.</li>
</ul><h3>5. Convolution vs Cross-Correlation</h3><ul>
<li><strong>Convolution</strong> (signal processing): kernel is flipped.</li>
<li><strong>Cross-correlation</strong> (CNNs): kernel is not flipped. In deep learning, this is fine because kernels are learned directly.</li>
</ul><h3>6. Summary</h3><ul>
<li>Convolution combines an input signal and a kernel to produce an output.</li>
<li>Continuous convolution uses integrals; discrete uses sums.</li>
<li>Kernel flipping is essential in convolution, but CNNs typically use cross-correlation.</li>
</ul><h2 id="Neural-Networks"><a class="headerlink" href="#Neural-Networks" title="Neural Networks"></a>1. Neural Networks</h2><h3 id="NN-Intro"><a class="headerlink" href="#NN-Intro" title="Introduction"></a>Introduction</h3><p>A feed‑forward neural network defines a function \(f_\theta: \mathbb{R}^d \to \mathbb{R}^k\) by composing affine transforms and nonlinearities:</p><p>\(a^{(0)} = x\), \(z^{(l)} = W^{(l)} a^{(l-1)} + b^{(l)}\), \(a^{(l)} = \sigma(z^{(l)})\), and \(f_\theta(x)=a^{(L)}\).</p><h3 id="Backprop"><a class="headerlink" href="#Backprop" title="Backpropagation"></a>Backpropagation</h3><p>Given loss \(\mathcal{L}(f_\theta(x), y)\), backprop computes gradients efficiently using the chain rule.</p><ul>
<li>Output layer error: \(\delta^{(L)} = \nabla_{a^{(L)}} \mathcal{L} \odot \sigma'(z^{(L)})\).</li>
<li>Hidden layer error: \(\delta^{(l)} = (W^{(l+1)})^T \delta^{(l+1)} \odot \sigma'(z^{(l)})\).</li>
<li>Gradients: \(\nabla_{W^{(l)}} \mathcal{L} = \delta^{(l)} (a^{(l-1)})^T\), \(\nabla_{b^{(l)}} \mathcal{L} = \delta^{(l)}\).</li>
</ul><h2 id="CNN"><a class="headerlink" href="#CNN" title="Convolutional Neural Networks"></a>2. Convolutional Neural Networks (CNNs)</h2><h3 id="CNN-Intro"><a class="headerlink" href="#CNN-Intro" title="Introduction"></a>Introduction</h3><p>CNNs are specialized for grid‑structured data (images, time‑series). They exploit local connectivity and parameter sharing to reduce complexity.</p><h3 id="Receptive-Field"><a class="headerlink" href="#Receptive-Field" title="Receptive Field"></a>Receptive Field</h3><p>The receptive field is the input region that influences a particular feature map activation. With kernel size \(k\), stride \(s\), and depth \(L\), receptive fields grow layer by layer.</p><p>If layer \(l\) has receptive field size \(R_l\), then for a subsequent conv layer:</p><p>\(R_{l+1} = R_l + (k_{l+1} - 1) \cdot \prod_{i=1}^{l} s_i\).</p><h4 id="RF-Problem"><a class="headerlink" href="#RF-Problem" title="Receptive Field Problem"></a>Practice Problem (Receptive Field)</h4><p>A CNN has three conv layers. Layer 1: kernel 3, stride 1. Layer 2: kernel 3, stride 2. Layer 3: kernel 5, stride 1. What is the receptive field size at layer 3?</p><details class="solution-block">
<summary>Show Solution</summary>
<p><strong>Answer:</strong> Use \(R_{l+1} = R_l + (k_{l+1}-1)\prod_{i=1}^{l} s_i\) with \(R_1=3\).</p>
<p>Layer 2: \(R_2 = 3 + (3-1)\cdot 1 = 5\). Layer 3: \(R_3 = 5 + (5-1)\cdot (1\cdot 2)=5+8=13\).</p>
<p><strong>Receptive field = 13</strong>.</p>
</details><h3 id="Kernel-Size"><a class="headerlink" href="#Kernel-Size" title="Kernel Size and Output Shape"></a>Kernel Size and Output Shape</h3><p>For input spatial size \(n\), kernel size \(k\), padding \(p\), stride \(s\):</p><p>\(n_{out} = \left\lfloor \frac{n + 2p - k}{s} \right\rfloor + 1\).</p><p>To keep size unchanged (“same” padding): choose \(p = \frac{k-1}{2}\) when \(k\) is odd.</p><h4 id="KS-Problem"><a class="headerlink" href="#KS-Problem" title="Kernel Size/Output Shape Problem"></a>Practice Problem (Kernel Size &amp; Output Shape)</h4><p>Input is \(64	imes64\). Apply conv with kernel \(k=7\), padding \(p=3\), stride \(s=2\). What is the output size?</p><details class="solution-block">
<summary>Show Solution</summary>
<p>\(n_{out} = \left\lfloor rac{n+2p-k}{s} 
ight
floor + 1 = \left\lfloor rac{64+6-7}{2} 
ight
floor + 1 = \left\lfloor rac{63}{2} 
ight
floor + 1 = 31 + 1 = 32\).</p>
<p><strong>Output size = 32 × 32</strong>.</p>
</details><h3 id="Num-Kernels"><a class="headerlink" href="#Num-Kernels" title="Number of Kernels per Layer"></a>Number of Kernels per Layer</h3><p>If a conv layer has \(C_{in}\) input channels and \(C_{out}\) kernels, then:</p><ul>
<li>Each kernel has shape \(k \times k \times C_{in}\).</li>
<li>The number of output feature maps equals \(C_{out}\).</li>
<li>Total parameters: \(C_{out} \cdot (k^2 C_{in} + 1)\) (including bias).</li>
</ul><h4 id="NK-Problem"><a class="headerlink" href="#NK-Problem" title="Number of Kernels Problem"></a>Practice Problem (Number of Kernels)</h4><p>A conv layer takes \(C_{in}=64\) channels and uses \(C_{out}=128\) kernels of size \(3	imes3\). How many parameters does the layer have (including bias)?</p><details class="solution-block">
<summary>Show Solution</summary>
<p>Parameters per kernel: \(3\cdot3\cdot64 + 1 = 577\). Total: \(128\cdot577 = 73856\).</p>
<p><strong>Total parameters = 73,856</strong>.</p>
</details><h3 id="CNN-Pros-Cons"><a class="headerlink" href="#CNN-Pros-Cons" title="Pros and Cons"></a>Pros and Cons of CNNs</h3><ul>
<li><strong>Pros:</strong> parameter sharing, spatial inductive bias, strong performance on vision.</li>
<li><strong>Cons:</strong> less effective for global dependencies, requires many labeled images, limited rotation invariance unless augmented.</li>
</ul><h3 id="Cross-Correlation"><a class="headerlink" href="#Cross-Correlation" title="Cross-Correlation"></a>Cross‑Correlation in CNNs</h3><p>Most CNN libraries implement <strong>cross‑correlation</strong> instead of mathematical convolution. For input \(x\) and kernel \(k\):</p><p>\[(x \star k)[i,j] = \sum_{u=0}^{m-1} \sum_{v=0}^{n-1} x[i+u, j+v] \, k[u,v]\]</p><p>Convolution would flip the kernel: \((x * k)[i,j] = \sum_{u,v} x[i+u, j+v] \, k[m-1-u, n-1-v]\). In practice, the network learns the kernel weights, so using cross‑correlation is equivalent up to learned parameters.</p><h4 id="Cross-Correlation-Problem"><a class="headerlink" href="#Cross-Correlation-Problem" title="Practice Problem"></a>Practice Problem</h4><p>Given input \(x\) and kernel \(k\):</p><p>\(x=\begin{bmatrix}1 &amp; 2 &amp; 0 \\ 0 &amp; 1 &amp; 3 \\ 2 &amp; 1 &amp; 1\end{bmatrix}\), \(k=\begin{bmatrix}1 &amp; 0 \\ -1 &amp; 2\end{bmatrix}\).</p><p>Compute the top‑left output of cross‑correlation (no padding, stride 1): \((x \star k)[0,0]\).</p><details class="solution-block">
<summary>Show Solution</summary>
<p><strong>Solution:</strong> \((x \star k)[0,0] = 1\cdot1 + 2\cdot0 + 0\cdot(-1) + 1\cdot2 = 3\).</p>
</details><figure>
<img alt="CNN convolution example" src="/images/cnn-kernel-example.png"/>
<figcaption>Example of convolution kernels applied across channels to produce feature maps.</figcaption>
</figure><h2 id="Hilbert"><a class="headerlink" href="#Hilbert" title="Hilbert Spaces"></a>3. Hilbert Spaces</h2><h3 id="Hilbert-Def"><a class="headerlink" href="#Hilbert-Def" title="Definition"></a>Definition</h3><p>A Hilbert space is a complete inner‑product space \((\mathcal{H}, \langle \cdot, \cdot \rangle)\) where the norm \(|x| = \sqrt{\langle x, x \rangle}\) makes every Cauchy sequence converge.</p><h3 id="Hilbert-Examples"><a class="headerlink" href="#Hilbert-Examples" title="Examples"></a>Examples</h3><ul>
<li>\(\mathbb{R}^n\) with dot product.</li>
<li>\(L^2([a,b])\): square‑integrable functions with \(\langle f,g \rangle = \int_a^b f(x)g(x)\,dx\).</li>
<li>Sequence space \(\ell^2\): \(\sum_i x_i^2 &lt; \infty\).</li>
</ul><details class="solution-block">
<summary>Completeness of \(L^{2}\): proof</summary>
<h1>Completeness of \(L^{2}\): \(L^{2}(\Omega,\Sigma,\mu)\) is a Hilbert space</h1>
<p>Let \((\Omega,\Sigma,\mu)\) be a measure space. Define</p>
<p>\[
L^{2}(\Omega) := \left\{ f:\Omega\to\mathbb{C}\text{ measurable }:\ \int_\Omega |f|^{2}\,d\mu &lt; \infty \right\}\Big/\sim,
\]</p>
<p>where \(f\sim g\) iff \(f=g\) \(\mu\)-a.e.</p>
<p>Equip \(L^{2}\) with the inner product</p>
<p>\[
\langle f,g\rangle := \int_\Omega f\overline{g}\,d\mu,
\]</p>
<p>and norm \(\|f\|_{2} := \big(\int_\Omega |f|^{2}\,d\mu\big)^{1/2}\).</p>
<p>We prove: <strong>every Cauchy sequence in \((L^{2},\|\cdot\|_{2})\) converges in \(\|\cdot\|_{2}\)</strong>.<br/>
Hence \(L^{2}\) is complete as a normed space; together with the inner product, it is a <strong>Hilbert space</strong>.</p>
<hr/>
<h2>Theorem</h2>
<p>\((L^{2}(\Omega),\langle\cdot,\cdot\rangle)\) is a Hilbert space.</p>
<h3>Proof (completeness of \(L^{2}\))</h3>
<p>Let \((f_n)\) be a Cauchy sequence in \(L^{2}(\Omega)\).<br/>
So for every \(\varepsilon&gt;0\) there exists \(N\) such that for all \(m,n\ge N\),</p>
<p>\[
\|f_m-f_n\|_{2} &lt; \varepsilon.
\]</p>
<h3>Step 1: Extract a rapidly Cauchy subsequence</h3>
<p>Since \((f_n)\) is Cauchy, we can choose a subsequence \((f_{n_k})_{k\ge 1}\) such that</p>
<p>\[
\|f_{n_{k+1}} - f_{n_k}\|_{2} \le 2^{-k}\quad \text{for all }k\ge 1.
\]</p>
<p>Define the increments</p>
<p>\[
h_k := f_{n_{k+1}} - f_{n_k}\ \in L^{2}.
\]</p>
<p>Then</p>
<p>\[
\|h_k\|_{2} \le 2^{-k}
\quad\Longrightarrow\quad
\sum_{k=1}^{\infty} 2^{k}\|h_k\|_2^2 \le \sum_{k=1}^{\infty} 2^{k}\cdot 2^{-2k} = \sum_{k=1}^{\infty} 2^{-k} &lt; \infty.
\]</p>
<h3>Step 2: Show the subsequence converges pointwise a.e.</h3>
<p>Consider the nonnegative measurable function</p>
<p>\[
H(x):=\sum_{k=1}^{\infty} 2^{k}\,|h_k(x)|^{2}.
\]</p>
<p>By Tonelli’s theorem,</p>
<p>\[
\int_\Omega H\,d\mu
= \sum_{k=1}^{\infty} 2^{k}\int_\Omega |h_k|^{2}\,d\mu
= \sum_{k=1}^{\infty} 2^{k}\|h_k\|_2^2 &lt; \infty.
\]</p>
<p>Therefore \(H(x)&lt;\infty\) for \(\mu\)-a.e. \(x\).</p>
<p>Fix such an \(x\). For integers \(m&lt;\ell\), by Cauchy–Schwarz:</p>
<p>\[
\sum_{k=m}^{\ell} |h_k(x)|
= \sum_{k=m}^{\ell} 2^{-k/2}\,\big(2^{k/2}|h_k(x)|\big)
\le \left(\sum_{k=m}^{\ell} 2^{-k}\right)^{1/2}
     \left(\sum_{k=m}^{\ell} 2^{k}|h_k(x)|^{2}\right)^{1/2}.
\]</p>
<p>Since \(\sum_{k=m}^{\ell} 2^{-k}\le 2^{-m+1}\) and \(\sum_{k=m}^{\ell}2^{k}|h_k(x)|^{2}\le H(x)&lt;\infty\), we get</p>
<p>\[
\sum_{k=m}^{\ell} |h_k(x)|
\le \sqrt{2^{-m+1}}\cdot \sqrt{H(x)} \xrightarrow[m\to\infty]{} 0.
\]</p>
<p>Hence \(\sum_{k=1}^{\infty} h_k(x)\) is absolutely convergent, so the partial sums</p>
<p>\[
S_K(x) := \sum_{k=1}^{K} h_k(x)
\]</p>
<p>converge as \(K\to\infty\) for \(\mu\)-a.e. \(x\).</p>
<p>Define</p>
<p>\[
f(x) := f_{n_1}(x) + \sum_{k=1}^{\infty} h_k(x)
\quad\text{for a.e. }x.
\]</p>
<p>Then for \(\mu\)-a.e. \(x\),</p>
<p>\[
f_{n_{K+1}}(x) = f_{n_1}(x) + \sum_{k=1}^{K} h_k(x) \xrightarrow[K\to\infty]{} f(x).
\]</p>
<p>So \(f_{n_k}\to f\) pointwise a.e. along the subsequence.</p>
<h3>Step 3: Show the subsequence converges to \(f\) in \(L^{2}\)</h3>
<p>For each fixed \(m\ge 1\), consider the tails</p>
<p>\[
T_{m,r}(x) := \sum_{k=m}^{r} h_k(x) = f_{n_{r+1}}(x) - f_{n_m}(x).
\]</p>
<p>Pointwise a.e., \(T_{m,r}(x)\to f(x)-f_{n_m}(x)\) as \(r\to\infty\).
By Fatou’s lemma applied to \(|T_{m,r}|^2\),</p>
<p>\[
\|f-f_{n_m}\|_2^2
= \int_\Omega \left| \lim_{r\to\infty} T_{m,r} \right|^2 d\mu
\le \liminf_{r\to\infty} \int_\Omega |T_{m,r}|^2\,d\mu
= \liminf_{r\to\infty} \|T_{m,r}\|_2^2.
\]</p>
<p>But by the triangle inequality in \(L^2\),</p>
<p>\[
\|T_{m,r}\|_2
= \left\| \sum_{k=m}^{r} h_k \right\|_2
\le \sum_{k=m}^{r} \|h_k\|_2
\le \sum_{k=m}^{\infty} 2^{-k}
= 2^{-m+1}.
\]</p>
<p>Thus \(\|T_{m,r}\|_2^2 \le 2^{-2m+2}\) for all \(r\ge m\), hence</p>
<p>\[
\|f-f_{n_m}\|_2^2 \le 2^{-2m+2}
\quad\Longrightarrow\quad
\|f-f_{n_m}\|_2 \le 2^{-m+1}\xrightarrow[m\to\infty]{}0.
\]</p>
<p>So the subsequence \(f_{n_m}\to f\) in \(L^2\). In particular, \(f\in L^2\).</p>
<h3>Step 4: Lift convergence from subsequence to the whole sequence</h3>
<p>Since \((f_n)\) is Cauchy in \(L^2\), for any \(\varepsilon&gt;0\) choose \(N\) such that for all \(n\ge N\),</p>
<p>\[
\|f_n - f_N\|_2 &lt; \varepsilon/2.
\]</p>
<p>Choose \(m\) with \(n_m\ge N\) and \(\|f_{n_m}-f\|_2 &lt; \varepsilon/2\).
Then for all \(n\ge N\),</p>
<p>\[
\|f_n - f\|_2
\le \|f_n - f_{n_m}\|_2 + \|f_{n_m}-f\|_2
&lt; \varepsilon/2 + \varepsilon/2
= \varepsilon.
\]</p>
<p>Therefore \(f_n\to f\) in \(L^2\). This proves <strong>every Cauchy sequence converges</strong> in \(L^2\).</p>
<p>Hence \((L^2,\|\cdot\|_2)\) is complete, and with the inner product \(\langle\cdot,\cdot\rangle\), it is a <strong>Hilbert space</strong>. ∎</p>
<hr/>
<h2>Notes (what was essential)</h2>
<ul>
<li>The subsequence choice \(\|f_{n_{k+1}}-f_{n_k}\|_2\le 2^{-k}\) gives a “summable control”.</li>
<li>The weighted series \(\sum 2^k |h_k|^2\) has finite integral, so it is finite a.e.</li>
<li>Cauchy–Schwarz with weights converts that into pointwise convergence of \(\sum h_k(x)\).</li>
<li>Fatou + triangle inequality yields \(L^2\) convergence.</li>
</ul>
</details><h2 id="PD"><a class="headerlink" href="#PD" title="Positive Definite Functions"></a>4. Positive Definite Functions and Kernels</h2><h3 id="PD-Def"><a class="headerlink" href="#PD-Def" title="Definition"></a>Definition</h3><p>A function \(K: X \times X \to \mathbb{R}\) is <strong>positive definite</strong> if for any \(x_1,\ldots,x_n\) and any \(a \in \mathbb{R}^n\):</p><p>\(\sum_{i=1}^n \sum_{j=1}^n a_i a_j K(x_i, x_j) \ge 0\).</p><p><strong>Example:</strong> For the RBF kernel \(K(x,x')=\exp(-|x-x'|^2/(2\sigma^2))\), the Gram matrix \(K\) is symmetric and all eigenvalues are non‑negative for any finite sample, so it is positive definite.</p><h3 id="PD-Test"><a class="headerlink" href="#PD-Test" title="How to Test Positive Definiteness"></a>How to Test Positive Definiteness</h3><ul>
<li>Form Gram matrix \(K_{ij} = K(x_i, x_j)\); check that \(K\) is symmetric and all eigenvalues are non‑negative.</li>
<li>Show that \(K(x, x') = \langle \phi(x), \phi(x') \rangle\) for some feature map \(\phi\).</li>
<li>Use closure properties: sums, products, and limits of p.d. kernels are p.d.</li>
</ul><h4 id="PD-Test-Example"><a class="headerlink" href="#PD-Test-Example" title="Example"></a>Example</h4><p>Let \(K(x,x') = \exp(-|x-x'|^2)\) (RBF kernel). For two distinct points with distance \(d&gt;0\), the Gram matrix is</p><p>\(K = \begin{bmatrix}1 &amp; e^{-d^2} \\ e^{-d^2} &amp; 1\end{bmatrix}\).</p><details class="solution-block">
<summary>Show Check</summary>
<p>The eigenvalues are \(1\pm e^{-d^2}\). Since \(0&lt;e^{-d^2}&lt;1\), both eigenvalues are positive, so \(K\) is positive definite.</p>
</details>
<h2 id="Kernel-Methods"><a class="headerlink" href="#Kernel-Methods" title="Kernel Methods"></a>5. Kernel Methods</h2><h3 id="Kernel-Def"><a class="headerlink" href="#Kernel-Def" title="What is a Kernel"></a>What is a Kernel?</h3><p>A kernel is a similarity function \(K(x, x')\) that acts like an inner product in some (possibly infinite‑dimensional) feature space.</p><h3 id="Kernel-Trick"><a class="headerlink" href="#Kernel-Trick" title="Kernel Trick"></a>Kernel Trick</h3><p>Algorithms relying on dot products can be rewritten in terms of \(K(x, x')\), avoiding explicit feature mappings.</p><h3 id="Representer-Theorem"><a class="headerlink" href="#Representer-Theorem" title="Representer Theorem"></a>Representer Theorem</h3><p>For regularized risk minimization in RKHS, the solution has the form:</p><p>\(f(x) = \sum_{i=1}^n \alpha_i K(x_i, x)\).</p><h4 id="Representer-Proof"><a class="headerlink" href="#Representer-Proof" title="Proof Sketch"></a>Proof Sketch</h4><p>Any \(f \in \mathcal{H}\) can be decomposed as \(f=f_\parallel+f_\perp\), where \(f_\parallel\) lies in the span of \(\{K(x_i,\cdot)\}\) and \(f_\perp\) is orthogonal. The data‑fit term depends only on \(f_\parallel\), while the RKHS norm satisfies \(\|f\|^2=\|f_\parallel\|^2+\|f_\perp\|^2\). Thus the minimizer must have \(f_\perp=0\).</p><h4 id="Representer-Problem"><a class="headerlink" href="#Representer-Problem" title="Practice Problem"></a>Practice Problem</h4><p>For kernel ridge regression with RBF kernel, write the solution form and the system you must solve.</p><details class="solution-block">
<summary>Show Solution</summary>
<p>The representer theorem gives \(f(x)=\sum_{i=1}^n \alpha_i K(x_i,x)\). Solve \((K+\lambda n I)\alpha = y\) for \(\alpha\), where \(K\) is the Gram matrix.</p>
</details><h3 id="Kernel-Apps"><a class="headerlink" href="#Kernel-Apps" title="Applications and Examples"></a>Applications and Examples</h3><ul>
<li>Support Vector Machines (classification).</li>
<li>Kernel Ridge Regression (regression).</li>
<li>Gaussian RBF kernel: \(K(x,x') = \exp(-|x-x'|^2 / (2\sigma^2))\).</li>
<li>Polynomial kernel: \(K(x,x') = (x^T x' + c)^d\).</li>
</ul><details class="solution-block"><summary>Further explanation on kernel methods</summary>
<h2>Why Kernel Methods Require Hilbert Space: Linear Separability and Non‑Separability</h2>
<h3>1. Linearly Separable</h3>
<p>Linearly separable means there exists a linear decision boundary (a line in 2D, a hyperplane in higher dimensions) that perfectly separates classes with no errors.</p>
<p>Geometric intuition (● positive, ○ negative; vertical line is the boundary):</p>
<pre>● ● ● | ○ ○ ○
● ● ● | ○ ○ ○
● ● ● | ○ ○ ○</pre>
<p>Formally, given training samples \((x_i, y_i)\) with \(x_i \in \mathbb{R}^d\), \(y_i \in \{+1, -1\}\), if there exist \(w\) and \(b\) such that for all samples:</p>
<p>\(y_i ( w^T x_i + b ) &gt; 0\)</p>
<p>then the dataset is linearly separable. This is the foundation of the perceptron, linear classifiers, and linear SVM.</p>
<h3>2. Linearly Non‑Separable</h3>
<p>Linearly non‑separable means no line or hyperplane can perfectly separate classes.</p>
<p>The classic example is XOR:</p>
<table>
<thead>
<tr><th>x₁</th><th>x₂</th><th>y</th></tr>
</thead>
<tbody>
<tr><td>0</td><td>0</td><td>0</td></tr>
<tr><td>1</td><td>1</td><td>0</td></tr>
<tr><td>0</td><td>1</td><td>1</td></tr>
<tr><td>1</td><td>0</td><td>1</td></tr>
</tbody>
</table>
<p>Geometric intuition:</p>
<pre>y
↑
| ● (0,1) ○ (1,1)
|
|
| ○ (0,0) ● (1,0)
+------------------------------→ x</pre>
<p>Linear models use:</p>
<p>\(f(x) = \mathrm{sign}( w^T x + b )\)</p>
<p>which cannot represent XOR’s non‑linear logic, e.g.:</p>
<p>\((x_1 \wedge \neg x_2) \lor (\neg x_1 \wedge x_2)\)</p>
<h3>3. What the Kernel Does</h3>
<p>Kernel methods use an implicit feature map to lift data to a higher‑dimensional space where it can become linearly separable.</p>
<p>Intuition (left: original space, right: higher‑dimensional space; vertical line is the new boundary):</p>
<pre>Original space:  Higher‑dimensional space:
● ○ ● ● ● ● | ○ ○ ○ ○
○ ● ● ● ● ● | ○ ○ ○ ○</pre>
<p>Mathematically, a kernel computes inner products:</p>
<p>\(K(x, z) = \langle \phi(x), \phi(z) \rangle\)</p>
<p>where \(\phi(x)\) is the implicit feature map. The kernel trick avoids explicitly constructing \(\phi(x)\).</p>
<h3>4. Why Kernel Must Live in a Hilbert Space</h3>
<p>A kernel is an inner product, so it must live in a space with a valid inner‑product structure. A Hilbert space is a complete inner‑product space, which guarantees geometry (norm, distance, angle) and existence of minimizers.</p>
<p>Completeness intuition:</p>
<pre>f₁ → f₂ → f₃ → ... → f*</pre>
<p>Typical kernel learning objective:</p>
<p>\(\min_{f \in \mathcal{H}} \mathrm{loss}(f) + \lambda\|f\|^2\)</p>
<p>If the space is not complete, the infimum may exist without any minimizing function.</p>
<p>The representer theorem also relies on Hilbert‑space structure:</p>
<p>\(f^*(x) = \sum_i \alpha_i K(x, x_i)\)</p>
<h3>5. RKHS: The Hilbert Space Induced by the Kernel</h3>
<p>For a positive definite kernel \(K\), there exists a Hilbert space \(\mathcal{H}\) such that:</p>
<p>\(K(x, z) = \langle K(x,\cdot), K(z,\cdot) \rangle_{\mathcal{H}}\)</p>
<p>This is the Reproducing Kernel Hilbert Space (RKHS), which satisfies the reproducing property:</p>
<p>\(f(x) = \langle f, K(x,\cdot) \rangle\)</p>
<h3>6. Unified Logic Chain</h3>
<pre>Linearly non‑separable
↓
Linear models fail
↓
Introduce feature map φ
↓
Kernel (inner product)
↓
Inner product needs completeness
↓
Hilbert space
↓
RKHS</pre>
<h3>7. Research‑Level One‑Sentence Summary</h3>
<p>When data are not linearly separable, kernels implicitly lift them to a higher‑dimensional space; because a kernel is an inner product and the existence/geometry of optimal solutions relies on completeness, kernels must be defined in a Hilbert space, which is the mathematical foundation of RKHS.</p>
</details><h2 id="Summary"><a class="headerlink" href="#Summary" title="Summary"></a>Summary</h2><ul>
<li>Backprop provides efficient gradients for deep networks.</li>
<li>CNNs exploit locality and parameter sharing for grid data.</li>
<li>Hilbert spaces underpin kernel methods via completeness and inner products.</li>
<li>Positive definite kernels enable the kernel trick and representer theorem.</li>
</ul><div class="post-categoris-bottom">
<div class="post-categoris-name">Math</div>
<ul>
<li class="me base">
<a class="post-categoris-bottom-link" href="/2026/02/05/Numeric-Methods/">1. Convolution and Kernel Operators</a>
</li>
</ul>
</div>
<!-- top型目录 -->
<!-- 分类文章 -->
</div><div class="post-content-inner-space">
<div class="space-toc-main animate__animated animate__fadeInUp">
<ol class="space-toc"><li class="space-toc-item space-toc-level-2"><a class="space-toc-link" href="#Reading-List"><span class="space-toc-text">Reading List</span></a></li><li class="space-toc-item space-toc-level-2"><a class="space-toc-link" href="#Convolution-Cross-Correlation"><span class="space-toc-text">Understanding Convolution and Cross-Correlation</span></a></li><li class="space-toc-item space-toc-level-2"><a class="space-toc-link" href="#Neural-Networks"><span class="space-toc-text">1. Neural Networks</span></a></li><li class="space-toc-item space-toc-level-2"><a class="space-toc-link" href="#CNN"><span class="space-toc-text">2. Convolutional Neural Networks (CNNs)</span></a></li><li class="space-toc-item space-toc-level-2"><a class="space-toc-link" href="#Hilbert"><span class="space-toc-text">3. Hilbert Spaces</span></a></li><li class="space-toc-item space-toc-level-2"><a class="space-toc-link" href="#PD"><span class="space-toc-text">4. Positive Definite Functions and Kernels</span></a></li><li class="space-toc-item space-toc-level-2"><a class="space-toc-link" href="#Kernel-Methods"><span class="space-toc-text">5. Kernel Methods</span></a></li><li class="space-toc-item space-toc-level-2"><a class="space-toc-link" href="#Summary"><span class="space-toc-text">Summary</span></a></li></ol>
</div>
</div>
</div>
<!-- 评论 -->
</div>
</article>
</div>
</div>
<!-- 如果是home模式的话，不在首页就显示footer，如果不是home模式的话 所有都显示footer -->
<div class="footer-outer animate__animated animate__fadeInUp">
<div class="footer-inner">
<div class="footer-text">
<p>Artificial Intelligence |  Financial Engineering | Mathematics | Computer Science  <strong>Rongyan <i class="ri-copyright-line"></i> 2026</strong></p>
</div>
<div class="footer-contact">
<ul class="footer-ul">
<li class="footer-li">
<a href="https://github.com/RongyanYuan" target="_blank">
<i class="ri-github-line"></i>
</a>
</li>
<li class="footer-li">
<a href="mailto:adrianrongyanyun@gmail.com" target="_blank">
<i class="ri-mail-line"></i>
</a>
</li>
<li class="footer-li">
<a href="https://www.linkedin.com/in/rongyan-yuan-a076971b2/" target="_blank">
<i class="ri-linkedin-box-line"></i>
</a>
</li>
</ul>
</div>
</div>
</div>
<script src="/js/white.js"></script>
<script>
  window.MathJax = {
    tex: {inlineMath: [['\\(','\\)'], ['$', '$']]},
    svg: {fontCache: 'global'}
  };
</script>
<script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-svg.js"></script>
</body>
</html>
