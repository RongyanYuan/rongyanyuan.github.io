
<!DOCTYPE html>

<html lang="en">
<head>
<meta charset="utf-8"/>
<meta content="width=device-width, initial-scale=1.0" name="viewport"/>
<title>  Neural Networks, CNNs, and Kernel Methods |    Rongyan's Blog</title>
<meta content="Financial Engineering | Mathematics | Computer Science" name="description"/>
<!-- 标签页图标 -->
<!-- 图标库 -->
<link href="https://cdn.jsdelivr.net/npm/remixicon@2.2.0/fonts/remixicon.css" rel="stylesheet"/>
<!-- 动画库 -->
<link href="https://cdn.jsdelivr.net/gh/fushaolei/cdn-white@1.0/css/animate.css" rel="stylesheet"/>
<!-- css文件 -->
<link href="/css/white.css" rel="stylesheet"/>
<!-- 代码高亮 -->
<meta content="Hexo 6.2.0" name="generator"/></head>
<body>
<div class="menu-outer">
<div class="menu-inner">
<div class="menu-site-name animate__animated animate__fadeInUp">
<a href="/">
          Rongyan's Blog
        </a>
<div class="menu-redirect"><a class="nav-link back-topic-link" href="/categories/math/numeric-methods/">← Numerical Methods</a></div></div>
<div class="menu-group">
<ul class="menu-ul">
<a class="nav-link" href="/">
<li class="menu-li animate__animated animate__fadeInUp">
              HOME
            </li>
</a>
<a class="nav-link" href="/archives">
<li class="menu-li animate__animated animate__fadeInUp">
              BLOG
            </li>
</a>
<li class="menu-li animate__animated animate__fadeInUp" id="sort">
             CATEGORIES
             <div class="categories-outer" id="sort-div">
<ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/AboutMe/">AboutMe</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/ai/">AI</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs/">CS</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/math/">Math</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/quant/">Quant</a></li></ul>
</div>
</li>
<a href="/search">
<li class="menu-li animate__animated animate__fadeInUp">
<i class="ri-search-line"></i>
</li>
</a>
<li class="menu-li animate__animated animate__fadeInUp lang-switcher-desktop">
<a href="/2026/02/05/Numeric-Methods/index.html">EN</a> |
            <a href="/2026/02/05/Numeric-Methods/index-zh.html">中文</a>
</li>
<li class="menu-li animate__animated animate__fadeInUp" id="mobile-menu">
<i class="ri-menu-line"></i>
</li>
</ul>
</div>
</div>
</div>
<div class="animate__animated animate__fadeIn" id="mobile-main">
<div class="mobile-menu-inner">
<div class="mobile-menu-site-name animate__animated animate__fadeInUp">
<a href="/">
        Rongyan's Blog
      </a>
</div>
<div class="mobile-menu-group" id="mobile-close">
<i class="ri-close-line"></i>
</div>
</div>
<div class="mobile-menu-div">
<a class="mobile-nav-link" href="/">
<div class="mobile-menu-child animate__animated animate__fadeInUp">
<span>HOME</span>
</div>
</a>
<a class="mobile-nav-link" href="/archives">
<div class="mobile-menu-child animate__animated animate__fadeInUp">
<span>BLOG</span>
</div>
</a>
<div class="mobile-menu-child animate__animated animate__fadeInUp lang-switcher-mobile">
<a href="/2026/02/05/Numeric-Methods/index.html">EN</a> |
      <a href="/2026/02/05/Numeric-Methods/index-zh.html">中文</a>
</div>
<a href="/search">
<div class="mobile-menu-child animate__animated animate__fadeInUp">
<i class="ri-search-line"></i>
</div>
</a>
</div>
</div>
<div class="body-outer">
<div class="body-inner">
<article class="post-inner">
<div class="post-content-outer">
<div class="post-intro">
<div class="post-title animate__animated animate__fadeInUp">Neural Networks, CNNs, and Kernel Methods</div>
<div class="meta-intro animate__animated animate__fadeInUp">Feb 05 2026</div>
</div>
<div class="post-content-inner">
<div class="post-content-inner-space">
</div>
<div class="post-content-main animate__animated animate__fadeInUp"><h1 id="NN-CNN-Kernel-Notes"><a class="headerlink" href="#NN-CNN-Kernel-Notes" title="Gist"></a>Gist</h1><p>These notes cover neural networks and backpropagation, CNN design details, and kernel methods with Hilbert space foundations. 
          All formulas are written in standard \(\LaTeX\) math notation.</p><h2 id="Reading-List"><a class="headerlink" href="#Reading-List" title="Reading List"></a>Reading List</h2><ul>
<li><a href="/pdf/lecture20.pdf" target="_blank">Deep Learning and CNN by CMU</a></li>
<li><a href="/pdf/lecture1_whatIsRKHS.pdf" target="_blank">Lecture 1: What is RKHS?</a></li>
</ul><h2 id="Convolution-Cross-Correlation"><a class="headerlink" href="#Convolution-Cross-Correlation" title="Understanding Convolution and Cross-Correlation"></a>Understanding Convolution and Cross-Correlation</h2><h3>1. Convolution Definition (Continuous and Discrete)</h3><p>Convolution is an operation that combines two functions (or signals) to produce a third. In signal processing and image processing, it combines an input signal with a kernel (filter) to create an output.</p><ul>
<li><strong>Continuous convolution</strong>: defined using an integral.</li>
<li><strong>Discrete convolution</strong>: defined using a sum.</li>
</ul><h3>2. Continuous Convolution</h3><p>In continuous convolution, the operation combines two functions, \( f(t) \) and \( g(t) \), and is defined as:</p><p>\[
(f * g)(t) = \int_{-\infty}^{\infty} f(\tau) g(t - \tau) d\tau
\]</p><h4>Breaking it down</h4><ul>
<li>\( f(t) \): the input function (signal).</li>
<li>\( g(t) \): the kernel (filter or pattern).</li>
<li>\( t \): the current time/position in the output.</li>
<li>\( \tau \): integration variable representing shifts of \( f(t) \).</li>
<li>\( g(t-\tau) \): the kernel is shifted and <strong>flipped</strong> (this makes it convolution rather than correlation).</li>
</ul><h3>3. Discrete Convolution</h3><p>For discrete signals, the formula is:</p><p>\[
(f * g)[n] = \sum_{m=-\infty}^{\infty} f[m] \cdot g[n - m]
\]</p><h4>Breaking it down</h4><ul>
<li>\( f[m] \): input signal at index \( m \).</li>
<li>\( g[n-m] \): shifted, flipped kernel.</li>
<li>\( n \): output index.</li>
</ul><h3>4. Why Is the Kernel Flipped?</h3><ul>
<li>Flipping is part of the formal definition of convolution.</li>
<li>It ensures symmetry (commutativity) and aligns with Fourier properties.</li>
<li>It makes convolution consistent with the Fourier transform identity: convolution in time equals multiplication in frequency.</li>
</ul><h3>5. Convolution vs Cross-Correlation</h3><ul>
<li><strong>Convolution</strong> (signal processing): kernel is flipped.</li>
<li><strong>Cross-correlation</strong> (CNNs): kernel is not flipped. In deep learning, this is fine because kernels are learned directly.</li>
</ul><h3>6. Summary</h3><ul>
<li>Convolution combines an input signal and a kernel to produce an output.</li>
<li>Continuous convolution uses integrals; discrete uses sums.</li>
<li>Kernel flipping is essential in convolution, but CNNs typically use cross-correlation.</li>
</ul><h2 id="Neural-Networks"><a class="headerlink" href="#Neural-Networks" title="Neural Networks"></a>1. Neural Networks</h2><h3 id="NN-Intro"><a class="headerlink" href="#NN-Intro" title="Introduction"></a>Introduction</h3><p>A feed‑forward neural network defines a function \(f_\theta: \mathbb{R}^d \to \mathbb{R}^k\) by composing affine transforms and nonlinearities:</p><p>\(a^{(0)} = x\), \(z^{(l)} = W^{(l)} a^{(l-1)} + b^{(l)}\), \(a^{(l)} = \sigma(z^{(l)})\), and \(f_\theta(x)=a^{(L)}\).</p><h3 id="Backprop"><a class="headerlink" href="#Backprop" title="Backpropagation"></a>Backpropagation</h3><p>Given loss \(\mathcal{L}(f_\theta(x), y)\), backprop computes gradients efficiently using the chain rule.</p><ul>
<li>Output layer error: \(\delta^{(L)} = \nabla_{a^{(L)}} \mathcal{L} \odot \sigma'(z^{(L)})\).</li>
<li>Hidden layer error: \(\delta^{(l)} = (W^{(l+1)})^T \delta^{(l+1)} \odot \sigma'(z^{(l)})\).</li>
<li>Gradients: \(\nabla_{W^{(l)}} \mathcal{L} = \delta^{(l)} (a^{(l-1)})^T\), \(\nabla_{b^{(l)}} \mathcal{L} = \delta^{(l)}\).</li>
</ul><h2 id="CNN"><a class="headerlink" href="#CNN" title="Convolutional Neural Networks"></a>2. Convolutional Neural Networks (CNNs)</h2><h3 id="CNN-Intro"><a class="headerlink" href="#CNN-Intro" title="Introduction"></a>Introduction</h3><p>CNNs are specialized for grid‑structured data (images, time‑series). They exploit local connectivity and parameter sharing to reduce complexity.</p><h3 id="Receptive-Field"><a class="headerlink" href="#Receptive-Field" title="Receptive Field"></a>Receptive Field</h3><p>The receptive field is the input region that influences a particular feature map activation. With kernel size \(k\), stride \(s\), and depth \(L\), receptive fields grow layer by layer.</p><p>If layer \(l\) has receptive field size \(R_l\), then for a subsequent conv layer:</p><p>\(R_{l+1} = R_l + (k_{l+1} - 1) \cdot \prod_{i=1}^{l} s_i\).</p><h4 id="RF-Problem"><a class="headerlink" href="#RF-Problem" title="Receptive Field Problem"></a>Practice Problem (Receptive Field)</h4><p>A CNN has three conv layers. Layer 1: kernel 3, stride 1. Layer 2: kernel 3, stride 2. Layer 3: kernel 5, stride 1. What is the receptive field size at layer 3?</p><details class="solution-block">
<summary>Show Solution</summary>
<p><strong>Answer:</strong> Use \(R_{l+1} = R_l + (k_{l+1}-1)\prod_{i=1}^{l} s_i\) with \(R_1=3\).</p>
<p>Layer 2: \(R_2 = 3 + (3-1)\cdot 1 = 5\). Layer 3: \(R_3 = 5 + (5-1)\cdot (1\cdot 2)=5+8=13\).</p>
<p><strong>Receptive field = 13</strong>.</p>
</details><h3 id="Kernel-Size"><a class="headerlink" href="#Kernel-Size" title="Kernel Size and Output Shape"></a>Kernel Size and Output Shape</h3><p>For input spatial size \(n\), kernel size \(k\), padding \(p\), stride \(s\):</p><p>\(n_{out} = \left\lfloor \frac{n + 2p - k}{s} \right\rfloor + 1\).</p><p>To keep size unchanged (“same” padding): choose \(p = \frac{k-1}{2}\) when \(k\) is odd.</p><h4 id="KS-Problem"><a class="headerlink" href="#KS-Problem" title="Kernel Size/Output Shape Problem"></a>Practice Problem (Kernel Size &amp; Output Shape)</h4><p>Input is \(64	imes64\). Apply conv with kernel \(k=7\), padding \(p=3\), stride \(s=2\). What is the output size?</p><details class="solution-block">
<summary>Show Solution</summary>
<p>\(n_{out} = \left\lfloor rac{n+2p-k}{s} 
ight
floor + 1 = \left\lfloor rac{64+6-7}{2} 
ight
floor + 1 = \left\lfloor rac{63}{2} 
ight
floor + 1 = 31 + 1 = 32\).</p>
<p><strong>Output size = 32 × 32</strong>.</p>
</details><h3 id="Num-Kernels"><a class="headerlink" href="#Num-Kernels" title="Number of Kernels per Layer"></a>Number of Kernels per Layer</h3><p>If a conv layer has \(C_{in}\) input channels and \(C_{out}\) kernels, then:</p><ul>
<li>Each kernel has shape \(k \times k \times C_{in}\).</li>
<li>The number of output feature maps equals \(C_{out}\).</li>
<li>Total parameters: \(C_{out} \cdot (k^2 C_{in} + 1)\) (including bias).</li>
</ul><h4 id="NK-Problem"><a class="headerlink" href="#NK-Problem" title="Number of Kernels Problem"></a>Practice Problem (Number of Kernels)</h4><p>A conv layer takes \(C_{in}=64\) channels and uses \(C_{out}=128\) kernels of size \(3	imes3\). How many parameters does the layer have (including bias)?</p><details class="solution-block">
<summary>Show Solution</summary>
<p>Parameters per kernel: \(3\cdot3\cdot64 + 1 = 577\). Total: \(128\cdot577 = 73856\).</p>
<p><strong>Total parameters = 73,856</strong>.</p>
</details><h3 id="CNN-Pros-Cons"><a class="headerlink" href="#CNN-Pros-Cons" title="Pros and Cons"></a>Pros and Cons of CNNs</h3><ul>
<li><strong>Pros:</strong> parameter sharing, spatial inductive bias, strong performance on vision.</li>
<li><strong>Cons:</strong> less effective for global dependencies, requires many labeled images, limited rotation invariance unless augmented.</li>
</ul><h3 id="Cross-Correlation"><a class="headerlink" href="#Cross-Correlation" title="Cross-Correlation"></a>Cross‑Correlation in CNNs</h3><p>Most CNN libraries implement <strong>cross‑correlation</strong> instead of mathematical convolution. For input \(x\) and kernel \(k\):</p><p>\[(x \star k)[i,j] = \sum_{u=0}^{m-1} \sum_{v=0}^{n-1} x[i+u, j+v] \, k[u,v]\]</p><p>Convolution would flip the kernel: \((x * k)[i,j] = \sum_{u,v} x[i+u, j+v] \, k[m-1-u, n-1-v]\). In practice, the network learns the kernel weights, so using cross‑correlation is equivalent up to learned parameters.</p><h4 id="Cross-Correlation-Problem"><a class="headerlink" href="#Cross-Correlation-Problem" title="Practice Problem"></a>Practice Problem</h4><p>Given input \(x\) and kernel \(k\):</p><p>\(x=\begin{bmatrix}1 &amp; 2 &amp; 0 \\ 0 &amp; 1 &amp; 3 \\ 2 &amp; 1 &amp; 1\end{bmatrix}\), \(k=\begin{bmatrix}1 &amp; 0 \\ -1 &amp; 2\end{bmatrix}\).</p><p>Compute the top‑left output of cross‑correlation (no padding, stride 1): \((x \star k)[0,0]\).</p><details class="solution-block">
<summary>Show Solution</summary>
<p><strong>Solution:</strong> \((x \star k)[0,0] = 1\cdot1 + 2\cdot0 + 0\cdot(-1) + 1\cdot2 = 3\).</p>
</details><figure>
<img alt="CNN convolution example" src="/images/cnn-kernel-example.png"/>
<figcaption>Example of convolution kernels applied across channels to produce feature maps.</figcaption>
</figure>
<details class="solution-block"><summary>Conv1 vs Conv2 Filters: Q&amp;A</summary><h3>Question</h3><p><strong>What is the difference between the filter matrix of Conv2 and the filter matrix of Conv1 in a Convolutional Neural Network (CNN)? Explain both the structural and conceptual differences.</strong></p><hr/><h3>Answer</h3><p>The filter matrices of <strong>Conv1</strong> and <strong>Conv2</strong> differ in both <strong>structure</strong> and <strong>function</strong>, reflecting hierarchical feature learning in CNNs.</p><h4>1. Structural Difference (Filter Dimensions)</h4><h5>Conv1 (First Convolutional Layer)</h5><ul>
<li><strong>Input channels:</strong> 1 (grayscale image)</li>
<li><strong>Filter size:</strong> \(4 \times 4\)</li>
<li><strong>Number of filters:</strong> 16</li>
</ul><p>Each Conv1 filter has shape:</p><p>\[
\boxed{4 \times 4 \times 1}
\]</p><p>Total number of weights in Conv1:</p><p>\[
4 \times 4 \times 1 \times 16 = 256
\]</p><h5>Conv2 (Second Convolutional Layer)</h5><ul>
<li><strong>Input channels:</strong> 16 (output feature maps from Conv1)</li>
<li><strong>Filter size:</strong> \(4 \times 4\)</li>
<li><strong>Number of filters:</strong> 16</li>
</ul><p>Each Conv2 filter has shape:</p><p>\[
\boxed{4 \times 4 \times 16}
\]</p><p>Total number of weights in Conv2:</p><p>\[
4 \times 4 \times 16 \times 16 = 4096
\]</p><h4>2. Mathematical Interpretation</h4><h5>Conv1 Operation</h5><p>At a spatial location \((i,j)\):</p><p>\[
y_k(i,j) = \sum_{u,v} W_k(u,v)\,x(i+u,j+v)
\]</p><p>No summation over channels; filters operate directly on raw pixels.</p><h5>Conv2 Operation</h5><p>At a spatial location \((i,j)\):</p><p>\[
y_k(i,j) = \sum_{c=1}^{16}\sum_{u,v} W_{k,c}(u,v)\,x_c(i+u,j+v)
\]</p><p>There is a summation over all input channels, combining information from multiple feature maps.</p><h4>3. Conceptual Difference (Feature Hierarchy)</h4><p><strong>Conv1 filters learn:</strong> low‑level features (edges, corners, strokes, local contrast).</p><p><strong>Conv2 filters learn:</strong> higher‑level features (combinations of edges, curves, parts of digits/objects).</p><h4>4. Intuition</h4><ul>
<li>Conv1 answers: “Where are the basic visual patterns?”</li>
<li>Conv2 answers: “How do these patterns combine to form meaningful shapes?”</li>
</ul><h4>5. Key Takeaway (Exam‑Ready)</h4><blockquote>
<p>Conv1 filters have size \(4\times4\times1\) and extract low‑level features directly from pixel values, whereas Conv2 filters have size \(4\times4\times16\), span all previous feature maps, and learn higher‑level feature compositions by combining earlier features.</p>
</blockquote><h4>6. Golden Rule to Remember</h4><p>\[
\boxed{\text{Convolution filter size} = (\text{kernel height}) \times (\text{kernel width}) \times (\text{number of input channels})}
\]</p><p>Each filter produces <strong>one</strong> output channel, and deeper layers learn increasingly abstract representations.</p></details><h2 id="Hilbert"><a class="headerlink" href="#Hilbert" title="Hilbert Spaces"></a>3. Hilbert Spaces</h2><h3 id="Hilbert-Def"><a class="headerlink" href="#Hilbert-Def" title="Definition"></a>Definition</h3><p>A Hilbert space is a complete inner‑product space \((\mathcal{H}, \langle \cdot, \cdot \rangle)\) where the norm \(|x| = \sqrt{\langle x, x \rangle}\) makes every Cauchy sequence converge.</p><h3 id="Hilbert-Examples"><a class="headerlink" href="#Hilbert-Examples" title="Examples"></a>Examples</h3><ul>
<li>\(\mathbb{R}^n\) with dot product.</li>
<li>\(L^2([a,b])\): square‑integrable functions with \(\langle f,g \rangle = \int_a^b f(x)g(x)\,dx\).</li>
<li>Sequence space \(\ell^2\): \(\sum_i x_i^2 &lt; \infty\).</li>
</ul><details class="solution-block">
<summary>Completeness of \(L^{2}\): proof</summary>
<h1>Completeness of \(L^{2}\): \(L^{2}(\Omega,\Sigma,\mu)\) is a Hilbert space</h1>
<p>Let \((\Omega,\Sigma,\mu)\) be a measure space. Define</p>
<p>\[
L^{2}(\Omega) := \left\{ f:\Omega\to\mathbb{C}\text{ measurable }:\ \int_\Omega |f|^{2}\,d\mu &lt; \infty \right\}\Big/\sim,
\]</p>
<p>where \(f\sim g\) iff \(f=g\) \(\mu\)-a.e.</p>
<p>Equip \(L^{2}\) with the inner product</p>
<p>\[
\langle f,g\rangle := \int_\Omega f\overline{g}\,d\mu,
\]</p>
<p>and norm \(\|f\|_{2} := \big(\int_\Omega |f|^{2}\,d\mu\big)^{1/2}\).</p>
<p>We prove: <strong>every Cauchy sequence in \((L^{2},\|\cdot\|_{2})\) converges in \(\|\cdot\|_{2}\)</strong>.<br/>
Hence \(L^{2}\) is complete as a normed space; together with the inner product, it is a <strong>Hilbert space</strong>.</p>
<hr/>
<h2>Theorem</h2>
<p>\((L^{2}(\Omega),\langle\cdot,\cdot\rangle)\) is a Hilbert space.</p>
<h3>Proof (completeness of \(L^{2}\))</h3>
<p>Let \((f_n)\) be a Cauchy sequence in \(L^{2}(\Omega)\).<br/>
So for every \(\varepsilon&gt;0\) there exists \(N\) such that for all \(m,n\ge N\),</p>
<p>\[
\|f_m-f_n\|_{2} &lt; \varepsilon.
\]</p>
<h3>Step 1: Extract a rapidly Cauchy subsequence</h3>
<p>Since \((f_n)\) is Cauchy, we can choose a subsequence \((f_{n_k})_{k\ge 1}\) such that</p>
<p>\[
\|f_{n_{k+1}} - f_{n_k}\|_{2} \le 2^{-k}\quad \text{for all }k\ge 1.
\]</p>
<p>Define the increments</p>
<p>\[
h_k := f_{n_{k+1}} - f_{n_k}\ \in L^{2}.
\]</p>
<p>Then</p>
<p>\[
\|h_k\|_{2} \le 2^{-k}
\quad\Longrightarrow\quad
\sum_{k=1}^{\infty} 2^{k}\|h_k\|_2^2 \le \sum_{k=1}^{\infty} 2^{k}\cdot 2^{-2k} = \sum_{k=1}^{\infty} 2^{-k} &lt; \infty.
\]</p>
<h3>Step 2: Show the subsequence converges pointwise a.e.</h3>
<p>Consider the nonnegative measurable function</p>
<p>\[
H(x):=\sum_{k=1}^{\infty} 2^{k}\,|h_k(x)|^{2}.
\]</p>
<p>By Tonelli’s theorem,</p>
<p>\[
\int_\Omega H\,d\mu
= \sum_{k=1}^{\infty} 2^{k}\int_\Omega |h_k|^{2}\,d\mu
= \sum_{k=1}^{\infty} 2^{k}\|h_k\|_2^2 &lt; \infty.
\]</p>
<p>Therefore \(H(x)&lt;\infty\) for \(\mu\)-a.e. \(x\).</p>
<p>Fix such an \(x\). For integers \(m&lt;\ell\), by Cauchy–Schwarz:</p>
<p>\[
\sum_{k=m}^{\ell} |h_k(x)|
= \sum_{k=m}^{\ell} 2^{-k/2}\,\big(2^{k/2}|h_k(x)|\big)
\le \left(\sum_{k=m}^{\ell} 2^{-k}\right)^{1/2}
     \left(\sum_{k=m}^{\ell} 2^{k}|h_k(x)|^{2}\right)^{1/2}.
\]</p>
<p>Since \(\sum_{k=m}^{\ell} 2^{-k}\le 2^{-m+1}\) and \(\sum_{k=m}^{\ell}2^{k}|h_k(x)|^{2}\le H(x)&lt;\infty\), we get</p>
<p>\[
\sum_{k=m}^{\ell} |h_k(x)|
\le \sqrt{2^{-m+1}}\cdot \sqrt{H(x)} \xrightarrow[m\to\infty]{} 0.
\]</p>
<p>Hence \(\sum_{k=1}^{\infty} h_k(x)\) is absolutely convergent, so the partial sums</p>
<p>\[
S_K(x) := \sum_{k=1}^{K} h_k(x)
\]</p>
<p>converge as \(K\to\infty\) for \(\mu\)-a.e. \(x\).</p>
<p>Define</p>
<p>\[
f(x) := f_{n_1}(x) + \sum_{k=1}^{\infty} h_k(x)
\quad\text{for a.e. }x.
\]</p>
<p>Then for \(\mu\)-a.e. \(x\),</p>
<p>\[
f_{n_{K+1}}(x) = f_{n_1}(x) + \sum_{k=1}^{K} h_k(x) \xrightarrow[K\to\infty]{} f(x).
\]</p>
<p>So \(f_{n_k}\to f\) pointwise a.e. along the subsequence.</p>
<h3>Step 3: Show the subsequence converges to \(f\) in \(L^{2}\)</h3>
<p>For each fixed \(m\ge 1\), consider the tails</p>
<p>\[
T_{m,r}(x) := \sum_{k=m}^{r} h_k(x) = f_{n_{r+1}}(x) - f_{n_m}(x).
\]</p>
<p>Pointwise a.e., \(T_{m,r}(x)\to f(x)-f_{n_m}(x)\) as \(r\to\infty\).
By Fatou’s lemma applied to \(|T_{m,r}|^2\),</p>
<p>\[
\|f-f_{n_m}\|_2^2
= \int_\Omega \left| \lim_{r\to\infty} T_{m,r} \right|^2 d\mu
\le \liminf_{r\to\infty} \int_\Omega |T_{m,r}|^2\,d\mu
= \liminf_{r\to\infty} \|T_{m,r}\|_2^2.
\]</p>
<p>But by the triangle inequality in \(L^2\),</p>
<p>\[
\|T_{m,r}\|_2
= \left\| \sum_{k=m}^{r} h_k \right\|_2
\le \sum_{k=m}^{r} \|h_k\|_2
\le \sum_{k=m}^{\infty} 2^{-k}
= 2^{-m+1}.
\]</p>
<p>Thus \(\|T_{m,r}\|_2^2 \le 2^{-2m+2}\) for all \(r\ge m\), hence</p>
<p>\[
\|f-f_{n_m}\|_2^2 \le 2^{-2m+2}
\quad\Longrightarrow\quad
\|f-f_{n_m}\|_2 \le 2^{-m+1}\xrightarrow[m\to\infty]{}0.
\]</p>
<p>So the subsequence \(f_{n_m}\to f\) in \(L^2\). In particular, \(f\in L^2\).</p>
<h3>Step 4: Lift convergence from subsequence to the whole sequence</h3>
<p>Since \((f_n)\) is Cauchy in \(L^2\), for any \(\varepsilon&gt;0\) choose \(N\) such that for all \(n\ge N\),</p>
<p>\[
\|f_n - f_N\|_2 &lt; \varepsilon/2.
\]</p>
<p>Choose \(m\) with \(n_m\ge N\) and \(\|f_{n_m}-f\|_2 &lt; \varepsilon/2\).
Then for all \(n\ge N\),</p>
<p>\[
\|f_n - f\|_2
\le \|f_n - f_{n_m}\|_2 + \|f_{n_m}-f\|_2
&lt; \varepsilon/2 + \varepsilon/2
= \varepsilon.
\]</p>
<p>Therefore \(f_n\to f\) in \(L^2\). This proves <strong>every Cauchy sequence converges</strong> in \(L^2\).</p>
<p>Hence \((L^2,\|\cdot\|_2)\) is complete, and with the inner product \(\langle\cdot,\cdot\rangle\), it is a <strong>Hilbert space</strong>. ∎</p>
<hr/>
<h2>Notes (what was essential)</h2>
<ul>
<li>The subsequence choice \(\|f_{n_{k+1}}-f_{n_k}\|_2\le 2^{-k}\) gives a “summable control”.</li>
<li>The weighted series \(\sum 2^k |h_k|^2\) has finite integral, so it is finite a.e.</li>
<li>Cauchy–Schwarz with weights converts that into pointwise convergence of \(\sum h_k(x)\).</li>
<li>Fatou + triangle inequality yields \(L^2\) convergence.</li>
</ul>
</details><h2 id="PD"><a class="headerlink" href="#PD" title="Positive Definite Functions"></a>4. Positive Definite Functions and Kernels</h2><p>A kernel is a similarity function that acts like an inner product in some (possibly infinite‑dimensional) feature space, and it is in a Hilbert space.</p><h3 id="PD-Def"><a class="headerlink" href="#PD-Def" title="Definition"></a>Definition</h3><p>A function \(K: X \times X \to \mathbb{R}\) is <strong>positive definite</strong> if for any \(x_1,\ldots,x_n\) and any \(a \in \mathbb{R}^n\):</p><p>\(\sum_{i=1}^n \sum_{j=1}^n a_i a_j K(x_i, x_j) \ge 0\).</p><p><strong>Example:</strong> For the RBF kernel \(K(x,x')=\exp(-|x-x'|^2/(2\sigma^2))\), the Gram matrix \(K\) is symmetric and all eigenvalues are non‑negative for any finite sample, so it is positive definite.</p><h3 id="PD-Test"><a class="headerlink" href="#PD-Test" title="How to Test Positive Definiteness"></a>How to Test Positive Definiteness</h3><ul>
<li>Form Gram matrix \(K_{ij} = K(x_i, x_j)\); check that \(K\) is symmetric and all eigenvalues are non‑negative.</li>
<li>Show that \(K(x, x') = \langle \phi(x), \phi(x') \rangle\) for some feature map \(\phi\).</li>
<li>Use closure properties: sums, products, and limits of p.d. kernels are p.d.</li>
</ul><h4 id="PD-Test-Example"><a class="headerlink" href="#PD-Test-Example" title="Example"></a>Example</h4><p>Let \(K(x,x') = \exp(-|x-x'|^2)\) (RBF kernel). For two distinct points with distance \(d&gt;0\), the Gram matrix is</p><p>\(K = \begin{bmatrix}1 &amp; e^{-d^2} \\ e^{-d^2} &amp; 1\end{bmatrix}\).</p><details class="solution-block">
<summary>Show Check</summary>
<p>The eigenvalues are \(1\pm e^{-d^2}\). Since \(0&lt;e^{-d^2}&lt;1\), both eigenvalues are positive, so \(K\) is positive definite.</p>
</details>
<h2 id="Kernel-Methods"><a class="headerlink" href="#Kernel-Methods" title="Kernel Methods"></a>5. Kernel Methods</h2><h3 id="Kernel-Def"><a class="headerlink" href="#Kernel-Def" title="What is a Kernel"></a>What is a Kernel?</h3><h3 id="Kernel-Trick"><a class="headerlink" href="#Kernel-Trick" title="Kernel Trick"></a>Kernel Trick</h3><p>Algorithms relying on dot products can be rewritten in terms of \(K(x, x')\), avoiding explicit feature mappings.</p><details class="solution-block"><summary>Meaning of K(x,·) and Reproducing Property</summary><p>RKHS is the unique setting where evaluating a function is the same as taking an inner product.</p><h3>Meaning of \(K(x,\cdot)\) and the Reproducing Property (with a Concrete Numerical Example)</h3><hr/><h4>1. What does the dot “\(\cdot\)” mean in \(K(x,\cdot)\)?</h4><h5>Definition</h5><p>The dot “\(\cdot\)” is a <strong>placeholder for the argument of a function</strong>.</p><p>Given a kernel</p><p>\[
K : X \times X \to \mathbb{R},
\]</p><p>the expression</p><p>\[
K(x,\cdot)
\]</p><p>means: fix the first argument at \(x\), and view the kernel as a <strong>function of the second argument</strong>.</p><p>Formally,</p><p>\[
K(x,\cdot) : z \longmapsto K(x,z).
\]</p><p>So \(K(x,\cdot)\) is <strong>not a number</strong> — it is a <strong>function</strong>.</p><hr/><h4>2. Why do we need \(K(x,\cdot)\)?</h4><p>In Reproducing Kernel Hilbert Space (RKHS) theory:</p><ul>
<li>The elements of the Hilbert space \(\mathcal H\) are <strong>functions</strong>.</li>
<li>For every \(x \in X\), the function \(K(x,\cdot)\) itself belongs to \(\mathcal H\).</li>
</ul><p>That is,</p><p>\[
K(x,\cdot) \in \mathcal H.
\]</p><p>This is essential for defining the <strong>reproducing property</strong>.</p><hr/><h4>3. The Reproducing Property (statement)</h4><p>Let \(\mathcal H\) be the RKHS associated with kernel \(K\). For every \(f \in \mathcal H\) and every \(x \in X\),</p><p>\[
\boxed{
 f(x) = \langle f,\; K(x,\cdot) \rangle_{\mathcal H}
}
\]</p><p>This means evaluating a function at a point is the same as taking an inner product with the kernel function \(K(x,\cdot)\).</p><hr/><h4>4. Intuition (before numbers)</h4><ul>
<li>In finite‑dimensional vector spaces, inner products extract coordinates.</li>
<li>In RKHS, inner products extract <strong>function values</strong>.</li>
</ul><p>So \(K(x,\cdot)\) plays the role of a <strong>generalized basis vector</strong> that “picks out” the value \(f(x)\).</p><hr/><h4>5. Simple symbolic example</h4><h5>Linear kernel</h5><p>Let</p><p>\[
K(x,z) = xz \quad \text{on } \mathbb{R}.
\]</p><p>Fix \(x = 3\). Then:</p><p>\[
K(3,\cdot)(z) = 3z.
\]</p><p>So \(K(3,\cdot)\) is the function \(z \mapsto 3z\).</p><hr/><h4>6. Concrete numerical example of the reproducing property</h4><h5>Step 1: Choose a kernel</h5><p>Use the <strong>linear kernel</strong>:</p><p>\[
K(x,z) = xz.
\]</p><p>The associated RKHS is the space of linear functions:</p><p>\[
f(z) = az
\]</p><p>with inner product:</p><p>\[
\langle f,g\rangle = a b \quad \text{if } f(z)=az,\ g(z)=bz.
\]</p><h5>Step 2: Choose a function \(f\)</h5><p>Let</p><p>\[
f(z) = 2z.
\]</p><h5>Step 3: Choose a point \(x\)</h5><p>Let</p><p>\[
x = 3.
\]</p><p>Then</p><p>\[
f(3) = 2 \times 3 = 6.
\]</p><h5>Step 4: Compute \(K(x,\cdot)\)</h5><p>\[
K(3,\cdot)(z) = 3z.
\]</p><h5>Step 5: Compute the inner product</h5><p>\[
\langle f,\; K(3,\cdot) \rangle
= \langle 2z,\; 3z \rangle
= 2 \times 3
= 6.
\]</p><h5>Step 6: Compare both sides</h5><p>\[
\boxed{
 f(3) = 6
 \quad=\quad
 \langle f,\; K(3,\cdot) \rangle
}
\]</p><p>✔ The reproducing property holds exactly.</p><hr/><h4>7. Another intuition: Gaussian kernel example (conceptual)</h4><p>For the Gaussian kernel</p><p>\[
K(x,z) = \exp\!\left(-\frac{(x-z)^2}{2\sigma^2}\right),
\]</p><ul>
<li>\(K(x,\cdot)\) is a Gaussian‑shaped function centered at \(x\).</li>
<li>Inner product with \(f\) measures alignment with that Gaussian.</li>
<li>That alignment equals \(f(x)\).</li>
</ul><hr/><h4>8. Common misconceptions (important)</h4><ul>
<li>❌ The dot does <strong>not</strong> mean multiplication.</li>
<li>❌ The dot does <strong>not</strong> mean inner product.</li>
<li>❌ The dot does <strong>not</strong> mean derivative.</li>
<li>✅ It means <strong>“the argument of the function”</strong>.</li>
</ul><hr/><h4>9. One‑line exam‑ready summary</h4><blockquote>
<p>In RKHS theory, \(K(x,\cdot)\) denotes the function obtained by fixing one argument of the kernel, and the reproducing property states that evaluating a function at \(x\) is equivalent to taking its inner product with \(K(x,\cdot)\).</p>
</blockquote><hr/><h4>10. Big‑picture takeaway</h4><blockquote>
<p><strong>Kernels do not just compare points — they generate functions that act as coordinate probes in an infinite‑dimensional Hilbert space.</strong></p>
</blockquote></details>
<h3 id="Representer-Theorem"><a class="headerlink" href="#Representer-Theorem" title="Representer Theorem"></a>Representer Theorem</h3><p>For regularized risk minimization in RKHS, the solution has the form:</p><p>\(f(x) = \sum_{i=1}^n \alpha_i K(x_i, x)\).</p><h4 id="Representer-Proof"><a class="headerlink" href="#Representer-Proof" title="Proof Sketch"></a>Proof Sketch</h4><p>Any \(f \in \mathcal{H}\) can be decomposed as \(f=f_\parallel+f_\perp\), where \(f_\parallel\) lies in the span of \(\{K(x_i,\cdot)\}\) and \(f_\perp\) is orthogonal. The data‑fit term depends only on \(f_\parallel\), while the RKHS norm satisfies \(\|f\|^2=\|f_\parallel\|^2+\|f_\perp\|^2\). Thus the minimizer must have \(f_\perp=0\).</p><h4 id="Representer-Problem"><a class="headerlink" href="#Representer-Problem" title="Practice Problem"></a>Practice Problem</h4><p>For kernel ridge regression with RBF kernel, write the solution form and the system you must solve.</p><details class="solution-block">
<summary>Show Solution</summary>
<p>The representer theorem gives \(f(x)=\sum_{i=1}^n \alpha_i K(x_i,x)\). Solve \((K+\lambda n I)\alpha = y\) for \(\alpha\), where \(K\) is the Gram matrix.</p>
</details><h3 id="Kernel-Apps"><a class="headerlink" href="#Kernel-Apps" title="Applications and Examples"></a>Applications and Examples</h3><ul>
<li>Support Vector Machines (classification).</li>
<li>Kernel Ridge Regression (regression).</li>
<li>Gaussian RBF kernel: \(K(x,x') = \exp(-|x-x'|^2 / (2\sigma^2))\).</li>
<li>Polynomial kernel: \(K(x,x') = (x^T x' + c)^d\).</li>
</ul><details class="solution-block"><summary>Further explanation on kernel methods</summary>
<h2>Why Kernel Methods Require Hilbert Space: Linear Separability and Non‑Separability</h2>
<h3>1. Linearly Separable</h3>
<p>Linearly separable means there exists a linear decision boundary (a line in 2D, a hyperplane in higher dimensions) that perfectly separates classes with no errors.</p>
<p>Geometric intuition (● positive, ○ negative; vertical line is the boundary):</p>
<pre>● ● ● | ○ ○ ○
● ● ● | ○ ○ ○
● ● ● | ○ ○ ○</pre>
<p>Formally, given training samples \((x_i, y_i)\) with \(x_i \in \mathbb{R}^d\), \(y_i \in \{+1, -1\}\), if there exist \(w\) and \(b\) such that for all samples:</p>
<p>\(y_i ( w^T x_i + b ) &gt; 0\)</p>
<p>then the dataset is linearly separable. This is the foundation of the perceptron, linear classifiers, and linear SVM.</p>
<h3>2. Linearly Non‑Separable</h3>
<p>Linearly non‑separable means no line or hyperplane can perfectly separate classes.</p>
<p>The classic example is XOR:</p>
<table>
<thead>
<tr><th>x₁</th><th>x₂</th><th>y</th></tr>
</thead>
<tbody>
<tr><td>0</td><td>0</td><td>0</td></tr>
<tr><td>1</td><td>1</td><td>0</td></tr>
<tr><td>0</td><td>1</td><td>1</td></tr>
<tr><td>1</td><td>0</td><td>1</td></tr>
</tbody>
</table>
<p>Geometric intuition:</p>
<pre>y
↑
| ● (0,1) ○ (1,1)
|
|
| ○ (0,0) ● (1,0)
+------------------------------→ x</pre>
<p>Linear models use:</p>
<p>\(f(x) = \mathrm{sign}( w^T x + b )\)</p>
<p>which cannot represent XOR’s non‑linear logic, e.g.:</p>
<p>\((x_1 \wedge \neg x_2) \lor (\neg x_1 \wedge x_2)\)</p>
<h3>3. What the Kernel Does</h3>
<p>Kernel methods use an implicit feature map to lift data to a higher‑dimensional space where it can become linearly separable.</p>
<p>Intuition (left: original space, right: higher‑dimensional space; vertical line is the new boundary):</p>
<pre>Original space:  Higher‑dimensional space:
● ○ ● ● ● ● | ○ ○ ○ ○
○ ● ● ● ● ● | ○ ○ ○ ○</pre>
<p>Mathematically, a kernel computes inner products:</p>
<p>\(K(x, z) = \langle \phi(x), \phi(z) \rangle\)</p>
<p>where \(\phi(x)\) is the implicit feature map. The kernel trick avoids explicitly constructing \(\phi(x)\).</p>
<h3>4. Why Kernel Must Live in a Hilbert Space</h3>
<p>A kernel is an inner product, so it must live in a space with a valid inner‑product structure. A Hilbert space is a complete inner‑product space, which guarantees geometry (norm, distance, angle) and existence of minimizers.</p>
<p>Completeness intuition:</p>
<pre>f₁ → f₂ → f₃ → ... → f*</pre>
<p>Typical kernel learning objective:</p>
<p>\(\min_{f \in \mathcal{H}} \mathrm{loss}(f) + \lambda\|f\|^2\)</p>
<p>If the space is not complete, the infimum may exist without any minimizing function.</p>
<p>The representer theorem also relies on Hilbert‑space structure:</p>
<p>\(f^*(x) = \sum_i \alpha_i K(x, x_i)\)</p>
<h3>5. RKHS: The Hilbert Space Induced by the Kernel</h3>
<p>For a positive definite kernel \(K\), there exists a Hilbert space \(\mathcal{H}\) such that:</p>
<p>\(K(x, z) = \langle K(x,\cdot), K(z,\cdot) \rangle_{\mathcal{H}}\)</p>
<p>This is the Reproducing Kernel Hilbert Space (RKHS), which satisfies the reproducing property:</p>
<p>\(f(x) = \langle f, K(x,\cdot) \rangle\)</p>
<h3>6. Unified Logic Chain</h3>
<pre>Linearly non‑separable
↓
Linear models fail
↓
Introduce feature map φ
↓
Kernel (inner product)
↓
Inner product needs completeness
↓
Hilbert space
↓
RKHS</pre>
<h3>7. Research‑Level One‑Sentence Summary</h3>
<p>When data are not linearly separable, kernels implicitly lift them to a higher‑dimensional space; because a kernel is an inner product and the existence/geometry of optimal solutions relies on completeness, kernels must be defined in a Hilbert space, which is the mathematical foundation of RKHS.</p>
</details><details class="solution-block">
<summary>SVM, Kernel, and Hilbert Space — Numerical Example</summary>
<h2>SVM, Kernel, and Hilbert Space — A Fully Numerical Example</h2>
<p>This example explains <strong>how SVMs use kernels and Hilbert spaces</strong> step by step, using <strong>explicit numbers</strong> throughout.</p>
<hr/>
<h3>1. A concrete dataset (not linearly separable)</h3>
<p>Consider a simple <strong>1D classification problem</strong>:</p>
<table>
<thead>
<tr><th>Index</th><th>Input \(x\)</th><th>Label \(y\)</th></tr>
</thead>
<tbody>
<tr><td>1</td><td>−1</td><td>−1</td></tr>
<tr><td>2</td><td>0</td><td>+1</td></tr>
<tr><td>3</td><td>1</td><td>−1</td></tr>
</tbody>
</table>
<p>Intuition on the number line:</p>
<pre>x^2
↑
| ● (-1,1) ● (1,1) ← negative class
|
| ○ (0,0) ← positive class
+------------------------→ x</pre>
<p>These points <strong>are linearly separable</strong> in this 2D space.</p>
<hr/>
<h3>4. Linear SVM in feature (Hilbert) space</h3>
<p>In feature space, the SVM is still linear:</p>
<p>\[
f(x) = \langle w, \phi(x) \rangle + b
\]</p>
<p>Choose:</p>
<p>\[
w = (0,1), \quad b = -0.5
\]</p>
<p>Evaluate explicitly:</p>
<ul>
<li>\(x=-1\):  \(f = (0,1)\cdot(-1,1) - 0.5 = 1 - 0.5 = 0.5 \Rightarrow y=-1\)</li>
<li>\(x=0\):  \(f = (0,1)\cdot(0,0) - 0.5 = -0.5 \Rightarrow y=+1\)</li>
<li>\(x=1\):  \(f = (0,1)\cdot(1,1) - 0.5 = 1 - 0.5 = 0.5 \Rightarrow y=-1\)</li>
</ul>
<p>Classification is perfect.</p>
<hr/>
<h3>5. Inner products in feature space (explicit numbers)</h3>
<p>Compute inner products:</p>
<p>\[
\langle \phi(x), \phi(x') \rangle
=
 x x' + x^2 x'^2
\]</p>
<p>Examples:</p>
<ul>
<li>\(x=-1,\ x'=1\): \((-1)(1) + (1)(1) = -1 + 1 = 0\)</li>
<li>\(x=1,\ x'=1\): \(1 + 1 = 2\)</li>
</ul>
<p>The SVM only depends on these <strong>inner products</strong>.</p>
<hr/>
<h3>6. Replace inner products with a kernel</h3>
<p>Define the kernel:</p>
<p>\[
K(x,x') = x x' + x^2 x'^2
\]</p>
<p>This satisfies:</p>
<p>\[
K(x,x') = \langle \phi(x), \phi(x') \rangle
\]</p>
<p>Now compute the <strong>Gram matrix</strong>:</p>
<p>\[
K =
\begin{pmatrix}
K(-1,-1) &amp; K(-1,0) &amp; K(-1,1) \\
K(0,-1) &amp; K(0,0) &amp; K(0,1) \\
K(1,-1) &amp; K(1,0) &amp; K(1,1)
\end{pmatrix}
=
\begin{pmatrix}
2 &amp; 0 &amp; 0 \\
0 &amp; 0 &amp; 0 \\
0 &amp; 0 &amp; 2
\end{pmatrix}
\]</p>
<p>This matrix is <strong>positive semidefinite</strong>, so the kernel defines a <strong>Hilbert space</strong>.</p>
<hr/>
<h3>7. SVM solution written purely with kernels</h3>
<p>By the Representer Theorem, the classifier has the form:</p>
<p>\[
f(x) = \sum_{i=1}^n \alpha_i y_i K(x_i,x) + b
\]</p>
<p>Assume the learned coefficients are:</p>
<p>\[
\alpha_1 = \alpha_3 = 0.5,\quad \alpha_2 = 1
\]</p>
<p>Then:</p>
<p>\[
f(x)
=
0.5(-1)K(-1,x)
+ 1(+1)K(0,x)
+ 0.5(-1)K(1,x)
+ b
\]</p>
<p>No explicit \(\phi(x)\) appears — <strong>only kernel evaluations</strong>.</p>
<hr/>
<h3>8. Where the Hilbert space appears</h3>
<p>The kernel induces a <strong>Reproducing Kernel Hilbert Space (RKHS)</strong> \(\mathcal H_K\) where:</p>
<ul>
<li>Functions have the form: \(f(x) = \sum_i c_i K(x_i,x)\)</li>
<li>Inner products satisfy: \(\langle K(x,\cdot), K(x',\cdot) \rangle_{\mathcal H_K} = K(x,x')\)</li>
<li>Function evaluation is reproduced: \(f(x) = \langle f, K(x,\cdot) \rangle_{\mathcal H_K}\)</li>
</ul>
<p>Thus:</p>
<ul>
<li>\(K(x,\cdot)\) acts like a feature vector</li>
<li>The SVM finds a <strong>maximum‑margin hyperplane</strong> in this Hilbert space</li>
</ul>
<hr/>
<h3>9. Final takeaway</h3>
<blockquote>
<p>The SVM is always linear in a kernel‑defined Hilbert space, and by replacing feature‑space inner products with kernel values, it learns nonlinear decision boundaries using only the Gram matrix.</p>
</blockquote>
<p>This example shows <strong>explicitly with numbers</strong> how kernels, Hilbert spaces, and SVMs fit together.</p>
</details><h2 id="Summary"><a class="headerlink" href="#Summary" title="Summary"></a>Summary</h2><ul>
<li>Backprop provides efficient gradients for deep networks.</li>
<li>CNNs exploit locality and parameter sharing for grid data.</li>
<li>Hilbert spaces underpin kernel methods via completeness and inner products.</li>
<li>Positive definite kernels enable the kernel trick and representer theorem.</li>
<li>Kernel methods can be applied to almost any supervised learning tasks.</li><li>However, how to choose the kernel has not been well understood yet.</li><li>It is also not well known how to choose the regularization parameter.</li></ul><div class="post-categoris-bottom">
<div class="post-categoris-name">Math</div>
<ul>
<li class="me base">
<a class="post-categoris-bottom-link" href="/2026/02/05/Numeric-Methods/">1. Convolution and Kernel Operators</a>
</li>
</ul>
</div>
<!-- top型目录 -->
<!-- 分类文章 -->
</div><div class="post-content-inner-space">
<div class="space-toc-main animate__animated animate__fadeInUp">
<ol class="space-toc"><li class="space-toc-item space-toc-level-2"><a class="space-toc-link" href="#Reading-List"><span class="space-toc-text">Reading List</span></a></li><li class="space-toc-item space-toc-level-2"><a class="space-toc-link" href="#Convolution-Cross-Correlation"><span class="space-toc-text">Understanding Convolution and Cross-Correlation</span></a></li><li class="space-toc-item space-toc-level-2"><a class="space-toc-link" href="#Neural-Networks"><span class="space-toc-text">1. Neural Networks</span></a></li><li class="space-toc-item space-toc-level-2"><a class="space-toc-link" href="#CNN"><span class="space-toc-text">2. Convolutional Neural Networks (CNNs)</span></a></li><li class="space-toc-item space-toc-level-2"><a class="space-toc-link" href="#Hilbert"><span class="space-toc-text">3. Hilbert Spaces</span></a></li><li class="space-toc-item space-toc-level-2"><a class="space-toc-link" href="#PD"><span class="space-toc-text">4. Positive Definite Functions and Kernels</span></a></li><li class="space-toc-item space-toc-level-2"><a class="space-toc-link" href="#Kernel-Methods"><span class="space-toc-text">5. Kernel Methods</span></a></li><li class="space-toc-item space-toc-level-2"><a class="space-toc-link" href="#Summary"><span class="space-toc-text">Summary</span></a></li></ol>
</div>
</div>
</div>
<!-- 评论 -->
</div>
</article>
</div>
</div>
<!-- 如果是home模式的话，不在首页就显示footer，如果不是home模式的话 所有都显示footer -->
<div class="footer-outer animate__animated animate__fadeInUp">
<div class="footer-inner">
<div class="footer-text">
<p>Artificial Intelligence |  Financial Engineering | Mathematics | Computer Science  <strong>Rongyan <i class="ri-copyright-line"></i> 2026</strong></p>
</div>
<div class="footer-contact">
<ul class="footer-ul">
<li class="footer-li">
<a href="https://github.com/RongyanYuan" target="_blank">
<i class="ri-github-line"></i>
</a>
</li>
<li class="footer-li">
<a href="mailto:adrianrongyanyun@gmail.com" target="_blank">
<i class="ri-mail-line"></i>
</a>
</li>
<li class="footer-li">
<a href="https://www.linkedin.com/in/rongyan-yuan-a076971b2/" target="_blank">
<i class="ri-linkedin-box-line"></i>
</a>
</li>
</ul>
</div>
</div>
</div>
<script src="/js/white.js"></script>
<script>
  window.MathJax = {
    tex: {inlineMath: [['\\(','\\)'], ['$', '$']]},
    svg: {fontCache: 'global'}
  };
</script>
<script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-svg.js"></script>
</body>
</html>
