
<!DOCTYPE html>

<html lang="zh-CN">
<head>
<meta charset="utf-8"/>
<meta content="width=device-width, initial-scale=1.0" name="viewport"/>
<title>  神经网络、CNN 与核方法 |    榕言的博客</title>
<meta content="Financial Engineering | Mathematics | Computer Science" name="description"/>
<!-- 标签页图标 -->
<!-- 图标库 -->
<link href="https://cdn.jsdelivr.net/npm/remixicon@2.2.0/fonts/remixicon.css" rel="stylesheet"/>
<!-- 动画库 -->
<link href="https://cdn.jsdelivr.net/gh/fushaolei/cdn-white@1.0/css/animate.css" rel="stylesheet"/>
<!-- css文件 -->
<link href="/css/white.css" rel="stylesheet"/>
<!-- 代码高亮 -->
<meta content="Hexo 6.2.0" name="generator"/></head>
<body>
<div class="menu-outer">
<div class="menu-inner">
<div class="menu-site-name animate__animated animate__fadeInUp">
<a href="/">
          榕言的博客
        </a>
<div class="menu-redirect"><a class="nav-link back-topic-link" href="/categories/math/numeric-methods/index-zh.html">← 数值方法</a></div></div>
<div class="menu-group">
<ul class="menu-ul">
<a class="nav-link" href="/">
<li class="menu-li animate__animated animate__fadeInUp">
              首页
            </li>
</a>
<a class="nav-link" href="/archives">
<li class="menu-li animate__animated animate__fadeInUp">
              博客
            </li>
</a>
<li class="menu-li animate__animated animate__fadeInUp" id="sort">
             分类
             <div class="categories-outer" id="sort-div">
<ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/AboutMe/">关于我</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/ai/">人工智能</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs/">计算机科学</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/math/">数学</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/quant/">量化</a></li></ul>
</div>
</li>
<a href="/search">
<li class="menu-li animate__animated animate__fadeInUp">
<i class="ri-search-line"></i>
</li>
</a>
<li class="menu-li animate__animated animate__fadeInUp lang-switcher-desktop">
<a href="/2026/02/05/Numeric-Methods/index.html">EN</a> |
            <a href="/2026/02/05/Numeric-Methods/index-zh.html">中文</a>
</li>
<li class="menu-li animate__animated animate__fadeInUp" id="mobile-menu">
<i class="ri-menu-line"></i>
</li>
</ul>
</div>
</div>
</div>
<div class="animate__animated animate__fadeIn" id="mobile-main">
<div class="mobile-menu-inner">
<div class="mobile-menu-site-name animate__animated animate__fadeInUp">
<a href="/">
        榕言的博客
      </a>
</div>
<div class="mobile-menu-group" id="mobile-close">
<i class="ri-close-line"></i>
</div>
</div>
<div class="mobile-menu-div">
<a class="mobile-nav-link" href="/">
<div class="mobile-menu-child animate__animated animate__fadeInUp">
<span>首页</span>
</div>
</a>
<a class="mobile-nav-link" href="/archives">
<div class="mobile-menu-child animate__animated animate__fadeInUp">
<span>博客</span>
</div>
</a>
<div class="mobile-menu-child animate__animated animate__fadeInUp lang-switcher-mobile">
<a href="/2026/02/05/Numeric-Methods/index.html">EN</a> |
      <a href="/2026/02/05/Numeric-Methods/index-zh.html">中文</a>
</div>
<a href="/search">
<div class="mobile-menu-child animate__animated animate__fadeInUp">
<i class="ri-search-line"></i>
</div>
</a>
</div>
</div>
<div class="body-outer">
<div class="body-inner">
<article class="post-inner">
<div class="post-content-outer">
<div class="post-intro">
<div class="post-title animate__animated animate__fadeInUp">神经网络、CNN 与核方法</div>
<div class="meta-intro animate__animated animate__fadeInUp">Feb 05 2026</div>
</div>
<div class="post-content-inner">
<div class="post-content-inner-space">
</div>
<div class="post-content-main animate__animated animate__fadeInUp"><h1 id="NN-CNN-Kernel-Notes"><a class="headerlink" href="#NN-CNN-Kernel-Notes" title="要点"></a>要点</h1><p>本笔记覆盖神经网络与反向传播、CNN 设计细节，以及核方法与 Hilbert 空间基础。公式采用标准 \(\LaTeX\) 形式。</p><h2 id="Reading-List"><a class="headerlink" href="#Reading-List" title="Reading List"></a>Reading List</h2><ul>
<li><a href="/pdf/lecture20.pdf" target="_blank">Deep Learning and CNN by CMU</a></li>
<li><a href="/pdf/lecture1_whatIsRKHS.pdf" target="_blank">Lecture 1: What is RKHS?</a></li>
</ul><h2 id="Convolution-Cross-Correlation"><a class="headerlink" href="#Convolution-Cross-Correlation" title="理解卷积与互相关"></a>理解卷积与互相关</h2><h3>1. 卷积定义（连续与离散）</h3><p>卷积是一种把两个函数（或信号）组合成第三个函数的运算。在信号处理与图像处理中，它用输入信号与卷积核（滤波器）产生输出。</p><ul>
<li><strong>连续卷积</strong>：用积分定义。</li>
<li><strong>离散卷积</strong>：用求和定义。</li>
</ul><h3>2. 连续卷积</h3><p>连续卷积将 \( f(t) \) 与 \( g(t) \) 组合为：</p><p>\[
(f * g)(t) = \int_{-\infty}^{\infty} f(\tau) g(t - \tau) d\tau
\]</p><h4>拆解含义</h4><ul>
<li>\( f(t) \)：输入信号。</li>
<li>\( g(t) \)：卷积核（滤波器）。</li>
<li>\( t \)：输出位置。</li>
<li>\( \tau \)：积分变量，表示对 \( f(t) \) 的平移。</li>
<li>\( g(t-\tau) \)：核被平移并<strong>翻转</strong>（这点区分了卷积与相关）。</li>
</ul><h3>3. 离散卷积</h3><p>离散信号的卷积公式：</p><p>\[
(f * g)[n] = \sum_{m=-\infty}^{\infty} f[m] \cdot g[n - m]
\]</p><h4>拆解含义</h4><ul>
<li>\( f[m] \)：第 \( m \) 个输入值。</li>
<li>\( g[n-m] \)：平移并翻转的核。</li>
<li>\( n \)：输出索引。</li>
</ul><h3>4. 为什么要翻转卷积核？</h3><ul>
<li>翻转是卷积的正式定义。</li>
<li>保证交换性（对称性）。</li>
<li>与傅里叶变换性质一致：时域卷积等于频域乘积。</li>
</ul><h3>5. 卷积 vs 互相关</h3><ul>
<li><strong>卷积</strong>（信号处理）：核会翻转。</li>
<li><strong>互相关</strong>（CNN 中常用）：核不翻转，且可通过训练学习合适的核。</li>
</ul><h3>6. 小结</h3><ul>
<li>卷积把输入与核结合得到输出。</li>
<li>连续卷积用积分，离散卷积用求和。</li>
<li>卷积要翻核，而 CNN 通常使用互相关。</li>
</ul><h2 id="Neural-Networks"><a class="headerlink" href="#Neural-Networks" title="神经网络"></a>1. 神经网络</h2><h3 id="NN-Intro"><a class="headerlink" href="#NN-Intro" title="介绍"></a>介绍</h3><p>前馈神经网络通过仿射变换与非线性激活的复合定义函数 \(f_\theta: \mathbb{R}^d \to \mathbb{R}^k\)：</p><p>\(a^{(0)} = x\), \(z^{(l)} = W^{(l)} a^{(l-1)} + b^{(l)}\), \(a^{(l)} = \sigma(z^{(l)})\), 且 \(f_\theta(x)=a^{(L)}\)。</p><h3 id="Backprop"><a class="headerlink" href="#Backprop" title="反向传播"></a>反向传播</h3><p>给定损失 \(\mathcal{L}(f_\theta(x), y)\)，反向传播用链式法则高效计算梯度。</p><ul>
<li>输出层误差：\(\delta^{(L)} = \nabla_{a^{(L)}} \mathcal{L} \odot \sigma'(z^{(L)})\)。</li>
<li>隐藏层误差：\(\delta^{(l)} = (W^{(l+1)})^T \delta^{(l+1)} \odot \sigma'(z^{(l)})\)。</li>
<li>梯度：\(\nabla_{W^{(l)}} \mathcal{L} = \delta^{(l)} (a^{(l-1)})^T\)，\(\nabla_{b^{(l)}} \mathcal{L} = \delta^{(l)}\)。</li>
</ul><h2 id="CNN"><a class="headerlink" href="#CNN" title="卷积神经网络"></a>2. 卷积神经网络（CNN）</h2><h3 id="CNN-Intro"><a class="headerlink" href="#CNN-Intro" title="介绍"></a>介绍</h3><p>CNN 针对网格结构数据（图像、时间序列）设计，通过局部连接与参数共享降低复杂度。</p><h3 id="Receptive-Field"><a class="headerlink" href="#Receptive-Field" title="感受野"></a>感受野</h3><p>感受野是影响某个特征图激活的输入区域。设卷积核大小为 \(k\)，步幅为 \(s\)，层数为 \(L\)。感受野随层数扩大：</p><p>\(R_{l+1} = R_l + (k_{l+1} - 1) \cdot \prod_{i=1}^{l} s_i\)。</p><h4 id="RF-Problem"><a class="headerlink" href="#RF-Problem" title="感受野练习"></a>练习题（感受野）</h4><p>三层卷积：第 1 层核 3、步幅 1；第 2 层核 3、步幅 2；第 3 层核 5、步幅 1。第 3 层的感受野是多少？</p><details class="solution-block">
<summary>查看答案</summary>
<p><strong>解答：</strong>用公式 \(R_{l+1} = R_l + (k_{l+1}-1)\prod_{i=1}^{l} s_i\)，且 \(R_1=3\)。</p>
<p>第 2 层：\(R_2 = 3 + (3-1)\cdot 1 = 5\)。第 3 层：\(R_3 = 5 + (5-1)\cdot (1\cdot 2)=13\)。</p>
<p><strong>感受野 = 13</strong>。</p>
</details><h3 id="Kernel-Size"><a class="headerlink" href="#Kernel-Size" title="卷积核大小与输出形状"></a>卷积核大小与输出形状</h3><p>输入尺寸 \(n\)、卷积核 \(k\)、填充 \(p\)、步幅 \(s\)：</p><p>\(n_{out} = \left\lfloor \frac{n + 2p - k}{s} \right\rfloor + 1\)。</p><p>若想保持尺寸不变（same padding），当 \(k\) 为奇数时取 \(p = \frac{k-1}{2}\)。</p><h4 id="KS-Problem"><a class="headerlink" href="#KS-Problem" title="输出尺寸练习"></a>练习题（核大小与输出尺寸）</h4><p>输入为 \(64	imes64\)。卷积核 \(k=7\)，填充 \(p=3\)，步幅 \(s=2\)。输出尺寸是多少？</p><details class="solution-block">
<summary>查看答案</summary>
<p>\(n_{out} = \left\lfloor rac{n+2p-k}{s} 
ight
floor + 1 = \left\lfloor rac{64+6-7}{2} 
ight
floor + 1 = 32\)。</p>
<p><strong>输出尺寸 = 32 × 32</strong>。</p>
</details><h3 id="Num-Kernels"><a class="headerlink" href="#Num-Kernels" title="每层卷积核数量"></a>每层卷积核数量</h3><p>若输入通道数 \(C_{in}\)，输出卷积核数 \(C_{out}\)：</p><ul>
<li>每个核形状为 \(k \times k \times C_{in}\)。</li>
<li>输出特征图数量为 \(C_{out}\)。</li>
<li>参数量：\(C_{out} \cdot (k^2 C_{in} + 1)\)（含偏置）。</li>
</ul><h4 id="NK-Problem"><a class="headerlink" href="#NK-Problem" title="核数量练习"></a>练习题（核数量与参数）</h4><p>输入通道 \(C_{in}=64\)，输出卷积核 \(C_{out}=128\)，核大小 \(3	imes3\)。包含偏置的参数总数是多少？</p><details class="solution-block">
<summary>查看答案</summary>
<p>每个核参数：\(3\cdot3\cdot64 + 1 = 577\)。总参数：\(128\cdot577 = 73856\)。</p>
<p><strong>参数总数 = 73,856</strong>。</p>
</details><h3 id="CNN-Pros-Cons"><a class="headerlink" href="#CNN-Pros-Cons" title="CNN 的优缺点"></a>CNN 的优缺点</h3><ul>
<li><strong>优点：</strong>参数共享、空间归纳偏置、视觉任务表现强。</li>
<li><strong>缺点：</strong>全局依赖建模弱、对大规模标注数据需求高、旋转等变性有限。</li>
</ul><h3 id="Cross-Correlation"><a class="headerlink" href="#Cross-Correlation" title="互相关"></a>CNN 中的互相关</h3><p>多数 CNN 框架实现的是<strong>互相关</strong>而非严格的卷积。对于输入 \(x\) 与卷积核 \(k\)：</p><p>\[(x \star k)[i,j] = \sum_{u=0}^{m-1} \sum_{v=0}^{n-1} x[i+u, j+v] \, k[u,v]\]</p><p>卷积会翻转核：\((x * k)[i,j] = \sum_{u,v} x[i+u, j+v] \, k[m-1-u, n-1-v]\)。由于核权重可学习，互相关与卷积在效果上等价。</p><h4 id="Cross-Correlation-Problem"><a class="headerlink" href="#Cross-Correlation-Problem" title="练习题"></a>练习题</h4><p>给定输入 \(x\) 与核 \(k\)：</p><p>\(x=\begin{bmatrix}1 &amp; 2 &amp; 0 \\ 0 &amp; 1 &amp; 3 \\ 2 &amp; 1 &amp; 1\end{bmatrix}\)，\(k=\begin{bmatrix}1 &amp; 0 \\ -1 &amp; 2\end{bmatrix}\)。</p><p>在无填充、步幅为 1 的情况下，计算互相关输出左上角 \((x \star k)[0,0]\)。</p><details class="solution-block">
<summary>查看答案</summary>
<p><strong>解答：</strong> \((x \star k)[0,0] = 1\cdot1 + 2\cdot0 + 0\cdot(-1) + 1\cdot2 = 3\)。</p>
</details><figure>
<img alt="CNN 卷积示意" src="/images/cnn-kernel-example.png"/>
<figcaption>卷积核跨通道滑动计算并生成特征图的示例。</figcaption>
</figure>
<details class="solution-block"><summary>Conv1 与 Conv2 滤波器：问答</summary><h3>问题</h3><p><strong>在 CNN 中，Conv2 的滤波器矩阵与 Conv1 的滤波器矩阵有什么区别？请从结构与概念两个层面解释。</strong></p><hr/><h3>答案</h3><p>Conv1 与 Conv2 的滤波器矩阵在<strong>结构</strong>与<strong>功能</strong>上均不同，反映了 CNN 的层级特征学习。</p><h4>1. 结构差异（滤波器维度）</h4><h5>Conv1（第一层卷积）</h5><ul>
<li><strong>输入通道：</strong>1（灰度图）</li>
<li><strong>卷积核尺寸：</strong>\(4 \times 4\)</li>
<li><strong>滤波器数量：</strong>16</li>
</ul><p>每个 Conv1 滤波器形状为：</p><p>\[
\boxed{4 \times 4 \times 1}
\]</p><p>Conv1 总参数量：</p><p>\[
4 \times 4 \times 1 \times 16 = 256
\]</p><h5>Conv2（第二层卷积）</h5><ul>
<li><strong>输入通道：</strong>16（来自 Conv1 的特征图）</li>
<li><strong>卷积核尺寸：</strong>\(4 \times 4\)</li>
<li><strong>滤波器数量：</strong>16</li>
</ul><p>每个 Conv2 滤波器形状为：</p><p>\[
\boxed{4 \times 4 \times 16}
\]</p><p>Conv2 总参数量：</p><p>\[
4 \times 4 \times 16 \times 16 = 4096
\]</p><h4>2. 数学表达差异</h4><h5>Conv1 的计算</h5><p>在位置 \((i,j)\)：</p><p>\[
y_k(i,j) = \sum_{u,v} W_k(u,v)\,x(i+u,j+v)
\]</p><p>无通道求和，直接在像素层计算。</p><h5>Conv2 的计算</h5><p>在位置 \((i,j)\)：</p><p>\[
y_k(i,j) = \sum_{c=1}^{16}\sum_{u,v} W_{k,c}(u,v)\,x_c(i+u,j+v)
\]</p><p>对所有输入通道求和，融合多张特征图的信息。</p><h4>3. 概念差异（特征层级）</h4><p><strong>Conv1 学到：</strong>低层特征（边缘、角点、笔画、局部对比）。</p><p><strong>Conv2 学到：</strong>高层特征（边缘组合、曲线、结构部件等）。</p><h4>4. 直觉理解</h4><ul>
<li>Conv1 回答：“哪里有基本视觉模式？”</li>
<li>Conv2 回答：“这些模式如何组合成有意义的形状？”</li>
</ul><h4>5. 关键总结（考试版）</h4><blockquote>
<p>Conv1 滤波器大小为 \(4\times4\times1\)，直接提取像素级低层特征；Conv2 滤波器大小为 \(4\times4\times16\)，跨越全部特征图通道，学习更高层次的特征组合。</p>
</blockquote><h4>6. 记忆公式</h4><p>\[
\boxed{\text{卷积核尺寸} = (\text{高}) \times (\text{宽}) \times (\text{输入通道数})}
\]</p><p>每个滤波器产生<strong>一个</strong>输出通道，越深层语义越抽象。</p></details><h2 id="Hilbert"><a class="headerlink" href="#Hilbert" title="Hilbert 空间"></a>3. Hilbert 空间</h2><h3 id="Hilbert-Def"><a class="headerlink" href="#Hilbert-Def" title="定义"></a>定义</h3><p>Hilbert 空间是完备的内积空间 \((\mathcal{H}, \langle \cdot, \cdot \rangle)\)，其范数 \(|x|=\sqrt{\langle x,x\rangle}\) 使所有 Cauchy 序列收敛。</p><h3 id="Hilbert-Examples"><a class="headerlink" href="#Hilbert-Examples" title="例子"></a>例子</h3><ul>
<li>带标准内积的 \(\mathbb{R}^n\)。</li>
<li>\(L^2([a,b])\)：平方可积函数空间。</li>
<li>序列空间 \(\ell^2\)：\(\sum_i x_i^2 &lt; \infty\)。</li>
</ul><details class="solution-block">
<summary>\(L^{2}\) 完备性证明</summary>
<h1>\(L^{2}\) 的完备性：\(L^{2}(\Omega,\Sigma,\mu)\) 是 Hilbert 空间</h1>
<p>令 \((\Omega,\Sigma,\mu)\) 为测度空间，定义</p>
<p>\[
L^{2}(\Omega) := \left\{ f:\Omega\to\mathbb{C}\text{ 可测 }:\ \int_\Omega |f|^{2}\,d\mu &lt; \infty \right\}\Big/\sim,
\]</p>
<p>其中 \(f\sim g\) 当且仅当 \(f=g\) 于 \(\mu\)-几乎处处成立。</p>
<p>在 \(L^{2}\) 上定义内积</p>
<p>\[
\langle f,g\rangle := \int_\Omega f\overline{g}\,d\mu,
\]</p>
<p>以及范数 \(\|f\|_{2} := \big(\int_\Omega |f|^{2}\,d\mu\big)^{1/2}\)。</p>
<p>我们证明：<strong>\((L^{2},\|\cdot\|_{2})\) 中的每个柯西序列都在 \(\|\cdot\|_{2}\) 下收敛</strong>。因此 \(L^{2}\) 作为赋范空间是完备的；配合内积即为 <strong>Hilbert 空间</strong>。</p>
<hr/>
<h2>定理</h2>
<p>\((L^{2}(\Omega),\langle\cdot,\cdot\rangle)\) 是 Hilbert 空间。</p>
<h3>证明（\(L^{2}\) 的完备性）</h3>
<p>设 \((f_n)\) 为 \(L^{2}(\Omega)\) 中的柯西序列。对任意 \(\varepsilon&gt;0\)，存在 \(N\) 使得对所有 \(m,n\ge N\)，</p>
<p>\[
\|f_m-f_n\|_{2} &lt; \varepsilon.
\]</p>
<h3>步骤 1：抽取“快速柯西”子序列</h3>
<p>由于 \((f_n)\) 是柯西序列，可取子序列 \((f_{n_k})_{k\ge 1}\)，满足</p>
<p>\[
\|f_{n_{k+1}} - f_{n_k}\|_{2} \le 2^{-k}\quad (k\ge 1).
\]</p>
<p>令增量</p>
<p>\[
h_k := f_{n_{k+1}} - f_{n_k} \in L^{2}.
\]</p>
<p>则</p>
<p>\[
\|h_k\|_{2} \le 2^{-k}
\quad\Longrightarrow\quad
\sum_{k=1}^{\infty} 2^{k}\|h_k\|_2^2 \le \sum_{k=1}^{\infty} 2^{-k} &lt; \infty.
\]</p>
<h3>步骤 2：子序列几乎处处收敛</h3>
<p>定义非负可测函数</p>
<p>\[
H(x):=\sum_{k=1}^{\infty} 2^{k}\,|h_k(x)|^{2}.
\]</p>
<p>由 Tonelli 定理，</p>
<p>\[
\int_\Omega H\,d\mu = \sum_{k=1}^{\infty} 2^{k}\|h_k\|_2^2 &lt; \infty.
\]</p>
<p>因此 \(H(x)&lt;\infty\) 于 \(\mu\)-几乎处处成立。</p>
<p>固定这样的 \(x\)。对整数 \(m&lt;\ell\)，由 Cauchy–Schwarz：</p>
<p>\[
\sum_{k=m}^{\ell} |h_k(x)|
\le \left(\sum_{k=m}^{\ell} 2^{-k}\right)^{1/2}
     \left(\sum_{k=m}^{\ell} 2^{k}|h_k(x)|^{2}\right)^{1/2}.
\]</p>
<p>因 \(\sum_{k=m}^{\ell} 2^{-k}\le 2^{-m+1}\) 且 \(\sum_{k=m}^{\ell} 2^{k}|h_k(x)|^{2}\le H(x)&lt;\infty\)，故</p>
<p>\[
\sum_{k=m}^{\ell} |h_k(x)| \to 0 \quad (m\to\infty).
\]</p>
<p>于是 \(\sum_{k=1}^{\infty} h_k(x)\) 绝对收敛，令</p>
<p>\[
S_K(x):=\sum_{k=1}^{K} h_k(x)
\]</p>
<p>则 \(S_K(x)\) 在 \(\mu\)-几乎处处收敛。</p>
<p>定义</p>
<p>\[
f(x) := f_{n_1}(x) + \sum_{k=1}^{\infty} h_k(x)
\quad\text{(a.e.)}.
\]</p>
<p>则 \(f_{n_k}(x)\to f(x)\) 于 \(\mu\)-几乎处处成立。</p>
<h3>步骤 3：在 \(L^{2}\) 中收敛</h3>
<p>对任意 \(m\ge 1\)，令</p>
<p>\[
T_{m,r}(x):=\sum_{k=m}^{r} h_k(x) = f_{n_{r+1}}(x)-f_{n_m}(x).
\]</p>
<p>点态上 \(T_{m,r}(x)\to f(x)-f_{n_m}(x)\)。由 Fatou 引理：</p>
<p>\[
\|f-f_{n_m}\|_2^2 \le \liminf_{r\to\infty} \|T_{m,r}\|_2^2.
\]</p>
<p>而由三角不等式，</p>
<p>\[
\|T_{m,r}\|_2 \le \sum_{k=m}^{\infty} 2^{-k} = 2^{-m+1}.
\]</p>
<p>于是 \(\|f-f_{n_m}\|_2 \le 2^{-m+1}\to 0\)。</p>
<p>因此 \(f_{n_m}\to f\) 于 \(L^2\)，并且 \(f\in L^2\)。</p>
<h3>步骤 4：推广到原序列</h3>
<p>由于 \((f_n)\) 是柯西序列，对任意 \(\varepsilon&gt;0\)，取 \(N\) 使得 \(n\ge N\) 时 \(\|f_n-f_N\|_2 &lt; \varepsilon/2\)。再取 \(m\) 使 \(n_m\ge N\) 且 \(\|f_{n_m}-f\|_2&lt;\varepsilon/2\)。则</p>
<p>\[
\|f_n-f\|_2 \le \|f_n-f_{n_m}\|_2 + \|f_{n_m}-f\|_2 &lt; \varepsilon.
\]</p>
<p>因此 \(f_n\to f\) 于 \(L^2\)，从而证明 \(L^2\) 完备。∎</p>
<hr/>
<h2>要点（为什么成立）</h2>
<ul>
<li>选择 \(\|f_{n_{k+1}}-f_{n_k}\|_2\le 2^{-k}\) 保证“可求和控制”。</li>
<li>加权级数 \(\sum 2^k |h_k|^2\) 在积分意义下有限，几乎处处有限。</li>
<li>加权 Cauchy–Schwarz 推出逐点收敛。</li>
<li>Fatou + 三角不等式得到 \(L^2\) 收敛。</li>
</ul>
<p>如需第二种证明，可用：<br/>
1) <strong>Riesz–Fischer 定理</strong>（正交展开），或<br/>
2) <strong>一致可积性 / Banach 空间完备性方法</strong>。</p>
</details><h2 id="PD"><a class="headerlink" href="#PD" title="正定函数与核"></a>4. 正定函数与核</h2><p>核函数是一种相似度函数，它等价于某个（可能是无限维）特征空间中的内积，并且它属于 Hilbert 空间。</p><h3 id="PD-Def"><a class="headerlink" href="#PD-Def" title="定义"></a>定义</h3><p>函数 \(K: X \times X \to \mathbb{R}\) 若对任意 \(x_1,\ldots,x_n\) 与 \(a \in \mathbb{R}^n\) 满足：</p><p>\(\sum_{i=1}^n \sum_{j=1}^n a_i a_j K(x_i, x_j) \ge 0\)，则称 \(K\) 为正定。</p><p><strong>例：</strong> 对于 RBF 核 \(K(x,x')=\exp(-|x-x'|^2/(2\sigma^2))\)，任意样本点形成的 Gram 矩阵都是对称且特征值非负，因此是正定核。</p><h3 id="PD-Test"><a class="headerlink" href="#PD-Test" title="如何判断正定"></a>如何判断正定</h3><ul>
<li>构造 Gram 矩阵 \(K_{ij}=K(x_i,x_j)\)，检查对称性与特征值非负。</li>
<li>证明存在特征映射 \(\phi\) 使 \(K(x,x')=\langle \phi(x), \phi(x')\rangle\)。</li>
<li>利用闭包性质：加、乘、缩放仍为正定核。</li>
</ul><h4 id="PD-Test-Example"><a class="headerlink" href="#PD-Test-Example" title="示例"></a>示例</h4><p>设 \(K(x,x') = \exp(-|x-x'|^2)\)（RBF 核）。对两个不同点，距离为 \(d&gt;0\)，Gram 矩阵为</p><p>\(K = \begin{bmatrix}1 &amp; e^{-d^2} \\ e^{-d^2} &amp; 1\end{bmatrix}\)。</p><details class="solution-block">
<summary>查看检验</summary>
<p>特征值为 \(1\pm e^{-d^2}\)。因为 \(0&lt;e^{-d^2}&lt;1\)，两特征值均为正，因此 \(K\) 为正定。</p>
</details>
<h2 id="Kernel-Methods"><a class="headerlink" href="#Kernel-Methods" title="核方法"></a>5. 核方法</h2><h3 id="Kernel-Def"><a class="headerlink" href="#Kernel-Def" title="什么是核"></a>什么是核？</h3><h3 id="Kernel-Trick"><a class="headerlink" href="#Kernel-Trick" title="核技巧"></a>核技巧</h3><p>将算法中的内积替换为 \(K(x,x')\)，避免显式映射到高维。</p><details class="solution-block"><summary>K(x,·) 的含义与再生性质</summary><p>RKHS 是唯一使“函数在点处取值”等价于“与内积相乘”的数学环境。</p><h3>\(K(x,\cdot)\) 的含义与再生性质（含具体数值例子）</h3><hr/><h4>1. \(K(x,\cdot)\) 中的“\(\cdot\)”是什么意思？</h4><h5>定义</h5><p>“\(\cdot\)”是<strong>函数自变量的占位符</strong>。</p><p>给定核函数</p><p>\[
K : X \times X \to \mathbb{R},
\]</p><p>表达式</p><p>\[
K(x,\cdot)
\]</p><p>表示：把第一个参数固定为 \(x\)，把核函数视为<strong>关于第二个参数的函数</strong>。</p><p>形式化地，</p><p>\[
K(x,\cdot) : z \longmapsto K(x,z).
\]</p><p>因此 \(K(x,\cdot)\) 不是一个数，而是一个<strong>函数</strong>。</p><hr/><h4>2. 为什么需要 \(K(x,\cdot)\)?</h4><p>在 RKHS 理论中：</p><ul>
<li>Hilbert 空间 \(\mathcal H\) 的元素是<strong>函数</strong>。</li>
<li>对每个 \(x \in X\)，函数 \(K(x,\cdot)\) 本身属于 \(\mathcal H\)。</li>
</ul><p>即</p><p>\[
K(x,\cdot) \in \mathcal H.
\]</p><p>这为再生性质提供基础。</p><hr/><h4>3. 再生性质（表述）</h4><p>设 \(\mathcal H\) 为核 \(K\) 对应的 RKHS。对任意 \(f \in \mathcal H\) 与 \(x \in X\)，</p><p>\[
\boxed{
 f(x) = \langle f,\; K(x,\cdot) \rangle_{\mathcal H}
}
\]</p><p>这意味着：函数在点 \(x\) 的取值等于与核函数 \(K(x,\cdot)\) 的内积。</p><hr/><h4>4. 直觉理解（数字之前）</h4><ul>
<li>在有限维空间中，内积相当于提取坐标。</li>
<li>在 RKHS 中，内积提取的是<strong>函数值</strong>。</li>
</ul><p>因此 \(K(x,\cdot)\) 可以理解为“抽取 \(f(x)\) 的广义基向量”。</p><hr/><h4>5. 符号级例子</h4><h5>线性核</h5><p>令</p><p>\[
K(x,z) = xz \quad (\mathbb{R}).
\]</p><p>固定 \(x = 3\)，则</p><p>\[
K(3,\cdot)(z) = 3z.
\]</p><p>所以 \(K(3,\cdot)\) 是函数 \(z \mapsto 3z\)。</p><hr/><h4>6. 具体数值例子验证再生性质</h4><h5>步骤 1：选择核函数</h5><p>使用<strong>线性核</strong>：</p><p>\[
K(x,z) = xz.
\]</p><p>对应的 RKHS 是所有线性函数：</p><p>\[
f(z) = az
\]</p><p>其内积为：</p><p>\[
\langle f,g\rangle = ab \quad (f(z)=az,\ g(z)=bz).
\]</p><h5>步骤 2：选择函数 \(f\)</h5><p>令</p><p>\[
f(z) = 2z.
\]</p><h5>步骤 3：选择点 \(x\)</h5><p>令</p><p>\[
x = 3.
\]</p><p>则</p><p>\[
f(3) = 2 \times 3 = 6.
\]</p><h5>步骤 4：计算 \(K(x,\cdot)\)</h5><p>\[
K(3,\cdot)(z) = 3z.
\]</p><h5>步骤 5：计算内积</h5><p>\[
\langle f,\; K(3,\cdot) \rangle
= \langle 2z,\; 3z \rangle
= 2 \times 3
= 6.
\]</p><h5>步骤 6：对比两边</h5><p>\[
\boxed{
 f(3) = 6
 \quad=\quad
 \langle f,\; K(3,\cdot) \rangle
}
\]</p><p>✔ 再生性质严格成立。</p><hr/><h4>7. 直觉：高斯核（概念版）</h4><p>对高斯核</p><p>\[
K(x,z) = \exp\!\left(-\frac{(x-z)^2}{2\sigma^2}\right),
\]</p><ul>
<li>\(K(x,\cdot)\) 是以 \(x\) 为中心的高斯函数。</li>
<li>与 \(f\) 的内积衡量对齐程度。</li>
<li>该对齐值等于 \(f(x)\)。</li>
</ul><hr/><h4>8. 常见误解（重要）</h4><ul>
<li>❌ “\(\cdot\)”不是乘法。</li>
<li>❌ “\(\cdot\)”不是内积。</li>
<li>❌ “\(\cdot\)”不是导数。</li>
<li>✅ 它表示“函数自变量”。</li>
</ul><hr/><h4>9. 一句话总结（考试版）</h4><blockquote>
<p>在 RKHS 中，\(K(x,\cdot)\) 表示固定一个核参数得到的函数，而再生性质说明函数在 \(x\) 处取值等于其与 \(K(x,\cdot)\) 的内积。</p>
</blockquote><hr/><h4>10. 大图景总结</h4><blockquote>
<p><strong>核函数不仅比较样本点，更生成函数作为“坐标探针”，在无限维 Hilbert 空间中提取函数值。</strong></p>
</blockquote></details>
<h3 id="Representer-Theorem"><a class="headerlink" href="#Representer-Theorem" title="表示定理"></a>表示定理</h3><p>正则化风险最小化的解可写为：</p><p>\(f(x) = \sum_{i=1}^n \alpha_i K(x_i, x)\)。</p><h4 id="Representer-Proof"><a class="headerlink" href="#Representer-Proof" title="证明思路"></a>证明思路</h4><p>任意 \(f\in\mathcal{H}\) 可写为 \(f=f_\parallel+f_\perp\)，其中 \(f_\parallel\) 属于 \(\{K(x_i,\cdot)\}\) 的张成空间，\(f_\perp\) 与其正交。拟合项只依赖 \(f_\parallel\)，且 \(\|f\|^2=\|f_\parallel\|^2+\|f_\perp\|^2\)。因此最优解满足 \(f_\perp=0\)。</p><h4 id="Representer-Problem"><a class="headerlink" href="#Representer-Problem" title="练习题"></a>练习题</h4><p>在 RBF 核的核岭回归中，写出解的形式，并说明需求解的方程。</p><details class="solution-block">
<summary>查看答案</summary>
<p>由表示定理，\(f(x)=\sum_{i=1}^n \alpha_i K(x_i,x)\)。系数 \(\alpha\) 由线性方程 \((K+\lambda n I)\alpha=y\) 求得，\(K\) 为 Gram 矩阵。</p>
</details><h3 id="Kernel-Apps"><a class="headerlink" href="#Kernel-Apps" title="应用与示例"></a>应用与示例</h3><ul>
<li>SVM（分类）。</li>
<li>核岭回归（回归）。</li>
<li>RBF 核：\(K(x,x')=\exp(-|x-x'|^2/(2\sigma^2))\)。</li>
<li>多项式核：\(K(x,x')=(x^T x' + c)^d\)。</li>
</ul><h2 id="Summary"><a class="headerlink" href="#Summary" title="总结"></a>总结</h2><ul>
<li>反向传播高效计算深度网络梯度。</li>
<li>CNN 利用局部性与参数共享处理图像等数据。</li>
<li>Hilbert 空间为核方法提供理论基础。</li>
<li>正定核使核技巧与表示定理成立。</li>
<li>核方法几乎可以应用于所有监督学习任务。</li><li>然而，如何选择核函数仍未被充分理解。</li><li>正则化参数的选择方式也尚未有公认的结论。</li></ul><div class="post-categoris-bottom">
<div class="post-categoris-name">数学</div>
<ul>
<li class="me base">
<a class="post-categoris-bottom-link" href="/2026/02/05/Numeric-Methods/index-zh.html">1. 卷积与核算子</a>
</li>
</ul>
</div>
<!-- top型目录 -->
<!-- 分类文章 -->
<details class="solution-block">
<summary>SVM、Kernel 与 Hilbert 空间 — 数值例子</summary>
<h2>SVM、Kernel 与 Hilbert 空间 — 完整数值例子</h2>
<p>本例用<strong>具体数字</strong>逐步说明 SVM 如何使用 kernel 与 Hilbert 空间。</p>
<hr/>
<h3>1. 一个具体数据集（线性不可分）</h3>
<p>考虑一个简单的 <strong>1D 分类问题</strong>：</p>
<table>
<thead>
<tr><th>序号</th><th>输入 \(x\)</th><th>标签 \(y\)</th></tr>
</thead>
<tbody>
<tr><td>1</td><td>−1</td><td>−1</td></tr>
<tr><td>2</td><td>0</td><td>+1</td></tr>
<tr><td>3</td><td>1</td><td>−1</td></tr>
</tbody>
</table>
<p>数轴直觉：</p>
<pre>x^2
↑
| ● (-1,1) ● (1,1) ← 负类
|
| ○ (0,0) ← 正类
+------------------------→ x</pre>
<p>这些点在 2D 空间中<strong>线性可分</strong>。</p>
<hr/>
<h3>4. 特征（Hilbert）空间中的线性 SVM</h3>
<p>在特征空间，SVM 仍然是线性的：</p>
<p>\[
f(x) = \langle w, \phi(x) \rangle + b
\]</p>
<p>取：</p>
<p>\[
w = (0,1), \quad b = -0.5
\]</p>
<p>逐点计算：</p>
<ul>
<li>\(x=-1\)：\(f = (0,1)\cdot(-1,1) - 0.5 = 0.5 \Rightarrow y=-1\)</li>
<li>\(x=0\)：\(f = (0,1)\cdot(0,0) - 0.5 = -0.5 \Rightarrow y=+1\)</li>
<li>\(x=1\)：\(f = (0,1)\cdot(1,1) - 0.5 = 0.5 \Rightarrow y=-1\)</li>
</ul>
<p>分类完全正确。</p>
<hr/>
<h3>5. 特征空间内积（显式数值）</h3>
<p>计算内积：</p>
<p>\[
\langle \phi(x), \phi(x') \rangle
=
 x x' + x^2 x'^2
\]</p>
<p>例子：</p>
<ul>
<li>\(x=-1,\ x'=1\)：\((-1)(1) + (1)(1) = 0\)</li>
<li>\(x=1,\ x'=1\)：\(1 + 1 = 2\)</li>
</ul>
<p>SVM 只依赖这些<strong>内积</strong>。</p>
<hr/>
<h3>6. 用核函数替代内积</h3>
<p>定义核：</p>
<p>\[
K(x,x') = x x' + x^2 x'^2
\]</p>
<p>满足：</p>
<p>\[
K(x,x') = \langle \phi(x), \phi(x') \rangle
\]</p>
<p>计算 <strong>Gram 矩阵</strong>：</p>
<p>\[
K =
\begin{pmatrix}
K(-1,-1) &amp; K(-1,0) &amp; K(-1,1) \\
K(0,-1) &amp; K(0,0) &amp; K(0,1) \\
K(1,-1) &amp; K(1,0) &amp; K(1,1)
\end{pmatrix}
=
\begin{pmatrix}
2 &amp; 0 &amp; 0 \\
0 &amp; 0 &amp; 0 \\
0 &amp; 0 &amp; 2
\end{pmatrix}
\]</p>
<p>该矩阵是<strong>半正定</strong>的，因此核定义了一个<strong>Hilbert 空间</strong>。</p>
<hr/>
<h3>7. 只用核写出 SVM 解</h3>
<p>由表示定理，分类器形式为：</p>
<p>\[
f(x) = \sum_{i=1}^n \alpha_i y_i K(x_i,x) + b
\]</p>
<p>假设系数为：</p>
<p>\[
\alpha_1 = \alpha_3 = 0.5,\quad \alpha_2 = 1
\]</p>
<p>则：</p>
<p>\[
f(x)
=
0.5(-1)K(-1,x)
+ 1(+1)K(0,x)
+ 0.5(-1)K(1,x)
+ b
\]</p>
<p>无需显式 \(\phi(x)\) —— 只需核函数值。</p>
<hr/>
<h3>8. Hilbert 空间出现在哪里</h3>
<p>核诱导<strong>再生核 Hilbert 空间（RKHS）</strong> \(\mathcal H_K\)：</p>
<ul>
<li>函数形式：\(f(x) = \sum_i c_i K(x_i,x)\)</li>
<li>内积性质：\(\langle K(x,\cdot), K(x',\cdot) \rangle_{\mathcal H_K} = K(x,x')\)</li>
<li>再生性质：\(f(x) = \langle f, K(x,\cdot) \rangle_{\mathcal H_K}\)</li>
</ul>
<p>因此：</p>
<ul>
<li>\(K(x,\cdot)\) 像特征向量</li>
<li>SVM 在该 Hilbert 空间中寻找<strong>最大间隔超平面</strong></li>
</ul>
<hr/>
<h3>9. 最终结论</h3>
<blockquote>
<p>SVM 在核定义的 Hilbert 空间中始终是线性的，通过用核值替换特征空间内积，它只依赖 Gram 矩阵即可学习非线性决策边界。</p>
</blockquote>
<p>该例子用<strong>具体数字</strong>展示了 kernel、Hilbert 空间与 SVM 的关系。</p>
</details></div><div class="post-content-inner-space">
<div class="space-toc-main animate__animated animate__fadeInUp">
<ol class="space-toc"><li class="space-toc-item space-toc-level-2"><a class="space-toc-link" href="#Reading-List"><span class="space-toc-text">Reading List</span></a></li><li class="space-toc-item space-toc-level-2"><a class="space-toc-link" href="#Convolution-Cross-Correlation"><span class="space-toc-text">理解卷积与互相关</span></a></li><li class="space-toc-item space-toc-level-2"><a class="space-toc-link" href="#Neural-Networks"><span class="space-toc-text">1. 神经网络</span></a></li><li class="space-toc-item space-toc-level-2"><a class="space-toc-link" href="#CNN"><span class="space-toc-text">2. 卷积神经网络（CNN）</span></a></li><li class="space-toc-item space-toc-level-2"><a class="space-toc-link" href="#Hilbert"><span class="space-toc-text">3. Hilbert 空间</span></a></li><li class="space-toc-item space-toc-level-2"><a class="space-toc-link" href="#PD"><span class="space-toc-text">4. 正定函数与核</span></a></li><li class="space-toc-item space-toc-level-2"><a class="space-toc-link" href="#Kernel-Methods"><span class="space-toc-text">5. 核方法</span></a></li><li class="space-toc-item space-toc-level-2"><a class="space-toc-link" href="#Summary"><span class="space-toc-text">总结</span></a></li></ol>
</div>
</div>
</div>
<!-- 评论 -->
</div>
</article>
</div>
</div>
<!-- 如果是home模式的话，不在首页就显示footer，如果不是home模式的话 所有都显示footer -->
<div class="footer-outer animate__animated animate__fadeInUp">
<div class="footer-inner">
<div class="footer-text">
<p>人工智能  | 金融工程 | 数学 | 计算机科学  <strong>袁榕言 <i class="ri-copyright-line"></i> 2026</strong></p>
</div>
<div class="footer-contact">
<ul class="footer-ul">
<li class="footer-li">
<a href="https://github.com/RongyanYuan" target="_blank">
<i class="ri-github-line"></i>
</a>
</li>
<li class="footer-li">
<a href="mailto:adrianrongyanyun@gmail.com" target="_blank">
<i class="ri-mail-line"></i>
</a>
</li>
<li class="footer-li">
<a href="https://www.linkedin.com/in/rongyan-yuan-a076971b2/" target="_blank">
<i class="ri-linkedin-box-line"></i>
</a>
</li>
</ul>
</div>
</div>
</div>
<script src="/js/white.js"></script>
<script>
  window.MathJax = {
    tex: {inlineMath: [['\\(','\\)'], ['$', '$']]},
    svg: {fontCache: 'global'}
  };
</script>
<script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-svg.js"></script>
</body>
</html>
