<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>  01. Decision Tree and Random Forest |    Rongyan's Blog</title>
  <meta name="description" content="Financial Engineering | Mathematics | Computer Science">
  <link href="https://cdn.jsdelivr.net/npm/remixicon@2.2.0/fonts/remixicon.css" rel="stylesheet">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/fushaolei/cdn-white@1.0/css/animate.css"/>
  <link rel="stylesheet" href="/css/white.css">
</head>
<body>

<div class="menu-outer">
  <div class="menu-inner">
    <div class="menu-site-name  animate__animated  animate__fadeInUp">
      <a href="/">Rongyan's Blog</a>
      <div class="menu-redirect"><a class="nav-link back-topic-link" href="/categories/ai/data-mining-and-text-analytics/">‚Üê Data Mining and Text Analytics</a></div>
    </div>
    <div class="menu-group">
      <ul class="menu-ul">
        <a href="/" class="nav-link"><li class="menu-li  animate__animated  animate__fadeInUp">HOME</li></a>
        <a href="/archives" class="nav-link"><li class="menu-li  animate__animated  animate__fadeInUp">BLOG</li></a>
        <li class="menu-li animate__animated  animate__fadeInUp" id="sort">
           CATEGORIES
           <div class="categories-outer " id="sort-div">
             <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/AboutMe/">AboutMe</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/ai/">AI</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs/">CS</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/math/">Math</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/quant/">Quant</a></li></ul>
           </div>
        </li>
        <a href="/search"><li class="menu-li  animate__animated  animate__fadeInUp"><i class="ri-search-line"></i></li></a>
        <li class="menu-li animate__animated  animate__fadeInUp lang-switcher-desktop">
          <a href="/2026/02/10/Decision-Tree-and-Random-Forest/index.html">EN</a> |
          <a href="/2026/02/10/Decision-Tree-and-Random-Forest/index-zh.html">‰∏≠Êñá</a>
        </li>
        <li class="menu-li animate__animated  animate__fadeInUp" id="mobile-menu"><i class="ri-menu-line"></i></li>
      </ul>
    </div>
  </div>
</div>

<div id="mobile-main" class="animate__animated  animate__fadeIn">
  <div class="mobile-menu-inner">
    <div class="mobile-menu-site-name animate__animated  animate__fadeInUp"><a href="/">Rongyan's Blog</a></div>
    <div class="mobile-menu-group" id="mobile-close"><i class="ri-close-line"></i></div>
  </div>
  <div class="mobile-menu-div">
    <a href="/" class="mobile-nav-link"><div class="mobile-menu-child animate__animated  animate__fadeInUp"><span>HOME</span></div></a>
    <a href="/archives" class="mobile-nav-link"><div class="mobile-menu-child animate__animated  animate__fadeInUp"><span>BLOG</span></div></a>
    <div class="mobile-menu-child animate__animated  animate__fadeInUp lang-switcher-mobile">
      <a href="/2026/02/10/Decision-Tree-and-Random-Forest/index.html">EN</a> |
      <a href="/2026/02/10/Decision-Tree-and-Random-Forest/index-zh.html">‰∏≠Êñá</a>
    </div>
    <a href="/search"><div class="mobile-menu-child  animate__animated  animate__fadeInUp"><i class="ri-search-line"></i></div></a>
  </div>
</div>

<div class="body-outer">
  <div class="body-inner">
    <article class="post-inner">
      <div class="post-content-outer">
        <div class="post-intro">
          <div class="post-title animate__animated  animate__fadeInUp">01. Decision Tree and Random Forest</div>
          <div class="meta-intro animate__animated  animate__fadeInUp">Feb 10 2026</div>
        </div>
        <div class="post-content-inner">
          <div class="post-content-inner-space"></div>
          <div class="post-content-main animate__animated  animate__fadeInUp">
<hr>
<h2 id="1-overview-what-is-a-decision-tree-and-what-are-decision-criteria"><a href="#1-overview-what-is-a-decision-tree-and-what-are-decision-criteria" class="headerlink" title="1. Overview: What is a Decision Tree and What Are Decision Criteria?"></a>1. Overview: What is a Decision Tree and What Are Decision Criteria?</h2>
<p>A <strong>Decision Tree</strong> is a supervised learning model used for <strong>classification and regression</strong> that makes predictions by recursively splitting data based on feature values. At each node, the tree selects a <strong>decision criterion</strong> (also called an impurity-based criterion) to determine <strong>which feature and split best separate the data</strong>.</p>
<p>The core idea is simple:</p>
<blockquote><p><strong>Choose the split that makes the resulting child nodes as ‚Äúpure‚Äù as possible.</strong></p></blockquote>
<p>Here, <em>purity</em> means:</p>
<ul>
<li>In <strong>classification</strong>: samples in a node mostly belong to the same class</li>
<li>In <strong>regression</strong>: target values in a node are as close to each other as possible</li>
</ul>
<p>Common decision criteria include:</p>
<ul>
<li><strong>Entropy / Information Gain</strong> (information-theoretic uncertainty reduction)</li>
<li><strong>Gini Impurity</strong> (probability of misclassification)</li>
<li><strong>Variance / Mean Squared Error (MSE)</strong> (used in regression)</li>
</ul>
<hr>
<h2 id="2-decision-criteria-explained-with-examples"><a href="#2-decision-criteria-explained-with-examples" class="headerlink" title="2. Decision Criteria Explained (with Examples)"></a>2. Decision Criteria Explained (with Examples)</h2>
<p><img src="/images/data-mining-text-analytics/intro.png" alt="Entropy illustration" class="post-img"></p>
<h3 id="21-entropy"><a href="#21-entropy" class="headerlink" title="2.1 Entropy"></a>2.1 Entropy</h3>
<p><strong>Meaning:</strong> Entropy measures how uncertain or mixed the class labels are in a node.</p>
<p><img src="/images/data-mining-text-analytics/entropy%203.png" alt="Entropy illustration" class="post-img"></p>
<p><strong>Formula:</strong> \[ H(S) = -\sum_{k=1}^K p_k \log_2 p_k \]</p>
<ul>
<li>\(p_k\): proportion of class \(k\)</li>
<li>\(H = 0\): node is perfectly pure</li>
<li>Maximum when classes are evenly mixed</li>
</ul>
<p><strong>Example:</strong> A node has 5 positive and 5 negative samples: \[ H = - (0.5\log_2 0.5 + 0.5\log_2 0.5) = 1 \] (high uncertainty)</p>
<p><img src="/images/data-mining-text-analytics/entropy%201.png" alt="Entropy illustration" class="post-img"></p>
<p><img src="/images/data-mining-text-analytics/entropy%202.png" alt="Entropy illustration" class="post-img"></p>
<hr>
<h3 id="22-information-gain-ig"><a href="#22-information-gain-ig" class="headerlink" title="2.2 Information Gain (IG)"></a>2.2 Information Gain (IG)</h3>
<p><strong>Meaning:</strong> Information Gain measures <strong>how much entropy decreases after a split</strong>.</p>
<p><strong>Formula:</strong> \[ IG(S, A) = H(S) - \sum_{v} \frac{|S_v|}{|S|} H(S_v) \]</p>
<p><strong>Example:</strong></p>
<ul>
<li>Parent entropy = 1</li>
<li>Split produces:</li>
<li>Left node entropy = 0 (pure)</li>
<li>Right node entropy = 0.5</li>
</ul>
<p>If the split is even: \[ IG = 1 - (0.5 \cdot 0 + 0.5 \cdot 0.5) = 0.75 \]</p>
<p>Higher IG ‚áí better split.</p>
<p>Summary of entropy and info gain: we choose the best information gain (highest number) for subcategories at each layer to decide which subcategory should be the parent of that layer, where information gain is calculated by <strong>entropy(prev layer) - entropy(sub categories)</strong>. Below is an example of wheather to play tennis or not:</p>
<p><img src="/images/data-mining-text-analytics/entropy%200.png" alt="Entropy illustration" class="post-img"></p>
<hr>
<h3 id="23-gini-impurity"><a href="#23-gini-impurity" class="headerlink" title="2.3 Gini Impurity"></a>2.3 Gini Impurity</h3>
<p><strong>Meaning:</strong> Gini impurity measures the probability that a randomly chosen sample is misclassified.</p>
<p><strong>Formula:</strong> \[ G(S) = 1 - \sum_{k=1}^K p_k^2 \]</p>
<p><strong>Example:</strong> A node with 7 positives and 3 negatives: \[ G = 1 - (0.7^2 + 0.3^2) = 0.42 \]</p>
<ul>
<li>\(G = 0\): perfectly pure</li>
<li>Widely used in <strong>CART</strong> due to efficiency</li>
</ul>
<p>Following are slides about Gini impurity approach:</p>
<p><img src="/images/data-mining-text-analytics/gini%201.png" alt="Gini_imp illustration" class="post-img"></p>
<p>Perfect split:</p>
<p><img src="/images/data-mining-text-analytics/gini%202.png" alt="Gini_imp illustration" class="post-img"></p>
<p>Imperfect split</p>
<p><img src="/images/data-mining-text-analytics/gini%203.png" alt="Gini_imp illustration" class="post-img"></p>
<p><img src="/images/data-mining-text-analytics/gini%204.png" alt="Gini_imp illustration" class="post-img"></p>
<hr>
<h4 id="231-comparison-between-gini-and-entropy"><a href="#231-comparison-between-gini-and-entropy" class="headerlink" title="2.3.1 Comparison between gini and entropy:"></a>2.3.1 Comparison between gini and entropy:</h4>
<p>Entropy versus Gini Impurity</p>
<ul>
<li>If the data set is completely homogeneous (one</li>
<p>class) then the impurity is 0, therefore entropy is 0.</p>
<li>It is completely non-homogeneous & impurity is 100%, therefore entropy is 1.</li>
<li>Entropy and Gini Impurity give similar results in practice</li>
<li>They only disagree in about 2% of cases</li>
<li>Entropy might be slower to compute, because of the log</li>
</ul>
<hr>
<h3 id="24-variance-mse-regression-trees"><a href="#24-variance-mse-regression-trees" class="headerlink" title="2.4 Variance / MSE (Regression Trees)"></a>2.4 Variance / MSE (Regression Trees)</h3>
<p><strong>Meaning:</strong> Used when the target is continuous; measures spread of target values.</p>
<p><strong>Formula:</strong> \[ \text{MSE}(S) = \frac{1}{|S|}\sum_{i \in S}(y_i - \bar y_S)^2 \]</p>
<p><strong>Goal:</strong> Choose the split that <strong>minimizes weighted MSE</strong> of child nodes.</p>
<p><strong>Example:</strong> Targets: \([200, 210, 220, 500]\) Splitting separates low and high prices ‚Üí variance drops significantly.</p>
<hr>
<h2 id="3-how-a-decision-tree-is-trained"><a href="#3-how-a-decision-tree-is-trained" class="headerlink" title="3. How a Decision Tree Is Trained"></a>3. How a Decision Tree Is Trained</h2>
<p>Decision tree training is a <strong>greedy, top-down, recursive process</strong>:</p>
<ol>
<li><strong>Start at the root node</strong> with all training samples.</li>
<li><strong>Evaluate all possible splits</strong>:</li>
</ol>
<ul>
<li>For every feature</li>
<li>For every possible threshold or category subset</li>
</ul>
<ol>
<li><strong>Compute impurity reduction</strong> using a chosen criterion.</li>
<li><strong>Select the best split</strong> (maximum impurity reduction).</li>
<li><strong>Partition the data</strong> into child nodes.</li>
<li><strong>Repeat recursively</strong> on each child node.</li>
<li><strong>Stop</strong> when:</li>
</ol>
<ul>
<li>Node is pure</li>
<li>Minimum sample size reached</li>
<li>Maximum depth reached</li>
<li>No significant impurity reduction</li>
</ul>
<ol>
<li><em>(Optional)</em> <strong>Pruning</strong> is applied to reduce overfitting.</li>
</ol>
<p><strong>Key property:</strong> Decision trees optimize <strong>locally</strong>, not globally.</p>
<hr>
<summary>4. Real-World Example: Loan Approval Prediction</summary>
<details class="post-details">
  <summary>4. Real-World Example: Loan Approval Prediction</summary>
<h2 id="4-real-world-example-loan-approval-prediction"><a href="#4-real-world-example-loan-approval-prediction" class="headerlink" title="4. Real-World Example: Loan Approval Prediction"></a>4. Real-World Example: Loan Approval Prediction</h2>
<h3 id="problem"><a href="#problem" class="headerlink" title="Problem"></a>Problem</h3>
<p>A bank wants to decide whether to <strong>approve or reject a loan</strong> based on applicant information.</p>
<h3 id="training-data-simplified"><a href="#training-data-simplified" class="headerlink" title="Training Data (Simplified)"></a>Training Data (Simplified)</h3>
<table>
<thead><tr><th>Income</th><th>Credit Score</th><th>Debt Ratio</th><th>Loan Approved</th></tr></thead>
<tbody>
<tr><td>80k</td><td>720</td><td>0.2</td><td>Yes</td></tr>
<tr><td>45k</td><td>650</td><td>0.4</td><td>No</td></tr>
<tr><td>30k</td><td>600</td><td>0.5</td><td>No</td></tr>
<tr><td>90k</td><td>780</td><td>0.1</td><td>Yes</td></tr>
<tr><td>60k</td><td>690</td><td>0.3</td><td>Yes</td></tr>
</tbody></table>
<hr>
<h3 id="training-process"><a href="#training-process" class="headerlink" title="Training Process"></a>Training Process</h3>
<ol>
<li>All samples start at the root node.</li>
<li>The tree evaluates splits such as:</li>
</ol>
<ul>
<li>`Credit Score > 700`</li>
<li>`Income > 50k`</li>
<li>`Debt Ratio < 0.35`</li>
</ul>
<ol>
<li>Suppose `Credit Score > 700` yields the largest impurity reduction.</li>
<li>The tree splits and continues training on remaining mixed nodes.</li>
</ol>
<hr>
<h3 id="final-learned-tree"><a href="#final-learned-tree" class="headerlink" title="Final Learned Tree"></a>Final Learned Tree</h3>
<p>Credit Score > 700? ‚îú‚îÄ‚îÄ Yes ‚Üí Approve Loan ‚îî‚îÄ‚îÄ No ‚îú‚îÄ‚îÄ Debt Ratio < 0.35? ‚îÇ ‚îú‚îÄ‚îÄ Yes ‚Üí Approve Loan ‚îÇ ‚îî‚îÄ‚îÄ No ‚Üí Reject Loan</p>
<hr>
<h3 id="prediction"><a href="#prediction" class="headerlink" title="Prediction"></a>Prediction</h3>
<p>For a new applicant:</p>
<ul>
<li>Credit Score = 680</li>
<li>Debt Ratio = 0.28</li>
</ul>
<p>Decision path:</p>
<ol>
<li>Credit Score > 700? ‚Üí No</li>
<li>Debt Ratio < 0.35? ‚Üí Yes</li>
</ol>
<p>‚Üí <strong>Loan Approved</strong></p>
<hr>

<summary><summary>Code Demo</summary>

</details>
<details class="post-details">
  <summary>Code Demo</summary>
<h2 id="code-demo"><a href="#code-demo" class="headerlink" title="Code Demo:"></a>Code Demo:</h2>
<pre><code>from sklearn.datasets import load_iris
from sklearn import tree
from matplotlib import pyplot as plt
import pandas  as pd
iris = load_iris()

X = iris.data
y = iris.target
df = pd.DataFrame(list(zip(X, y)), columns =['Features', 'Label'])
df</code></pre>
<p><img src="/images/data-mining-text-analytics/code1.png" alt="Code illustration" class="post-img"></p>
<pre><code>#build decision tree
clf = tree.DecisionTreeClassifier(criterion='entropy', max_depth=4,min_samples_leaf=4)
#max_depth represents max level allowed in each tree, min_samples_leaf minumum samples storable in leaf node

#fit the tree to iris dataset
clf.fit(X,y)

#plot decision tree
fig, ax = plt.subplots(figsize=(6, 6)) #figsize value changes the size of plot
tree.plot_tree(clf,ax=ax,feature_names=['sepal length','sepal width','petal length','petal width'])
plt.show()</code></pre>
<p><img src="/images/data-mining-text-analytics/code2.png" alt="Entropy illustration" class="post-img"></p>




</details>
<h2 id="key-takeaway"><a href="#key-takeaway" class="headerlink" title="Key Takeaway"></a>Key Takeaway</h2>
<blockquote><p>A decision tree learns interpretable rules by greedily selecting feature splits that reduce impurity, making it a powerful and transparent model for both classification and regression tasks.</p></blockquote>
<h2 id="stop-here-and-think"><a href="#stop-here-and-think" class="headerlink" title="Stop here and think:"></a>Stop here and think:</h2>
<h3 id="from-here-you-may-wonder-what-if-our-tree-is-too-complex----overfiting-now-we-introduce-pruing"><a href="#from-here-you-may-wonder-what-if-our-tree-is-too-complex----overfiting-now-we-introduce-pruing" class="headerlink" title="From here you may wonder what if our tree is too complex --> overfiting! Now we introduce pruing!"></a>From here you may wonder what if our tree is too complex --> overfiting! Now we introduce pruing!</h3>
<ul>
<li>Pruning is a technique that reduces the size of a decision tree by removing branches of the tree which provide little predictive power</li>
<li>It is a regularization method that reduces the complexity of the final model, thus reducing overfitting</li>
</ul>
<h4 id="pruning-methods"><a href="#pruning-methods" class="headerlink" title="Pruning methods:"></a>Pruning methods:</h4>
<ul>
<li>Pre-pruning: Stop the tree building algorithm before it fully classifies the data:</li>
<p>Entropy (or Gini Impurity) of the current set</p>
<li>Number of samples in the current set</li>
<li>Gain of the best-splitting attribute</li>
<li>Depth of the tree</li>
<li>Post-pruning: Build the complete tree, then replace some non-leaf nodes with leaf nodes if this improves validation error</li>
</ul>
<p><img src="/images/data-mining-text-analytics/pruning.png" alt="Entropy illustration" class="post-img"></p>
<h2 id="decision-tree-alos"><a href="#decision-tree-alos" class="headerlink" title="Decision Tree alos:"></a>Decision Tree alos:</h2>
<p><img src="/images/data-mining-text-analytics/dtalgo.png" alt="Entropy illustration" class="post-img"></p>
<hr>
<h2 id="handling-of-missing-values"><a href="#handling-of-missing-values" class="headerlink" title="Handling of missing values"></a>Handling of missing values</h2>
<h1 id="how-id3-handles-numerical-attributes"><a href="#how-id3-handles-numerical-attributes" class="headerlink" title="How ID3 Handles Numerical Attributes"></a>How ID3 Handles Numerical Attributes</h1>
<hr>
<h2 id="1-short-answer-core-idea"><a href="#1-short-answer-core-idea" class="headerlink" title="1. Short answer (core idea)"></a>1. Short answer (core idea)</h2>
<blockquote><p><strong>ID3 handles numerical attributes by converting them into binary categorical splits using a threshold, then applying Information Gain to choose the best threshold.</strong></p></blockquote>
<p>In other words, ID3 <strong>does not treat numerical attributes directly as continuous</strong>. Instead, it <strong>discretizes</strong> them into a yes/no question of the form:</p>
<p>\[ x_j \le t \quad \text{vs.} \quad x_j > t \]</p>
<hr>
<h2 id="2-why-id3-needs-special-handling-for-numerical-attributes"><a href="#2-why-id3-needs-special-handling-for-numerical-attributes" class="headerlink" title="2. Why ID3 needs special handling for numerical attributes"></a>2. Why ID3 needs special handling for numerical attributes</h2>
<p>The original <strong>ID3 algorithm</strong> was designed for:</p>
<ul>
<li><strong>Categorical (discrete) features</strong></li>
<li>Using <strong>Entropy</strong> and <strong>Information Gain</strong></li>
</ul>
<p>Problem with numerical attributes:</p>
<ul>
<li>Infinite possible values</li>
<li>Cannot directly enumerate ‚Äúall values‚Äù like categories</li>
</ul>
<p>So ID3 must <strong>transform numerical attributes into categorical decisions</strong>.</p>
<hr>
<summary>3. Step-by-step: How ID3 handles a numerical attribute</summary>
<details class="post-details">
  <summary>3. Step-by-step: How ID3 handles a numerical attribute</summary>
<h2 id="3-step-by-step-how-id3-handles-a-numerical-attribute"><a href="#3-step-by-step-how-id3-handles-a-numerical-attribute" class="headerlink" title="3. Step-by-step: How ID3 handles a numerical attribute"></a>3. Step-by-step: How ID3 handles a numerical attribute</h2>
<p>Assume we have:</p>
<ul>
<li>Numerical feature \(x\)</li>
<li>Target label \(y\)</li>
</ul>
<h3 id="step-1-sort-the-data-by-the-numerical-attribute"><a href="#step-1-sort-the-data-by-the-numerical-attribute" class="headerlink" title="Step 1: Sort the data by the numerical attribute"></a>Step 1: Sort the data by the numerical attribute</h3>
<p>Example:</p>
<table>
<thead><tr><th>Value \(x\)</th><th>Class \(y\)</th></tr></thead>
<tbody>
<tr><td>20</td><td>No</td></tr>
<tr><td>30</td><td>No</td></tr>
<tr><td>40</td><td>Yes</td></tr>
<tr><td>60</td><td>Yes</td></tr>
</tbody></table>
<hr>
<h3 id="step-2-generate-candidate-thresholds"><a href="#step-2-generate-candidate-thresholds" class="headerlink" title="Step 2: Generate candidate thresholds"></a>Step 2: Generate candidate thresholds</h3>
<p>ID3 considers <strong>midpoints between consecutive values</strong>:</p>
<p>\[ t_1 = \frac{20+30}{2}=25,\quad t_2 = \frac{30+40}{2}=35,\quad t_3 = \frac{40+60}{2}=50 \]</p>
<hr>
<h3 id="step-3-convert-each-threshold-into-a-binary-split"><a href="#step-3-convert-each-threshold-into-a-binary-split" class="headerlink" title="Step 3: Convert each threshold into a binary split"></a>Step 3: Convert each threshold into a binary split</h3>
<p>For each \(t\): x ‚â§ t  vs. x > t</p>
<p>Example for \(t=35\):</p>
<ul>
<li>Left: {20, 30}</li>
<li>Right: {40, 60}</li>
</ul>
<hr>
<h3 id="step-4-compute-information-gain-for-each-threshold"><a href="#step-4-compute-information-gain-for-each-threshold" class="headerlink" title="Step 4: Compute Information Gain for each threshold"></a>Step 4: Compute Information Gain for each threshold</h3>
<p>For each split:</p>
<p>\[ IG(t) = H(S) - \left( \frac{|S_L|}{|S|}H(S_L) + \frac{|S_R|}{|S|}H(S_R) \right) \]</p>
<p>ID3 computes this <strong>exactly the same way</strong> as for categorical features.</p>
<hr>
<h3 id="step-5-choose-the-best-threshold"><a href="#step-5-choose-the-best-threshold" class="headerlink" title="Step 5: Choose the best threshold"></a>Step 5: Choose the best threshold</h3>
<p>\[ t^* = \arg\max_t IG(t) \]</p>
<p>The numerical attribute is then treated as a <strong>binary categorical attribute</strong> at that node.</p>
<hr>


</details>
<h2 id="4-simple-example"><a href="#4-simple-example" class="headerlink" title="4. Simple example"></a>4. Simple example</h2>
<p>Feature: <strong>Age</strong> Target: <strong>Play Tennis</strong></p>
<p>Candidate split: Age ‚â§ 35? ‚îú‚îÄ‚îÄ Yes ‚Üí Mostly No ‚îî‚îÄ‚îÄ No ‚Üí Mostly Yes</p>
<p>This split is chosen if it gives the <strong>highest information gain</strong>.</p>
<hr>
<h2 id="5-more-detailed-example-with-entropy"><a href="#5-more-detailed-example-with-entropy" class="headerlink" title="5. More detailed example (with entropy)"></a>5. More detailed example (with entropy)</h2>
<p>Suppose parent node has:</p>
<ul>
<li>6 Yes, 4 No</li>
</ul>
<p>Entropy: \[ H(S)= -0.6\log_2 0.6 - 0.4\log_2 0.4 = 0.971 \]</p>
<p>Split at \(t=35\):</p>
<ul>
<li>Left: 4 No ‚Üí entropy = 0</li>
<li>Right: 6 Yes ‚Üí entropy = 0</li>
</ul>
<p>Information Gain: \[ IG = 0.971 - 0 = 0.971 \]</p>
<p>üëâ Perfect split ‚Üí ID3 selects it.</p>
<hr>
<h2 id="6-important-characteristics-of-id3s-approach"><a href="#6-important-characteristics-of-id3s-approach" class="headerlink" title="6. Important characteristics of ID3‚Äôs approach"></a>6. Important characteristics of ID3‚Äôs approach</h2>
<h3 id="what-id3-does-well"><a href="#what-id3-does-well" class="headerlink" title="What ID3 does well"></a>What ID3 does well</h3>
<ul>
<li>Simple and intuitive</li>
<li>Uses same criterion (Information Gain) everywhere</li>
<li>Produces interpretable threshold rules</li>
</ul>
<h3 id="limitations"><a href="#limitations" class="headerlink" title="Limitations"></a>Limitations</h3>
<ul>
<li>Greedy and local</li>
<li>Sensitive to noise</li>
<li>Repeated threshold testing is computationally expensive</li>
<li>Information Gain is biased toward many-valued attributes</li>
</ul>
<hr>
<h2 id="7-comparison-with-other-tree-algorithms"><a href="#7-comparison-with-other-tree-algorithms" class="headerlink" title="7. Comparison with other tree algorithms"></a>7. Comparison with other tree algorithms</h2>
<table>
<thead><tr><th>Algorithm</th><th>Numerical handling</th></tr></thead>
<tbody>
<tr><td>ID3</td><td>Binary threshold + IG</td></tr>
<tr><td>C4.5</td><td>Threshold + <strong>Gain Ratio</strong></td></tr>
<tr><td>CART</td><td>Threshold + <strong>Gini / MSE</strong></td></tr>
<tr><td>LightGBM</td><td>Histogram-based thresholds</td></tr>
</tbody></table>
<hr>
<h2 id="8-summary"><a href="#8-summary" class="headerlink" title="8. Summary"></a>8. Summary</h2>
<blockquote><p>ID3 handles numerical attributes by sorting values, testing candidate threshold-based binary splits, and selecting the threshold that maximizes Information Gain.</p></blockquote>
<hr>
<h2 id="pros-and-cons-of-decision-tree"><a href="#pros-and-cons-of-decision-tree" class="headerlink" title="Pros and Cons of Decision tree:"></a>Pros and Cons of Decision tree:</h2>
<h3 id="pros"><a href="#pros" class="headerlink" title="Pros:"></a>Pros:</h3>
<h4 id="compared-to-other-algorithms-decision-trees-requires-less-effort-for-data-preparation-during-pre-processing"><a href="#compared-to-other-algorithms-decision-trees-requires-less-effort-for-data-preparation-during-pre-processing" class="headerlink" title="Compared to other algorithms decision trees requires less effort for data preparation during pre-processing."></a>Compared to other algorithms decision trees requires less effort for data preparation during pre-processing.</h4>
<ul>
<li>A decision tree does not require normalization of data.</li>
<li>A decision tree does not require scaling of data as well.</li>
<li>Missing values in the data also do NOT affect the process of building a decision tree to any considerable extent.</li>
<li>A Decision tree model is very intuitive and easy to explain to technical teams as well as stakeholders.</li>
<li>It can handle outlier problem.</li>
<li>Random Forest can be used to solve both classification as well as regression problems.</li>
<li>Decision Tree can handle both continuous and categorical variables.</li>
</ul>
<h3 id="cons"><a href="#cons" class="headerlink" title="Cons:"></a>Cons:</h3>
<h4 id="it-has-overfitting-problem-as-the-nodes-were-spited-in-detail"><a href="#it-has-overfitting-problem-as-the-nodes-were-spited-in-detail" class="headerlink" title="It has overfitting problem as the nodes were spited in detail."></a>It has overfitting problem as the nodes were spited in detail.</h4>
<ul>
<li>A <strong>small change</strong> in the data can cause a <strong>large change</strong> in the structure of the decision tree causing instability.</li>
<li>For a Decision tree sometimes calculation can go <strong>far more complex</strong> compared to other algorithms.</li>
<li>Decision tree often involves <strong>higher time</strong> to train the model.</li>
<li>Decision tree training is relatively expensive as the complexity and time has taken are more.</li>
<li>The Decision Tree algorithm is <strong>inadequate</strong> for applying <strong>regression</strong> and predicting <strong>continuous values</strong>.</li>
</ul>
<h2 id="random-forest"><a href="#random-forest" class="headerlink" title="Random Forest:"></a>Random Forest:</h2>
<p><img src="/images/data-mining-text-analytics/RF.png" alt="RF illustration" class="post-img"></p>
<h3 id="rf-algo"><a href="#rf-algo" class="headerlink" title="RF Algo"></a>RF Algo</h3>
<h4 id="diversity--when-creating-an-individual-tree-not-all-qualities-variables-or-features-are-taken-into-account-each-tree-is-unique"><a href="#diversity--when-creating-an-individual-tree-not-all-qualities-variables-or-features-are-taken-into-account-each-tree-is-unique" class="headerlink" title="Diversity- When creating an individual tree, not all qualities, variables, or features are taken into account; each tree is unique."></a>Diversity- When creating an individual tree, not all qualities, variables, or features are taken into account; each tree is unique.</h4>
<ul>
<li>Immune to dimensionality constraint- The feature space is minimized because each tree does not consider all features.</li>
<li>Parallelization- Each tree is built from scratch using different data and properties. This means we can fully utilize the CPU to create random forests.</li>
<li>Train-Test split- In a random forest, there is no need to separate the data for train and test because the decision tree will always miss 30% of the data.</li>
<li>Stability- The result is stable because it is based on majority voting/averaging.</li>
</ul>
<h3 id="how-random-forest-classifier-is-different-from-decision-trees"><a href="#how-random-forest-classifier-is-different-from-decision-trees" class="headerlink" title="How Random Forest Classifier is different from decision trees"></a>How Random Forest Classifier is different from decision trees</h3>
<ul>
<li>Overfitting - Overfitting is not there as in Decision trees since random forests are formed from subsets of data, and the final output is based on average or majority rating.</li>
<li>Speed - Random Forest Algorithm is relatively slower than Decision Trees.</li>
<li>Process - Random forest collects data at random, forms a decision tree, and averages the results. It does not rely on any formulas as in Decision trees.</li>
</ul>
<summary><summary>Code Demo</summary>
<details class="post-details">
  <summary>Code Demo</summary>
<h3 id="rf-code-demo"><a href="#rf-code-demo" class="headerlink" title="RF code demo"></a>RF code demo</h3>
<pre><code>from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
from sklearn.datasets import load_breast_cancer

# Load example dataset
X, y = load_breast_cancer(return_X_y=True)

# Train / test split
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)

# Build Random Forest
rf = RandomForestClassifier(
    n_estimators=100,      # number of trees
    max_depth=None,        # let trees grow fully
    min_samples_split=2,   # minimum samples to split
    random_state=42
)

# Train
rf.fit(X_train, y_train)

# Predict
y_pred = rf.predict(X_test)

# Evaluate
print("Accuracy:", accuracy_score(y_test, y_pred))</code></pre>

          </div>
<div class="post-content-inner-space">
            <div class="space-toc-main animate__animated  animate__fadeInUp">
              <ol class="space-toc">
<li class="space-toc-item space-toc-level-2"><a class="space-toc-link" href="#1-overview-what-is-a-decision-tree-and-what-are-decision-criteria"><span class="space-toc-text">1. Overview: What is a Decision Tree and What Are Decision Criteria?</span></a></li>
<li class="space-toc-item space-toc-level-2"><a class="space-toc-link" href="#2-decision-criteria-explained-with-examples"><span class="space-toc-text">2. Decision Criteria Explained (with Examples)</span></a></li>
<li class="space-toc-item space-toc-level-2"><a class="space-toc-link" href="#3-how-a-decision-tree-is-trained"><span class="space-toc-text">3. How a Decision Tree Is Trained</span></a></li>
<li class="space-toc-item space-toc-level-2"><a class="space-toc-link" href="#4-real-world-example-loan-approval-prediction"><span class="space-toc-text">4. Real-World Example: Loan Approval Prediction</span></a></li>
<li class="space-toc-item space-toc-level-2"><a class="space-toc-link" href="#code-demo"><span class="space-toc-text">Code Demo:</span></a></li>
<li class="space-toc-item space-toc-level-2"><a class="space-toc-link" href="#key-takeaway"><span class="space-toc-text">Key Takeaway</span></a></li>
<li class="space-toc-item space-toc-level-2"><a class="space-toc-link" href="#stop-here-and-think"><span class="space-toc-text">Stop here and think:</span></a></li>
<li class="space-toc-item space-toc-level-2"><a class="space-toc-link" href="#decision-tree-alos"><span class="space-toc-text">Decision Tree alos:</span></a></li>
<li class="space-toc-item space-toc-level-2"><a class="space-toc-link" href="#handling-of-missing-values"><span class="space-toc-text">Handling of missing values</span></a></li>
<li class="space-toc-item space-toc-level-2"><a class="space-toc-link" href="#1-short-answer-core-idea"><span class="space-toc-text">1. Short answer (core idea)</span></a></li>
<li class="space-toc-item space-toc-level-2"><a class="space-toc-link" href="#2-why-id3-needs-special-handling-for-numerical-attributes"><span class="space-toc-text">2. Why ID3 needs special handling for numerical attributes</span></a></li>
<li class="space-toc-item space-toc-level-2"><a class="space-toc-link" href="#3-step-by-step-how-id3-handles-a-numerical-attribute"><span class="space-toc-text">3. Step-by-step: How ID3 handles a numerical attribute</span></a></li>
<li class="space-toc-item space-toc-level-2"><a class="space-toc-link" href="#4-simple-example"><span class="space-toc-text">4. Simple example</span></a></li>
<li class="space-toc-item space-toc-level-2"><a class="space-toc-link" href="#5-more-detailed-example-with-entropy"><span class="space-toc-text">5. More detailed example (with entropy)</span></a></li>
<li class="space-toc-item space-toc-level-2"><a class="space-toc-link" href="#6-important-characteristics-of-id3s-approach"><span class="space-toc-text">6. Important characteristics of ID3‚Äôs approach</span></a></li>
<li class="space-toc-item space-toc-level-2"><a class="space-toc-link" href="#7-comparison-with-other-tree-algorithms"><span class="space-toc-text">7. Comparison with other tree algorithms</span></a></li>
<li class="space-toc-item space-toc-level-2"><a class="space-toc-link" href="#8-summary"><span class="space-toc-text">8. Summary</span></a></li>
<li class="space-toc-item space-toc-level-2"><a class="space-toc-link" href="#pros-and-cons-of-decision-tree"><span class="space-toc-text">Pros and Cons of Decision tree:</span></a></li>
<li class="space-toc-item space-toc-level-2"><a class="space-toc-link" href="#random-forest"><span class="space-toc-text">Random Forest:</span></a></li>
</ol>
            </div>
          </div>
        </div>
      </div>
    <div class="post-nav-box">
  <div class="post-nav-title">Next in Data Mining and Text Analytics</div>
  <a class="post-nav-link" href="/2026/02/10/Decision-Tree-and-Random-Forest/">01. Decision Tree and Random Forest</a>
</div>
</article>
  </div>
</div>

<div class="footer-outer animate__animated  animate__fadeInUp">
  <div class="footer-inner">
    <div class="footer-text">
      <p>Artificial Intelligence |  Financial Engineering | Mathematics | Computer Science  <strong>Rongyan <i class="ri-copyright-line"></i> 2026</strong></p>
    </div>
    <div class="footer-contact">
      <ul class="footer-ul">
        <li class="footer-li"><a href="https://github.com/RongyanYuan" target="_blank"><i class="ri-github-line"></i></a></li>
        <li class="footer-li"><a href="mailto:adrianrongyanyun@gmail.com" target="_blank"><i class="ri-mail-line"></i></a></li>
        <li class="footer-li"><a href="https://www.linkedin.com/in/rongyan-yuan-a076971b2/" target="_blank"><i class="ri-linkedin-box-line"></i></a></li>
      </ul>
    </div>
  </div>
</div>

<script src="/js/white.js"></script>
<script>
  window.MathJax = {
    tex: {inlineMath: [['\\(','\\)'], ['$', '$']]},
    svg: {fontCache: 'global'}
  };
</script>
<script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-svg.js"></script>
</body>
</html>




</details>
