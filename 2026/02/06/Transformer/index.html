


<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>  02. Transformer |    Rongyan&#39;s Blog</title>
  <meta name="description" content="Financial Engineering | Mathematics | Computer Science">
  <!-- 标签页图标 -->
  

  <!-- 图标库 -->
  <link href="https://cdn.jsdelivr.net/npm/remixicon@2.2.0/fonts/remixicon.css" rel="stylesheet">
  <!-- 动画库 -->
  
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/fushaolei/cdn-white@1.0/css/animate.css"/>
  
  <!-- css文件 -->
  
<link rel="stylesheet" href="/css/white.css">

  <!-- 代码高亮 -->
  
<meta name="generator" content="Hexo 6.2.0"></head>


<body>

<div class="menu-outer">
    <div class="menu-inner">
      <div class="menu-site-name  animate__animated  animate__fadeInUp">
        <a href="/">
          Rongyan&#39;s Blog
        </a>
        
      </div>
      <div class="menu-group">
        <ul class="menu-ul">
        
          <a href="/" class="nav-link">
            <li class="menu-li  animate__animated  animate__fadeInUp">
              HOME
            </li>
          </a>
        
          <a href="/archives" class="nav-link">
            <li class="menu-li  animate__animated  animate__fadeInUp">
              BLOG
            </li>
          </a>
        
        
          <li class="menu-li animate__animated  animate__fadeInUp" id="sort">
             CATEGORIES
             <div class="categories-outer " id="sort-div">
               <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/AboutMe/">AboutMe</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/ai/">AI</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs/">CS</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/math/">Math</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/quant/">Quant</a></li></ul>
             </div>
          </li>
        
        
        <a href="/search">
          <li class="menu-li  animate__animated  animate__fadeInUp">
            <i class="ri-search-line"></i>
          </li>
        </a>

          <li class="menu-li animate__animated  animate__fadeInUp lang-switcher-desktop">
            <a href="/2026/02/06/Transformer/index.html">EN</a> |
            <a href="/2026/02/06/Transformer/index-zh.html">中文</a>
          </li>
        
          <li class="menu-li animate__animated  animate__fadeInUp" id="mobile-menu">
            <i class="ri-menu-line"></i>
          </li>
        
        </ul>

      </div>

    </div>
</div>
<div id="mobile-main" class="animate__animated  animate__fadeIn">
  <div class="mobile-menu-inner">
    <div class="mobile-menu-site-name animate__animated  animate__fadeInUp">
      <a href="/">
        Rongyan&#39;s Blog
      </a>
    </div>
    <div class="mobile-menu-group" id="mobile-close">
      <i class="ri-close-line"></i>
    </div>

  </div>

  <div class="mobile-menu-div">
  
    <a href="/" class="mobile-nav-link">
      <div class="mobile-menu-child animate__animated  animate__fadeInUp">
        <span>HOME</span>
      </div>
    </a>
  
    <a href="/archives" class="mobile-nav-link">
      <div class="mobile-menu-child animate__animated  animate__fadeInUp">
        <span>BLOG</span>
      </div>
    </a>
  
  
    <div class="mobile-menu-child animate__animated  animate__fadeInUp lang-switcher-mobile">
      <a href="/2026/02/06/Transformer/index.html">EN</a> |
      <a href="/2026/02/06/Transformer/index-zh.html">中文</a>
    </div>

    <a href="/search">  
      <div class="mobile-menu-child  animate__animated  animate__fadeInUp">
        <i class="ri-search-line"></i>
      </div>
    </a>
    
  </div>


</div>

<div class="body-outer">
  <div class="body-inner">
    
<article class="post-inner">
  <div class="post-content-outer">
    <div class="post-intro">
      <div class="post-title animate__animated  animate__fadeInUp">02. Transformer</div>
      <div class="meta-intro animate__animated  animate__fadeInUp">Feb 06 2026</div>
      
    </div>
    <div class="post-content-inner">
      <div class="post-content-inner-space">

      </div>
      <div class="post-content-main animate__animated  animate__fadeInUp">
        <!-- top型目录 -->

        <h1 id="Transformer"><a href="#Transformer" class="headerlink" title="02. Transformer"></a>02. Transformer</h1>

        <p>In a Transformer, tokens are first mapped to embeddings and augmented with positional encodings, then passed through layers where learned linear projections produce Queries, Keys, and Values so self-attention computes weighted combinations of token information, encoders iteratively refine global contextual representations, and the decoder uses masked self-attention plus cross-attention to encoder outputs to autoregressively generate each next token.</p>

        <h2 id="Reading-List"><a href="#Reading-List" class="headerlink" title="Reading List"></a>Reading List</h2>
        <ul>
          <li><a href="/pdf/NIPS-2017-attention-is-all-you-need-Paper.pdf" target="_blank">Attention Is All You Need (NIPS 2017)</a></li>
          <li><a href="/pdf/transformer24aug.pdf" target="_blank">Transformer Lecture Notes (Aug 24)</a></li>
        </ul>



        <h2 id="Background"><a href="#Background" class="headerlink" title="Background"></a>1. Background: Why Earlier Models Fell Short</h2>
        <ul>
          <li><strong>RNNs:</strong> struggle with long‑range dependencies due to vanishing/exploding gradients and sequential computation.</li>
          <li><strong>LSTMs/GRUs:</strong> improve memory but still process tokens sequentially, limiting parallelism and speed.</li>
          <li><strong>CNNs for NLP:</strong> capture local patterns but need many layers to model long context.</li>
        </ul>

        <h2 id="Why-Works"><a href="#Why-Works" class="headerlink" title="Why Transformers Work Well"></a>2. Why Transformers Work Well in NLP</h2>
        <ul>
          <li><strong>Self‑attention:</strong> directly connects all tokens, modeling long‑range dependencies efficiently.</li>
          <li><strong>Parallelism:</strong> attention allows full‑sequence computation on GPUs.</li>
          <li><strong>Scalability:</strong> performance improves with more data and parameters.</li>
        </ul>

        <h2 id="Mechanism"><a href="#Mechanism" class="headerlink" title="Mechanism"></a>3. Mechanism of the Transformer</h2>
        <ul>
          <li><strong>Embedding + Positional Encoding:</strong> tokens are embedded and positions are encoded.</li>
          <li><strong>Multi‑Head Self‑Attention:</strong> attention computed by \(	ext{softmax}(QK^T/\sqrt{d})V\).</li>
          <li><strong>Feed‑Forward Networks:</strong> position‑wise MLPs add non‑linearity.</li>
          <li><strong>Residual + LayerNorm:</strong> stabilize training.</li>
        </ul>

        <h2 id="Pros-Cons"><a href="#Pros-Cons" class="headerlink" title="Pros and Cons"></a>4. Pros and Cons</h2>
        <ul>
          <li><strong>Pros:</strong> strong long‑context modeling, parallelizable, highly scalable.</li>
          <li><strong>Cons:</strong> quadratic attention cost in sequence length, large memory usage.</li>
        </ul>

        <h2 id="Future"><a href="#Future" class="headerlink" title="Future"></a>5. Future of Transformers</h2>
        <ul>
          <li>Efficient attention (linear, sparse, or kernelized attention).</li>
          <li>Long‑context architectures and retrieval‑augmented models.</li>
          <li>Better alignment, controllability, and interpretability.</li>
        </ul>

        <!-- 分类文章 -->
        
          <div class="post-categoris-bottom">
            <div class="post-categoris-name">AI</div>
            <ul>
              <li class="me base">
                <a  href="/2026/02/06/Transformer/" class="post-categoris-bottom-link">
                  02. Transformer
                </a>
              </li>
            </ul>
          </div>

        
      </div><div class="post-content-inner-space">
        
          <div class="space-toc-main animate__animated  animate__fadeInUp">
            <ol class="space-toc"><li class="space-toc-item space-toc-level-2"><a class="space-toc-link" href="#Background"><span class="space-toc-text">1. Background: Why Earlier Models Fell Short</span></a></li><li class="space-toc-item space-toc-level-2"><a class="space-toc-link" href="#Why-Works"><span class="space-toc-text">2. Why Transformers Work Well in NLP</span></a></li><li class="space-toc-item space-toc-level-2"><a class="space-toc-link" href="#Mechanism"><span class="space-toc-text">3. Mechanism of the Transformer</span></a></li><li class="space-toc-item space-toc-level-2"><a class="space-toc-link" href="#Pros-Cons"><span class="space-toc-text">4. Pros and Cons</span></a></li><li class="space-toc-item space-toc-level-2"><a class="space-toc-link" href="#Future"><span class="space-toc-text">5. Future of Transformers</span></a></li></ol>
           </div>
        </div>
   </div>
    <!-- 评论 -->
    
  </div>
</article>
  </div>
</div>



<!-- 如果是home模式的话，不在首页就显示footer，如果不是home模式的话 所有都显示footer -->

  <div class="footer-outer animate__animated  animate__fadeInUp">
    <div class="footer-inner">
    <div class="footer-text">
    <p>Artificial Intelligence |  Financial Engineering | Mathematics | Computer Science  <strong>Rongyan <i class="ri-copyright-line"></i> 2026</strong></p>

    </div>
    <div class="footer-contact">
    <ul class="footer-ul">
        
        <li class="footer-li">
            <a href="https://github.com/RongyanYuan" target="_blank">
                <i class="ri-github-line"></i>
            </a>
        </li>
        
        <li class="footer-li">
            <a href="mailto:adrianrongyanyun@gmail.com" target="_blank">
                <i class="ri-mail-line"></i>
            </a>
        </li>
        
        <li class="footer-li">
            <a href="https://www.linkedin.com/in/rongyan-yuan-a076971b2/" target="_blank">
                <i class="ri-linkedin-box-line"></i>
            </a>
        </li>
        
    </ul>
    </div>
    </div>
</div>






<script src="/js/white.js"></script>




<script>
  window.MathJax = {
    tex: {inlineMath: [['\\(','\\)'], ['$', '$']]},
    svg: {fontCache: 'global'}
  };
</script>
<script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-svg.js"></script>

</body>
</html>
