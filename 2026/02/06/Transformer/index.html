
<!DOCTYPE html>

<html lang="en">
<head>
<meta charset="utf-8"/>
<meta content="width=device-width, initial-scale=1.0" name="viewport"/>
<title>  02. Transformer |    Rongyan's Blog</title>
<meta content="Financial Engineering | Mathematics | Computer Science" name="description"/>
<!-- 标签页图标 -->
<!-- 图标库 -->
<link href="https://cdn.jsdelivr.net/npm/remixicon@2.2.0/fonts/remixicon.css" rel="stylesheet"/>
<!-- 动画库 -->
<link href="https://cdn.jsdelivr.net/gh/fushaolei/cdn-white@1.0/css/animate.css" rel="stylesheet">
<!-- css文件 -->
<link href="/css/white.css" rel="stylesheet"/>
<!-- 代码高亮 -->
<meta content="Hexo 6.2.0" name="generator"/></link></head>
<body>
<div class="menu-outer">
<div class="menu-inner">
<div class="menu-site-name animate__animated animate__fadeInUp">
<a href="/">
          Rongyan's Blog
        </a>
<div class="menu-redirect"><a class="nav-link back-topic-link" href="/categories/ai/ai-fundamentals/">← AI Fundamentals</a></div></div>
<div class="menu-group">
<ul class="menu-ul">
<a class="nav-link" href="/">
<li class="menu-li animate__animated animate__fadeInUp">
              HOME
            </li>
</a>
<a class="nav-link" href="/archives">
<li class="menu-li animate__animated animate__fadeInUp">
              BLOG
            </li>
</a>
<li class="menu-li animate__animated animate__fadeInUp" id="sort">
             CATEGORIES
             <div class="categories-outer" id="sort-div">
<ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/AboutMe/">AboutMe</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/ai/">AI</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs/">CS</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/math/">Math</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/quant/">Quant</a></li></ul>
</div>
</li>
<a href="/search">
<li class="menu-li animate__animated animate__fadeInUp">
<i class="ri-search-line"></i>
</li>
</a>
<li class="menu-li animate__animated animate__fadeInUp lang-switcher-desktop">
<a href="/2026/02/06/Transformer/index.html">EN</a> |
            <a href="/2026/02/06/Transformer/index-zh.html">中文</a>
</li>
<li class="menu-li animate__animated animate__fadeInUp" id="mobile-menu">
<i class="ri-menu-line"></i>
</li>
</ul>
</div>
</div>
</div>
<div class="animate__animated animate__fadeIn" id="mobile-main">
<div class="mobile-menu-inner">
<div class="mobile-menu-site-name animate__animated animate__fadeInUp">
<a href="/">
        Rongyan's Blog
      </a>
</div>
<div class="mobile-menu-group" id="mobile-close">
<i class="ri-close-line"></i>
</div>
</div>
<div class="mobile-menu-div">
<a class="mobile-nav-link" href="/">
<div class="mobile-menu-child animate__animated animate__fadeInUp">
<span>HOME</span>
</div>
</a>
<a class="mobile-nav-link" href="/archives">
<div class="mobile-menu-child animate__animated animate__fadeInUp">
<span>BLOG</span>
</div>
</a>
<div class="mobile-menu-child animate__animated animate__fadeInUp lang-switcher-mobile">
<a href="/2026/02/06/Transformer/index.html">EN</a> |
      <a href="/2026/02/06/Transformer/index-zh.html">中文</a>
</div>
<a href="/search">
<div class="mobile-menu-child animate__animated animate__fadeInUp">
<i class="ri-search-line"></i>
</div>
</a>
</div>
</div>
<div class="body-outer">
<div class="body-inner">
<article class="post-inner">
<div class="post-content-outer">
<div class="post-intro">
<div class="post-title animate__animated animate__fadeInUp">02. Transformer</div>
<div class="meta-intro animate__animated animate__fadeInUp">Feb 06 2026</div>
</div>
<div class="post-content-inner">
<div class="post-content-inner-space">
</div>
<div class="post-content-main animate__animated animate__fadeInUp">
<!-- top型目录 -->
<h1 id="Transformer"><a class="headerlink" href="#Transformer" title="02. Transformer"></a>02. Transformer</h1>
<p>In a Transformer, tokens are first mapped to embeddings and augmented with positional encodings, then passed through layers where learned linear projections produce Queries, Keys, and Values so self-attention computes weighted combinations of token information, encoders iteratively refine global contextual representations, and the decoder uses masked self-attention plus cross-attention to encoder outputs to autoregressively generate each next token.</p>
<h2 id="Reading-List"><a class="headerlink" href="#Reading-List" title="Reading List"></a>Reading List</h2>
<ul>
<li><a href="/pdf/NIPS-2017-attention-is-all-you-need-Paper.pdf" target="_blank">Attention Is All You Need (NIPS 2017)</a></li>
<li><a href="/pdf/transformer24aug.pdf" target="_blank">Transformer Lecture Notes (Aug 24)</a></li>
</ul>
<h2 id="Background"><a class="headerlink" href="#Background" title="Background"></a>1. Background: Why Earlier Models Fell Short</h2>
<ul>
<li><strong>RNNs:</strong> struggle with long‑range dependencies due to vanishing/exploding gradients and sequential computation.</li>
<li><strong>LSTMs/GRUs:</strong> improve memory but still process tokens sequentially, limiting parallelism and speed.</li>
<li><strong>CNNs for NLP:</strong> capture local patterns but need many layers to model long context.</li>
</ul>
<h2 id="Why-Works"><a class="headerlink" href="#Why-Works" title="Why Transformers Work Well"></a>2. Why Transformers Work Well in NLP</h2>
<ul>
<li><strong>Self‑attention:</strong> directly connects all tokens, modeling long‑range dependencies efficiently.</li>
<li><strong>Parallelism:</strong> attention allows full‑sequence computation on GPUs.</li>
<li><strong>Scalability:</strong> performance improves with more data and parameters.</li>
</ul>
<h2 id="Mechanism"><a class="headerlink" href="#Mechanism" title="Mechanism"></a>3. Mechanism of the Transformer</h2>
<ul>
<li><strong>Embedding + Positional Encoding:</strong> tokens are embedded and positions are encoded.</li>
<li><strong>Multi‑Head Self‑Attention:</strong> attention computed by \(	ext{softmax}(QK^T/\sqrt{d})V\).</li>
<li><strong>Feed‑Forward Networks:</strong> position‑wise MLPs add non‑linearity.</li>
<li><strong>Residual + LayerNorm:</strong> stabilize training.</li>
</ul>
<h2 id="Pros-Cons"><a class="headerlink" href="#Pros-Cons" title="Pros and Cons"></a>4. Pros and Cons</h2>
<ul>
<li><strong>Pros:</strong> strong long‑context modeling, parallelizable, highly scalable.</li>
<li><strong>Cons:</strong> quadratic attention cost in sequence length, large memory usage.</li>
</ul>
<h2 id="Future"><a class="headerlink" href="#Future" title="Future"></a>5. Future of Transformers</h2>
<ul>
<li>Efficient attention (linear, sparse, or kernelized attention).</li>
<li>Long‑context architectures and retrieval‑augmented models.</li>
<li>Better alignment, controllability, and interpretability.</li>
</ul>
<!-- 分类文章 -->
<div class="post-categoris-bottom">
<div class="post-categoris-name">AI</div>
<ul>
<li class="me base">
<a class="post-categoris-bottom-link" href="/2026/02/07/Reinforcement-Learning/">03. Reinforcement Learning</a>
</li>
</ul>
</div>
</div><div class="post-content-inner-space">
<div class="space-toc-main animate__animated animate__fadeInUp">
<ol class="space-toc"><li class="space-toc-item space-toc-level-2"><a class="space-toc-link" href="#Background"><span class="space-toc-text">1. Background: Why Earlier Models Fell Short</span></a></li><li class="space-toc-item space-toc-level-2"><a class="space-toc-link" href="#Why-Works"><span class="space-toc-text">2. Why Transformers Work Well in NLP</span></a></li><li class="space-toc-item space-toc-level-2"><a class="space-toc-link" href="#Mechanism"><span class="space-toc-text">3. Mechanism of the Transformer</span></a></li><li class="space-toc-item space-toc-level-2"><a class="space-toc-link" href="#Pros-Cons"><span class="space-toc-text">4. Pros and Cons</span></a></li><li class="space-toc-item space-toc-level-2"><a class="space-toc-link" href="#Future"><span class="space-toc-text">5. Future of Transformers</span></a></li></ol>
</div>
</div>
</div>
<!-- 评论 -->
</div>
</article>
</div>
</div>
<!-- 如果是home模式的话，不在首页就显示footer，如果不是home模式的话 所有都显示footer -->
<div class="footer-outer animate__animated animate__fadeInUp">
<div class="footer-inner">
<div class="footer-text">
<p>Artificial Intelligence |  Financial Engineering | Mathematics | Computer Science  <strong>Rongyan <i class="ri-copyright-line"></i> 2026</strong></p>
</div>
<div class="footer-contact">
<ul class="footer-ul">
<li class="footer-li">
<a href="https://github.com/RongyanYuan" target="_blank">
<i class="ri-github-line"></i>
</a>
</li>
<li class="footer-li">
<a href="mailto:adrianrongyanyun@gmail.com" target="_blank">
<i class="ri-mail-line"></i>
</a>
</li>
<li class="footer-li">
<a href="https://www.linkedin.com/in/rongyan-yuan-a076971b2/" target="_blank">
<i class="ri-linkedin-box-line"></i>
</a>
</li>
</ul>
</div>
</div>
</div>
<script src="/js/white.js"></script>
<script>
  window.MathJax = {
    tex: {inlineMath: [['\\(','\\)'], ['$', '$']]},
    svg: {fontCache: 'global'}
  };
</script>
<script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-svg.js"></script>
</body>
</html>
