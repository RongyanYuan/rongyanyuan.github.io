<!DOCTYPE html>

<html lang="en">
<head>
<meta charset="utf-8"/>
<meta content="width=device-width, initial-scale=1.0" name="viewport"/>
<title>  01. Deep Learning Basics |    Rongyan's Blog</title>
<meta content="Financial Engineering | Mathematics | Computer Science" name="description"/>
<link href="https://cdn.jsdelivr.net/npm/remixicon@2.2.0/fonts/remixicon.css" rel="stylesheet"/>
<link href="https://cdn.jsdelivr.net/gh/fushaolei/cdn-white@1.0/css/animate.css" rel="stylesheet"/>
<link href="/css/white.css" rel="stylesheet">
</link></head>
<body>
<div class="menu-outer">
<div class="menu-inner">
<div class="menu-site-name animate__animated animate__fadeInUp">
<a href="/">Rongyan's Blog</a>
<div class="menu-redirect"><a class="nav-link back-topic-link" href="/categories/ai/deep-learning/">← Deep Learning</a></div></div>
<div class="menu-group">
<ul class="menu-ul">
<a class="nav-link" href="/"><li class="menu-li animate__animated animate__fadeInUp">HOME</li></a>
<a class="nav-link" href="/archives"><li class="menu-li animate__animated animate__fadeInUp">BLOG</li></a>
<li class="menu-li animate__animated animate__fadeInUp" id="sort">
           CATEGORIES
           <div class="categories-outer" id="sort-div">
<ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/AboutMe/">AboutMe</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/ai/">AI</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs/">CS</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/math/">Math</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/quant/">Quant</a></li></ul>
</div>
</li>
<a href="/search"><li class="menu-li animate__animated animate__fadeInUp"><i class="ri-search-line"></i></li></a>
<li class="menu-li animate__animated animate__fadeInUp lang-switcher-desktop">
<a href="/2026/02/14/Deep-Learning-Basics/index.html">EN</a> |
          <a href="/2026/02/14/Deep-Learning-Basics/index-zh.html">中文</a>
</li>
<li class="menu-li animate__animated animate__fadeInUp" id="mobile-menu"><i class="ri-menu-line"></i></li>
</ul>
</div>
</div>
</div>
<div class="animate__animated animate__fadeIn" id="mobile-main">
<div class="mobile-menu-inner">
<div class="mobile-menu-site-name animate__animated animate__fadeInUp"><a href="/">Rongyan's Blog</a></div>
<div class="mobile-menu-group" id="mobile-close"><i class="ri-close-line"></i></div>
</div>
<div class="mobile-menu-div">
<a class="mobile-nav-link" href="/"><div class="mobile-menu-child animate__animated animate__fadeInUp"><span>HOME</span></div></a>
<a class="mobile-nav-link" href="/archives"><div class="mobile-menu-child animate__animated animate__fadeInUp"><span>BLOG</span></div></a>
<div class="mobile-menu-child animate__animated animate__fadeInUp lang-switcher-mobile">
<a href="/2026/02/14/Deep-Learning-Basics/index.html">EN</a> |
      <a href="/2026/02/14/Deep-Learning-Basics/index-zh.html">中文</a>
</div>
<a href="/search"><div class="mobile-menu-child animate__animated animate__fadeInUp"><i class="ri-search-line"></i></div></a>
</div>
</div>
<div class="body-outer">
<div class="body-inner">
<article class="post-inner">
<div class="post-content-outer">
<div class="post-intro">
<div class="post-title animate__animated animate__fadeInUp">01. Deep Learning Basics</div>
<div class="meta-intro animate__animated animate__fadeInUp">Feb 14 2026</div>
</div>
<div class="post-content-inner">
<div class="post-content-inner-space"></div>
<div class="post-content-main animate__animated animate__fadeInUp">
<h1 id="DL-Basics"><a class="headerlink" href="#DL-Basics" title="01. Deep Learning Basics"></a>01. Deep Learning Basics</h1>
<h2 id="Loss-Functions"><a class="headerlink" href="#Loss-Functions" title="Common Loss Functions"></a>Section 1 Common Loss Functions</h2>
<table>
<thead>
<tr>
<th>Loss</th>
<th>Formula</th>
<th>Use</th>
<th>Characteristics</th>
<th>Typical scenarios</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>MSE</strong></td>
<td>\(\frac{1}{n}\sum (y-\hat{y})^2\)</td>
<td>Regression</td>
<td>Sensitive to outliers; differentiable; penalizes large errors</td>
<td>Linear regression, numeric prediction</td>
</tr>
<tr>
<td><strong>MAE</strong></td>
<td>\(\frac{1}{n}\sum |y-\hat{y}|\)</td>
<td>Regression</td>
<td>Robust to outliers; non‑differentiable at 0; constant gradient</td>
<td>Robust regression, financial prediction</td>
</tr>
<tr>
<td><strong>Cross‑entropy</strong></td>
<td>\(-\sum y\,\log(\hat{y})\)</td>
<td>Classification</td>
<td>Measures distribution gap; gradient proportional to error</td>
<td>Logistic regression, NN classification</td>
</tr>
<tr>
<td><strong>Huber</strong></td>
<td>\(\frac{1}{2}\delta^2\ \text{if}\ |\delta|\le\varepsilon;\ \varepsilon(|\delta|-\frac{1}{2}\varepsilon)\ \text{otherwise}\)</td>
<td>Regression</td>
<td>Balance MSE/MAE; moderately sensitive to outliers</td>
<td>Robust regression, outlier handling</td>
</tr>
<tr>
<td><strong>Hinge</strong></td>
<td>\(\max(0, 1 - y\hat{y})\)</td>
<td>Classification</td>
<td>Max‑margin, focuses on boundary</td>
<td>SVM</td>
</tr>
<tr>
<td><strong>Log loss</strong></td>
<td>\(-\log P(y\mid x)\)</td>
<td>Probabilistic prediction</td>
<td>Penalizes low‑prob correct predictions</td>
<td>Probabilistic / multiclass models</td>
</tr>
</tbody>
</table>
<h3>Key selection factors</h3>
<ol>
<li><strong>Task type</strong>: regression → MSE/MAE/Huber; classification → cross‑entropy/hinge.</li>
<li><strong>Outliers</strong>: many outliers → MAE/Huber; few → MSE.</li>
<li><strong>Gradient smoothness</strong>: MSE/cross‑entropy are smooth; MAE is not.</li>
<li><strong>Efficiency</strong>: MSE is fastest; Huber is more complex.</li>
</ol>
<h2 id="GD"><a class="headerlink" href="#GD" title="Gradient Descent"></a>Section 2 Gradient Descent</h2>
<h3>Core idea</h3>
<p>Iteratively update parameters by moving in the negative gradient direction to minimize the loss.</p>
<h3>Update rule</h3>
<p><strong>θ = θ − η·∇J(θ)</strong></p>
<ul>
<li>θ: parameters</li>
<li>η: learning rate</li>
<li>∇J(θ): gradient of loss</li>
</ul>
<p><img alt="Gradient descent" class="post-img" src="/images/dl-basis/1764757368494.png"/></p>
<h3>Objective</h3>
<p>Minimize: \( f(W) = \frac{1}{M} \sum_{i=1}^M e(h(x_i), y_i) \)</p>
<p>Update: \( \Delta W = -\alpha \nabla f(W) \)</p>
<h3>Batch GD characteristics</h3>
<ul>
<li>Uses all M samples per update.</li>
<li>Stable convergence, slower per step.</li>
<li>Learning rate controls stability and speed.</li>
</ul>
<p><img alt="Batch GD" class="post-img" src="/images/dl-basis/1764757381151.png"/></p>
<p><img alt="Mini-batch" class="post-img" src="/images/dl-basis/1764757397543.png"/></p>
<p><img alt="GD comparison" class="post-img" src="/images/dl-basis/1764757633120.png"/></p>
<table>
<thead>
<tr>
<th>Aspect</th>
<th>GD</th>
<th>SGD</th>
<th>Mini‑batch GD</th>
</tr>
</thead>
<tbody>
<tr><td>Gradient</td><td>Full dataset</td><td>Single sample</td><td>Mini batch</td></tr>
<tr><td>Speed</td><td>Slow</td><td>Fast</td><td>Medium</td></tr>
<tr><td>Stability</td><td>Most stable</td><td>Least stable</td><td>More stable</td></tr>
<tr><td>Memory</td><td>Highest</td><td>Lowest</td><td>Medium</td></tr>
<tr><td>Efficiency</td><td>Lowest</td><td>Highest</td><td>High</td></tr>
<tr><td>Usage</td><td>Rare</td><td>Rare</td><td>Most common</td></tr>
<tr><td>Noise</td><td>None</td><td>Highest</td><td>Medium</td></tr>
</tbody>
</table>
<h3>Key concepts</h3>
<ul>
<li><strong>Learning rate</strong>: too big → divergence; too small → slow. Use decay or adaptive methods.</li>
<li><strong>Convergence</strong>: local minima, saddle points, global minima.</li>
</ul>
<h3>Optimization tricks</h3>
<ol>
<li><strong>Momentum</strong>: \( v = \beta v + (1-\beta)\nabla J(\theta) \), \( \theta = \theta - \eta v \)</li>
<li><strong>Adaptive LR</strong>: AdaGrad, RMSProp, Adam (most common)</li>
</ol>
<h3>Challenges</h3>
<ul>
<li>Vanishing/exploding gradients</li>
<li>Local minima in non‑convex optimization</li>
<li>Overfitting without regularization</li>
</ul>
<h3>Common interview points</h3>
<ul>
<li>Batch size impacts</li>
<li>Learning rate strategies</li>
<li>SGD vs Batch GD</li>
<li>Momentum intuition</li>
<li>Convergence criteria</li>
<li>GD vs normal equations (linear regression)</li>
</ul>
<h3>Convergence conditions</h3>
<ul>
<li>\( ||\nabla J(\theta)|| &lt; \epsilon \)</li>
<li>\( |J(\theta^{t+1}) - J(\theta^t)| &lt; \epsilon \)</li>
<li>Max iterations reached</li>
</ul>
<h2 id="Overfitting"><a class="headerlink" href="#Overfitting" title="Overfitting"></a>Section 3 Overfitting</h2>
<p><img alt="Overfitting" class="post-img" src="/images/dl-basis/1764758193835.png"/></p>
<ul>
<li><strong>Overfitting</strong>: great on training, poor on new data.</li>
<li><strong>Underfitting</strong>: poor on both training and new data.</li>
<li><strong>Generalization</strong>: performance on unseen data.</li>
</ul>
<h3>Strategy</h3>
<ul>
<li>Start with an overly complex model, then regularize and simplify.</li>
<li>Regularization (L1/L2), dropout, early stopping, data augmentation, cross‑validation.</li>
</ul>
<h2 id="Regularization"><a class="headerlink" href="#Regularization" title="Weight Regularization"></a>Section 4 Weight Regularization</h2>
<p><img alt="Regularization" class="post-img" src="/images/dl-basis/1764758315510.png"/></p>
<h3>Core idea</h3>
<p>Large weights imply complex models and overfitting; add penalties to the loss.</p>
<h3>L1 (Lasso)</h3>
<p>\( \min \frac{1}{M} \sum_i J(h_\theta(x_i), y_i) + \lambda \sum_j |\theta_j| \)</p>
<ul>
<li>Sparse solutions, feature selection</li>
</ul>
<h3>L2 (Ridge)</h3>
<p>\( \min \frac{1}{M} \sum_i J(h_\theta(x_i), y_i) + \lambda \sum_j \theta_j^2 \)</p>
<ul>
<li>Uniform weight shrinkage, efficient</li>
</ul>
<h3>Lambda (λ)</h3>
<ul>
<li>Controls regularization strength</li>
<li>λ = 0 → original loss</li>
</ul>
<table>
<thead>
<tr><th>Aspect</th><th>L1</th><th>L2</th></tr>
</thead>
<tbody>
<tr><td>Solution</td><td>Sparse</td><td>Dense</td></tr>
<tr><td>Feature selection</td><td>Yes</td><td>No</td></tr>
<tr><td>Complexity</td><td>Higher</td><td>Lower</td></tr>
<tr><td>Use cases</td><td>Feature selection, simplification</td><td>Overfitting control, stability</td></tr>
</tbody>
</table>
<h2 id="Dropout"><a class="headerlink" href="#Dropout" title="Dropout"></a>Section 5 Dropout</h2>
<p><img alt="Dropout" class="post-img" src="/images/dl-basis/1764759252327.png"/></p>
<p><img alt="Dropout 2" class="post-img" src="/images/dl-basis/1764759261177.png"/></p>
<h3>Random Dropout</h3>
<ul>
<li>Randomly drop neurons during forward pass (usually 50%).</li>
<li>Train: drop with probability p; Test: scale weights by p.</li>
<li>Prevents co‑adaptation, acts like ensemble.</li>
</ul>
<h3>Non‑random Dropout (meProp)</h3>
<ul>
<li>Drop based on path activity (selective).</li>
<li>Backprop only updates top‑k% gradients.</li>
<li>Reduces computation and compresses the model.</li>
</ul>
<table>
<thead>
<tr><th>Feature</th><th>Random Dropout</th><th>meProp</th></tr>
</thead>
<tbody>
<tr><td>Drop policy</td><td>Random</td><td>Selective</td></tr>
<tr><td>Efficiency</td><td>Lower</td><td>Higher</td></tr>
<tr><td>Model compression</td><td>Low</td><td>High</td></tr>
<tr><td>Implementation</td><td>Simple</td><td>Complex</td></tr>
<tr><td>Goal</td><td>Anti‑overfitting</td><td>Compression + anti‑overfitting</td></tr>
</tbody>
</table>
<h2 id="Early-Stopping"><a class="headerlink" href="#Early-Stopping" title="Early Stopping"></a>Section 6 Early Stopping</h2>
<p><img alt="Early stopping" class="post-img" src="/images/dl-basis/1764759433375.png"/></p>
<p><img alt="Early stopping 2" class="post-img" src="/images/dl-basis/1764759554123.png"/></p>
<h3>Core idea</h3>
<p>Stop training before overfitting begins.</p>
<h3>Goldilocks spot</h3>
<ul>
<li>Validation error reaches minimum before rising.</li>
<li>Best generalization point.</li>
</ul>
<h3>Procedure</h3>
<ol>
<li>Monitor validation after each epoch.</li>
<li>Use patience (allow short‑term noise).</li>
<li>Save best checkpoint.</li>
<li>Stop when it keeps worsening.</li>
</ol>
<h3>Pros &amp; cons</h3>
<ul>
<li>Pros: simple, efficient, avoids wasted compute.</li>
<li>Cons: depends on validation split quality; can stop too early.</li>
</ul>
<div class="post-categoris-bottom">
<div class="post-categoris-name">AI</div>
<ul>
<li class="me base">
<a class="post-categoris-bottom-link" href="/2026/02/15/Convolutional-Neural-Network/">02. Convolutional Neural Networks</a>
</li>
</ul>
</div>
</div>
<div class="post-content-inner-space">
<div class="space-toc-main animate__animated animate__fadeInUp">
<ol class="space-toc">
<li class="space-toc-item space-toc-level-2"><a class="space-toc-link" href="#Loss-Functions"><span class="space-toc-text">Section 1 Common Loss Functions</span></a></li>
<li class="space-toc-item space-toc-level-2"><a class="space-toc-link" href="#GD"><span class="space-toc-text">Section 2 Gradient Descent</span></a></li>
<li class="space-toc-item space-toc-level-2"><a class="space-toc-link" href="#Overfitting"><span class="space-toc-text">Section 3 Overfitting</span></a></li>
<li class="space-toc-item space-toc-level-2"><a class="space-toc-link" href="#Regularization"><span class="space-toc-text">Section 4 Weight Regularization</span></a></li>
<li class="space-toc-item space-toc-level-2"><a class="space-toc-link" href="#Dropout"><span class="space-toc-text">Section 5 Dropout</span></a></li>
<li class="space-toc-item space-toc-level-2"><a class="space-toc-link" href="#Early-Stopping"><span class="space-toc-text">Section 6 Early Stopping</span></a></li>
</ol>
</div>
</div>
</div>
</div>
</article>
</div>
</div>
<div class="footer-outer animate__animated animate__fadeInUp">
<div class="footer-inner">
<div class="footer-text">
<p>Artificial Intelligence |  Financial Engineering | Mathematics | Computer Science  <strong>Rongyan <i class="ri-copyright-line"></i> 2026</strong></p>
</div>
<div class="footer-contact">
<ul class="footer-ul">
<li class="footer-li"><a href="https://github.com/RongyanYuan" target="_blank"><i class="ri-github-line"></i></a></li>
<li class="footer-li"><a href="mailto:adrianrongyanyun@gmail.com" target="_blank"><i class="ri-mail-line"></i></a></li>
<li class="footer-li"><a href="https://www.linkedin.com/in/rongyan-yuan-a076971b2/" target="_blank"><i class="ri-linkedin-box-line"></i></a></li>
</ul>
</div>
</div>
</div>
<script src="/js/white.js"></script>
<script>
  window.MathJax = {
    tex: {inlineMath: [['\\(','\\)'], ['$', '$']]},
    svg: {fontCache: 'global'}
  };
</script>
<script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-svg.js"></script>
</body>
</html>
